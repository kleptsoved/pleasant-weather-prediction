{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Enhanced ML Model Training - Multi-Station Pleasant Weather Classification\n",
    "\n",
    "This notebook provides comprehensive functionality for multi-station weather data analysis with three approaches:\n",
    "1. **Single Station Model** - Train a model for one specific weather station\n",
    "2. **Multi-Station Model** - Leverage data from multiple stations \n",
    "3. **Traditional Approach** - Manual target and feature selection\n",
    "\n",
    "## üåü Key Features:\n",
    "- Automatic detection of multi-station weather datasets\n",
    "- Station-specific model training with pleasant weather prediction\n",
    "- Special focus on mean temperature features\n",
    "- Interactive feature selection (mean only, all temps, all weather, custom patterns)\n",
    "- Station-wise accuracy analysis and performance comparison\n",
    "- Enhanced neural network optimization for weather prediction\n",
    "\n",
    "## üìÅ Expected Structure\n",
    "```\n",
    "Your Project/\n",
    "‚îú‚îÄ‚îÄ 02_data/Processed_data/    ‚Üê Pre-scaled data\n",
    "‚îú‚îÄ‚îÄ 03_notebooks/              ‚Üê Run from here\n",
    "‚îî‚îÄ‚îÄ 05_results/                ‚Üê Output files\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration settings for the analysis\n",
    "CONFIG = {\n",
    "    'target_column': 'pleasant_weather',  # Column name pattern for pleasant weather labels\n",
    "    'station_column': 'station_id',  # Column name for station identifiers\n",
    "    'missing_threshold': 0.04,  # Maximum allowed missing data per station\n",
    "    'critical_features': ['temp', 'humidity', 'pressure', 'wind'],  # Features to check for missing data\n",
    "    'overfitting_threshold': 0.05,  # Maximum train-test score difference\n",
    "    'high_accuracy_threshold': 0.95,  # Threshold for high-accuracy station reporting\n",
    "    'exclude_patterns': ['date', 'time', 'year', 'month', 'day', 'hour', 'minute', 'id'],  # Temporal features to exclude\n",
    "    'weather_patterns': {  # Patterns to identify weather feature types\n",
    "        'temperature': ['temp'],\n",
    "        'humidity': ['humid', 'moisture'],\n",
    "        'pressure': ['pressure', 'press'],\n",
    "        'wind': ['wind', 'gust'],\n",
    "        'precipitation': ['rain', 'precip', 'snow'],\n",
    "        'radiation': ['radiation', 'solar'],\n",
    "        'visibility': ['visibility', 'vis'],\n",
    "        'clouds': ['cloud']\n",
    "    },\n",
    "    'neural_network_params': {\n",
    "        'hidden_layer_sizes': [(50,), (100,), (100,50), (100,75,50), (200,100,50)],\n",
    "        'max_iter': [500, 1000, 2000],\n",
    "        'tol': [1e-3, 1e-4, 1e-5],\n",
    "        'activation': ['relu', 'tanh'],\n",
    "        'solver': ['adam', 'lbfgs']\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Configuration loaded successfully!\")\n",
    "print(\"\\nüìã Key Settings:\")\n",
    "for key, value in CONFIG.items():\n",
    "    if key not in ['neural_network_params', 'weather_patterns']:\n",
    "        print(f\"  ‚Ä¢ {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "# ML imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold, StratifiedKFold\n",
    "from sklearn.metrics import *\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, f_regression, mutual_info_classif, mutual_info_regression\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso, ElasticNet, SGDClassifier, SGDRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.offline as pyo\n",
    "pyo.init_notebook_mode(connected=True)\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üìÖ Analysis date: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• 2. Load and Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the module\n",
    "import sys\n",
    "sys.path.append('./src')  # Optional: adapt if running outside src\n",
    "from file_handler import setup_paths, load_multiple_datasets\n",
    "\n",
    "# 1. Interactive project path setup\n",
    "project_root, input_path, output_path = setup_paths()\n",
    "\n",
    "# 2. File selection and loading\n",
    "datasets = load_multiple_datasets(input_path)\n",
    "\n",
    "# 3. Identify features and target datasets\n",
    "print(\"\\nüîç Identifying datasets...\")\n",
    "features_df = None\n",
    "target_df = None\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    print(f\"\\nüìä {name}:\")\n",
    "    print(f\"   Shape: {df.shape}\")\n",
    "    print(f\"   Columns sample: {list(df.columns[:5])}...\")\n",
    "    \n",
    "    # Check if this is the target dataset\n",
    "    if 'answer' in name.lower() or 'pleasant' in name.lower():\n",
    "        target_df = df\n",
    "        print(\"   ‚úÖ Identified as TARGET dataset\")\n",
    "    elif 'processed' in name.lower() or 'scaled' in name.lower():\n",
    "        features_df = df\n",
    "        print(\"   ‚úÖ Identified as FEATURES dataset\")\n",
    "\n",
    "if features_df is None or target_df is None:\n",
    "    print(\"\\n‚ö†Ô∏è Could not automatically identify datasets. Please select manually:\")\n",
    "    dataset_names = list(datasets.keys())\n",
    "    \n",
    "    print(\"\\nAvailable datasets:\")\n",
    "    for i, name in enumerate(dataset_names):\n",
    "        print(f\"  {i+1}. {name}\")\n",
    "    \n",
    "    features_idx = int(input(\"\\nüëâ Select FEATURES dataset number: \")) - 1\n",
    "    target_idx = int(input(\"üëâ Select TARGET dataset number: \")) - 1\n",
    "    \n",
    "    features_df = datasets[dataset_names[features_idx]]\n",
    "    target_df = datasets[dataset_names[target_idx]]\n",
    "\n",
    "print(f\"\\n‚úÖ Datasets identified:\")\n",
    "print(f\"   Features: {features_df.shape}\")\n",
    "print(f\"   Targets: {target_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Merge datasets\n",
    "print(\"\\nüîÑ Merging datasets...\")\n",
    "\n",
    "# Find common columns for merging\n",
    "common_cols = list(set(features_df.columns) & set(target_df.columns))\n",
    "print(f\"\\nCommon columns: {common_cols}\")\n",
    "\n",
    "# Identify merge keys (date and station identifiers)\n",
    "merge_keys = []\n",
    "date_cols = [col for col in common_cols if 'date' in col.lower()]\n",
    "station_cols = [col for col in common_cols if 'station' in col.lower() or 'id' in col.lower()]\n",
    "\n",
    "if date_cols:\n",
    "    merge_keys.extend(date_cols[:1])  # Use first date column\n",
    "if station_cols:\n",
    "    merge_keys.extend(station_cols[:1])  # Use first station column\n",
    "\n",
    "# If no automatic detection, ask user\n",
    "if not merge_keys:\n",
    "    print(\"\\n‚ö†Ô∏è Could not automatically detect merge keys.\")\n",
    "    print(\"Available columns in both datasets:\")\n",
    "    for i, col in enumerate(common_cols[:20]):\n",
    "        print(f\"  {i+1}. {col}\")\n",
    "    \n",
    "    key_indices = input(\"\\nüëâ Select merge key columns (comma-separated numbers): \")\n",
    "    merge_keys = [common_cols[int(i)-1] for i in key_indices.split(',')]\n",
    "\n",
    "print(f\"\\nüîó Merging on: {merge_keys}\")\n",
    "\n",
    "# Perform merge\n",
    "df = pd.merge(features_df, target_df[merge_keys + [col for col in target_df.columns if col not in merge_keys]], \n",
    "              on=merge_keys, how='inner')\n",
    "\n",
    "print(f\"‚úÖ Merged dataset: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "print(f\"   Memory usage: {df.memory_usage().sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Update station column name if needed\n",
    "if CONFIG['station_column'] not in df.columns:\n",
    "    station_candidates = [col for col in df.columns if 'station' in col.lower()]\n",
    "    if station_candidates:\n",
    "        CONFIG['station_column'] = station_candidates[0]\n",
    "        print(f\"\\nüìç Updated station column to: {CONFIG['station_column']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ 3. Data Quality Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify critical features\n",
    "critical_features = []\n",
    "for pattern in CONFIG['critical_features']:\n",
    "    matching_cols = [col for col in df.columns if pattern.lower() in col.lower()]\n",
    "    critical_features.extend(matching_cols)\n",
    "\n",
    "critical_features = list(set(critical_features))  # Remove duplicates\n",
    "print(f\"\\nüîç Found {len(critical_features)} critical features:\")\n",
    "for feat in critical_features[:10]:\n",
    "    print(f\"   ‚Ä¢ {feat}\")\n",
    "if len(critical_features) > 10:\n",
    "    print(f\"   ... and {len(critical_features) - 10} more\")\n",
    "\n",
    "# Check for station column\n",
    "if CONFIG['station_column'] in df.columns:\n",
    "    # Calculate missing data per station\n",
    "    print(f\"\\nüìä Analyzing data quality by station...\")\n",
    "    stations_to_drop = []\n",
    "    station_quality = {}\n",
    "    \n",
    "    for station in df[CONFIG['station_column']].unique():\n",
    "        station_data = df[df[CONFIG['station_column']] == station]\n",
    "        \n",
    "        # Calculate missing percentage for critical features\n",
    "        if critical_features:\n",
    "            missing_pct = station_data[critical_features].isnull().sum().sum() / (len(station_data) * len(critical_features))\n",
    "        else:\n",
    "            missing_pct = station_data.isnull().sum().sum() / (len(station_data) * len(station_data.columns))\n",
    "        \n",
    "        station_quality[station] = {\n",
    "            'missing_pct': missing_pct,\n",
    "            'row_count': len(station_data)\n",
    "        }\n",
    "        \n",
    "        if missing_pct > CONFIG['missing_threshold']:\n",
    "            stations_to_drop.append(station)\n",
    "    \n",
    "    # Report and drop stations\n",
    "    if stations_to_drop:\n",
    "        print(f\"\\n‚ö†Ô∏è Dropping {len(stations_to_drop)} stations with >{CONFIG['missing_threshold']*100:.0f}% missing data:\")\n",
    "        for station in stations_to_drop[:5]:\n",
    "            print(f\"   ‚Ä¢ {station}: {station_quality[station]['missing_pct']*100:.1f}% missing\")\n",
    "        if len(stations_to_drop) > 5:\n",
    "            print(f\"   ... and {len(stations_to_drop) - 5} more\")\n",
    "        \n",
    "        # Drop stations\n",
    "        df_before = len(df)\n",
    "        df = df[~df[CONFIG['station_column']].isin(stations_to_drop)]\n",
    "        print(f\"\\n‚úÖ Removed {df_before - len(df):,} rows from {len(stations_to_drop)} stations\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ All stations have acceptable data quality!\")\n",
    "    \n",
    "    # Summary of remaining stations\n",
    "    remaining_stations = df[CONFIG['station_column']].nunique()\n",
    "    print(f\"\\nüìç Remaining stations: {remaining_stations}\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Station column '{CONFIG['station_column']}' not found. Proceeding with overall data quality check.\")\n",
    "    \n",
    "    # Overall missing data handling\n",
    "    missing_pct = df.isnull().sum() / len(df) * 100\n",
    "    cols_to_drop = missing_pct[missing_pct > CONFIG['missing_threshold']*100].index\n",
    "    if len(cols_to_drop) > 0:\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "        print(f\"   Dropped {len(cols_to_drop)} columns with >{CONFIG['missing_threshold']*100:.0f}% missing data\")\n",
    "\n",
    "# Fill remaining missing values\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
    "\n",
    "print(f\"\\n‚úÖ Final dataset: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ 4. Interactive Target Selection, Station Filtering, and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MULTI-STATION WEATHER DATA DETECTION\n",
    "print(\"\\nüåç DETECTING MULTI-STATION WEATHER DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Detect if this is multi-station weather data\n",
    "pleasant_weather_cols = [col for col in df.columns if 'pleasant_weather' in col.lower()]\n",
    "station_names = []\n",
    "\n",
    "if pleasant_weather_cols:\n",
    "    # Extract station names from pleasant_weather columns\n",
    "    for col in pleasant_weather_cols:\n",
    "        station = col.replace('_pleasant_weather', '').replace('pleasant_weather', '')\n",
    "        if station and station not in ['', '_']:\n",
    "            station_names.append(station)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Detected multi-station weather dataset with {len(station_names)} stations:\")\n",
    "    for i, station in enumerate(station_names, 1):\n",
    "        # Count features for this station\n",
    "        station_features = [col for col in df.columns if col.startswith(station + '_')]\n",
    "        print(f\"  {i:2d}. {station} ({len(station_features)} features)\")\n",
    "    \n",
    "    # Ask user to select approach\n",
    "    print(\"\\nüìä Model Training Approach:\")\n",
    "    print(\"  1. Single Station Model - Train separate model for one station\")\n",
    "    print(\"  2. Multi-Station Model - Use data from all stations\")\n",
    "    print(\"  3. Traditional Approach - Select target column manually\")\n",
    "    \n",
    "    approach = input(\"\\nüëâ Select approach (1-3): \").strip()\n",
    "    \n",
    "    if approach == '1':\n",
    "        # Single station approach\n",
    "        print(\"\\nüéØ SELECT TARGET STATION\")\n",
    "        print(\"-\"*40)\n",
    "        for i, station in enumerate(station_names, 1):\n",
    "            print(f\"  {i:2d}. {station}\")\n",
    "        \n",
    "        station_idx = int(input(\"\\nüëâ Select station number: \")) - 1\n",
    "        selected_station = station_names[station_idx]\n",
    "        \n",
    "        # Show all features for the selected station\n",
    "        station_features = [col for col in df.columns if col.startswith(selected_station + '_')]\n",
    "        \n",
    "        print(f\"\\nüìä Available features for {selected_station}:\")\n",
    "        print(\"-\"*40)\n",
    "        \n",
    "        # Categorize features for better display\n",
    "        feature_categories = {\n",
    "            'Target': [],\n",
    "            'Temperature': [],\n",
    "            'Weather Conditions': [],\n",
    "            'Other': []\n",
    "        }\n",
    "        \n",
    "        for feat in station_features:\n",
    "            if 'pleasant_weather' in feat:\n",
    "                feature_categories['Target'].append(feat)\n",
    "            elif 'temp' in feat.lower():\n",
    "                feature_categories['Temperature'].append(feat)\n",
    "            elif any(weather in feat.lower() for weather in ['humid', 'pressure', 'wind', 'rain', 'cloud', 'sunshine']):\n",
    "                feature_categories['Weather Conditions'].append(feat)\n",
    "            else:\n",
    "                feature_categories['Other'].append(feat)\n",
    "        \n",
    "        # Display categorized features\n",
    "        feature_list = []\n",
    "        for category, features in feature_categories.items():\n",
    "            if features:\n",
    "                print(f\"\\n{category}:\")\n",
    "                for feat in features:\n",
    "                    feature_list.append(feat)\n",
    "                    print(f\"  {len(feature_list):2d}. {feat}\")\n",
    "                    # Show basic stats for numeric features\n",
    "                    if df[feat].dtype in [np.number]:\n",
    "                        if df[feat].nunique() == 2:\n",
    "                            value_counts = df[feat].value_counts()\n",
    "                            print(f\"      Binary: {dict(value_counts)}\")\n",
    "                        else:\n",
    "                            print(f\"      Range: [{df[feat].min():.2f}, {df[feat].max():.2f}]\")\n",
    "        \n",
    "        # Ask user to select target\n",
    "        print(\"\\nüéØ SELECT TARGET VARIABLE\")\n",
    "        print(\"üí° Tip: Choose pleasant_weather for classification, or a numeric feature for regression\")\n",
    "        \n",
    "        default_target = f\"{selected_station}_pleasant_weather\"\n",
    "        if default_target in feature_list:\n",
    "            default_idx = feature_list.index(default_target) + 1\n",
    "            target_idx = input(f\"\\nüëâ Select target feature number (1-{len(feature_list)}) [default: {default_idx}]: \").strip()\n",
    "            if not target_idx:\n",
    "                target_idx = default_idx\n",
    "            else:\n",
    "                target_idx = int(target_idx)\n",
    "        else:\n",
    "            target_idx = int(input(f\"\\nüëâ Select target feature number (1-{len(feature_list)}): \"))\n",
    "        \n",
    "        target_col = feature_list[target_idx - 1]\n",
    "        \n",
    "        print(f\"\\n‚úÖ Selected station: {selected_station}\")\n",
    "        print(f\"   Target column: {target_col}\")\n",
    "        \n",
    "        # Determine problem type based on target\n",
    "        if df[target_col].nunique() == 2:\n",
    "            problem_type = 'classification'\n",
    "            print(f\"   Problem type: Binary Classification\")\n",
    "        elif df[target_col].nunique() <= 10:\n",
    "            problem_type = 'classification'\n",
    "            print(f\"   Problem type: Multi-class Classification ({df[target_col].nunique()} classes)\")\n",
    "        else:\n",
    "            problem_type = 'regression'\n",
    "            print(f\"   Problem type: Regression\")\n",
    "        \n",
    "        # Store station selection for later filtering\n",
    "        single_station_mode = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE SELECTION WITH FOCUS ON MEAN FEATURES\n",
    "print(\"\\nüéØ FEATURE SELECTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Identify columns to exclude\n",
    "exclude_cols = [target_col, 'target']\n",
    "if CONFIG['station_column'] in df.columns:\n",
    "    exclude_cols.append(CONFIG['station_column'])\n",
    "\n",
    "# For single station mode, focus on that station's features\n",
    "if single_station_mode and selected_station:\n",
    "    print(f\"\\nüéØ Single Station Mode: {selected_station}\")\n",
    "    \n",
    "    # Get all features for the selected station\n",
    "    available_features = [col for col in df.columns \n",
    "                         if col.startswith(selected_station + '_') \n",
    "                         and col not in exclude_cols \n",
    "                         and df[col].dtype in [np.number]]\n",
    "    \n",
    "    # Categorize features\n",
    "    mean_features = [f for f in available_features if '_mean' in f]\n",
    "    temp_features = [f for f in available_features if 'temp' in f.lower()]\n",
    "    weather_features = {\n",
    "        'Temperature': [f for f in available_features if 'temp' in f.lower()],\n",
    "        'Humidity': [f for f in available_features if 'humid' in f.lower()],\n",
    "        'Pressure': [f for f in available_features if 'pressure' in f.lower()],\n",
    "        'Wind': [f for f in available_features if 'wind' in f.lower()],\n",
    "        'Precipitation': [f for f in available_features if any(p in f.lower() for p in ['rain', 'precip', 'snow'])],\n",
    "        'Other': [f for f in available_features if not any(p in f.lower() for p in ['temp', 'humid', 'pressure', 'wind', 'rain', 'precip', 'snow'])]\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüìä Available features for {selected_station}:\")\n",
    "    for category, features in weather_features.items():\n",
    "        if features:\n",
    "            print(f\"\\n{category} ({len(features)} features):\")\n",
    "            for feat in features[:5]:\n",
    "                print(f\"  ‚Ä¢ {feat}\")\n",
    "            if len(features) > 5:\n",
    "                print(f\"  ... and {len(features) - 5} more\")\n",
    "    \n",
    "    print(f\"\\nüå°Ô∏è Mean features: {mean_features}\")\n",
    "    \n",
    "else:\n",
    "    # Multi-station mode or traditional mode\n",
    "    # Get all numeric features\n",
    "    available_features = [col for col in df.columns \n",
    "                         if col not in exclude_cols \n",
    "                         and df[col].dtype in [np.number]]\n",
    "    \n",
    "    # Find all mean features across all stations\n",
    "    mean_features = [f for f in available_features if '_mean' in f]\n",
    "    print(f\"\\nüå°Ô∏è Found {len(mean_features)} mean features across all stations\")\n",
    "\n",
    "# Feature selection options\n",
    "print(\"\\nüìä Feature Selection Options:\")\n",
    "print(\"  1. Mean features only (temperature means)\")\n",
    "print(\"  2. All temperature features (mean, min, max)\")\n",
    "print(\"  3. All weather features\")\n",
    "print(\"  4. Custom selection by pattern\")\n",
    "print(\"  5. Manual feature selection\")\n",
    "if single_station_mode:\n",
    "    print(\"  6. All features from selected station\")\n",
    "\n",
    "selection_option = input(\"\\nüëâ Choose option (1-6): \").strip()\n",
    "\n",
    "if selection_option == '1':\n",
    "    # Mean features only\n",
    "    feature_cols = mean_features\n",
    "    print(f\"\\n‚úÖ Selected {len(feature_cols)} mean features\")\n",
    "    \n",
    "elif selection_option == '2':\n",
    "    # All temperature features\n",
    "    feature_cols = [f for f in available_features if 'temp' in f.lower()]\n",
    "    print(f\"\\n‚úÖ Selected {len(feature_cols)} temperature features\")\n",
    "    \n",
    "elif selection_option == '3':\n",
    "    # All weather features\n",
    "    feature_cols = available_features\n",
    "    print(f\"\\n‚úÖ Selected all {len(feature_cols)} weather features\")\n",
    "    \n",
    "elif selection_option == '4':\n",
    "    # Custom pattern\n",
    "    print(\"\\nEnter patterns to match (comma-separated)\")\n",
    "    print(\"Example: 'mean,pressure' for mean values and pressure\")\n",
    "    patterns = input(\"\\nüëâ Patterns: \").strip().split(',')\n",
    "    \n",
    "    feature_cols = []\n",
    "    for pattern in patterns:\n",
    "        matching = [f for f in available_features if pattern.strip().lower() in f.lower()]\n",
    "        feature_cols.extend(matching)\n",
    "        print(f\"  ‚Ä¢ '{pattern.strip()}' matched {len(matching)} features\")\n",
    "    \n",
    "    feature_cols = list(set(feature_cols))  # Remove duplicates\n",
    "    print(f\"\\n‚úÖ Selected {len(feature_cols)} features by pattern\")\n",
    "    \n",
    "elif selection_option == '5':\n",
    "    # Manual selection\n",
    "    print(\"\\nüìã Available features:\")\n",
    "    for i, feat in enumerate(available_features[:30], 1):\n",
    "        print(f\"  {i:3d}. {feat}\")\n",
    "    if len(available_features) > 30:\n",
    "        print(f\"  ... and {len(available_features) - 30} more\")\n",
    "    \n",
    "    print(\"\\nEnter feature numbers (comma-separated) or 'all'\")\n",
    "    selection = input(\"\\nüëâ Selection: \").strip()\n",
    "    \n",
    "    if selection.lower() == 'all':\n",
    "        feature_cols = available_features\n",
    "    else:\n",
    "        indices = [int(x.strip())-1 for x in selection.split(',')]\n",
    "        feature_cols = [available_features[i] for i in indices if i < len(available_features)]\n",
    "    \n",
    "    print(f\"\\n‚úÖ Selected {len(feature_cols)} features manually\")\n",
    "    \n",
    "elif selection_option == '6' and single_station_mode:\n",
    "    # All features from selected station\n",
    "    feature_cols = available_features\n",
    "    print(f\"\\n‚úÖ Selected all {len(feature_cols)} features from {selected_station}\")\n",
    "else:\n",
    "    # Default to mean features\n",
    "    feature_cols = mean_features\n",
    "    print(f\"\\n‚úÖ Defaulting to {len(feature_cols)} mean features\")\n",
    "\n",
    "# Exclude temporal features\n",
    "temporal_excluded = []\n",
    "for pattern in CONFIG['exclude_patterns']:\n",
    "    temporal_excluded.extend([f for f in feature_cols if pattern in f.lower()])\n",
    "\n",
    "feature_cols = [f for f in feature_cols if f not in temporal_excluded]\n",
    "\n",
    "print(f\"\\nüìä FINAL FEATURE SELECTION:\")\n",
    "print(f\"   Selected features: {len(feature_cols)}\")\n",
    "print(f\"   Excluded temporal: {len(temporal_excluded)}\")\n",
    "\n",
    "# Show sample of selected features\n",
    "print(\"\\nSample of selected features:\")\n",
    "for feat in feature_cols[:10]:\n",
    "    print(f\"   ‚Ä¢ {feat}\")\n",
    "if len(feature_cols) > 10:\n",
    "    print(f\"   ... and {len(feature_cols) - 10} more features\")\n",
    "\n",
    "# Store station information\n",
    "station_info = None\n",
    "if CONFIG['station_column'] in df.columns:\n",
    "    station_info = df[CONFIG['station_column']].values\n",
    "    print(f\"\\nüìç Station information preserved for analysis\")\n",
    "elif single_station_mode:\n",
    "    print(f\"\\nüìç Single station mode: {selected_station}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ 5. Data Preparation and Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = df[feature_cols]\n",
    "y = df['target']\n",
    "\n",
    "# Feature selection if too many features\n",
    "if X.shape[1] > 50:\n",
    "    k = min(30, X.shape[1] // 2)\n",
    "    selector = SelectKBest(f_classif, k=k)\n",
    "    \n",
    "    X_selected = selector.fit_transform(X, y)\n",
    "    selected_features = X.columns[selector.get_support()].tolist()\n",
    "    X = pd.DataFrame(X_selected, columns=selected_features)\n",
    "    print(f\"\\n‚úÖ Reduced features from {len(feature_cols)} to {k}\")\n",
    "else:\n",
    "    selected_features = feature_cols\n",
    "\n",
    "# Train-test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Also split station info if available\n",
    "if station_info is not None:\n",
    "    train_indices = X_train.index\n",
    "    test_indices = X_test.index\n",
    "    station_train = station_info[train_indices]\n",
    "    station_test = station_info[test_indices]\n",
    "\n",
    "print(f\"\\n‚úÇÔ∏è Data split:\")\n",
    "print(f\"   Training: {X_train.shape[0]:,} samples\")\n",
    "print(f\"   Testing: {X_test.shape[0]:,} samples\")\n",
    "print(f\"   Class balance (train): {dict(y_train.value_counts(normalize=True).round(3))}\")\n",
    "print(f\"   Class balance (test): {dict(y_test.value_counts(normalize=True).round(3))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì 6. Manual Gradient Descent Implementation (Optional)\n",
    "\n",
    "This section demonstrates gradient descent optimization from scratch. \n",
    "**Note**: This is primarily educational and works best with single-feature regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we should run gradient descent demo\n",
    "# Initialize results dictionaries if they don't exist\n",
    "if 'results' not in locals():\n",
    "    results = {}\n",
    "if 'best_models' not in locals():\n",
    "    best_models = {}\n",
    "\n",
    "# Gradient descent is only for regression, not classification\n",
    "problem_type = 'classification'  # Pleasant weather prediction is a classification task\n",
    "print(f\"\\nüí° Problem type: {problem_type}\")\n",
    "print(\"Gradient descent demo is available for regression problems only.\")\n",
    "print(\"Proceeding with classification models for pleasant weather prediction...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ 7. Model Training with Enhanced Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class balance\n",
    "class_props = y_train.value_counts(normalize=True)\n",
    "is_balanced = class_props.min() >= 0.2\n",
    "class_weight = None if is_balanced else 'balanced'\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, class_weight=class_weight),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=10, class_weight=class_weight),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=10, class_weight=class_weight, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, max_depth=5),\n",
    "    'SVM': SVC(kernel='rbf', probability=True, class_weight=class_weight),\n",
    "    'Neural Network': MLPClassifier(random_state=42),  # Will be tuned extensively\n",
    "    'Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "# Simplified parameter grids for non-neural models\n",
    "param_grids = {\n",
    "    'Logistic Regression': {'C': [0.1, 1, 10]},\n",
    "    'Decision Tree': {'max_depth': [5, 10, 15], 'min_samples_split': [2, 10]},\n",
    "    'Random Forest': {'n_estimators': [50, 100], 'max_depth': [10, 20]},\n",
    "    'Gradient Boosting': {'n_estimators': [50, 100], 'learning_rate': [0.1, 0.2]},\n",
    "    'SVM': {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto']},\n",
    "    'Neural Network': CONFIG['neural_network_params'],  # Extensive tuning\n",
    "    'Naive Bayes': {}\n",
    "}\n",
    "\n",
    "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scoring = 'balanced_accuracy' if not is_balanced else 'accuracy'\n",
    "\n",
    "print(f\"\\nü§ñ Training {len(models)} models...\")\n",
    "print(f\"   Scoring metric: {scoring}\")\n",
    "print(f\"   Class weight: {class_weight}\")\n",
    "print(f\"\\nüß† Neural Network will test {np.prod([len(v) for v in CONFIG['neural_network_params'].values()]):,} combinations!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this to the imports section at the beginning of the notebook\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm as tqdm_notebook  # For Jupyter notebooks\n",
    "import sys\n",
    "\n",
    "# Enhanced model training with progress bars\n",
    "print(f\"\\nü§ñ Training {len(models)} models...\")\n",
    "print(f\"   Scoring metric: {scoring}\")\n",
    "print(f\"   Class weight: {class_weight}\")\n",
    "print(f\"   Cross-validation folds: {cv_strategy.n_splits}\")\n",
    "\n",
    "# Calculate total parameter combinations for progress estimation\n",
    "total_params = {}\n",
    "for name, params in param_grids.items():\n",
    "    if params:\n",
    "        n_combos = 1\n",
    "        for param_values in params.values():\n",
    "            n_combos *= len(param_values)\n",
    "        total_params[name] = n_combos\n",
    "    else:\n",
    "        total_params[name] = 1\n",
    "\n",
    "print(f\"\\nüìä Parameter combinations per model:\")\n",
    "for name, n_combos in total_params.items():\n",
    "    print(f\"   ‚Ä¢ {name}: {n_combos} combinations\")\n",
    "    if name == 'Neural Network':\n",
    "        print(f\"     üß† Total: {n_combos:,} combinations √ó {cv_strategy.n_splits} folds = {n_combos * cv_strategy.n_splits:,} fits!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Train models with progress tracking\n",
    "results = {}\n",
    "best_models = {}\n",
    "training_times = {}\n",
    "\n",
    "# Create main progress bar for all models\n",
    "with tqdm(total=len(models), desc=\"Overall Progress\", position=0, leave=True) as pbar_main:\n",
    "    \n",
    "    for model_idx, (name, model) in enumerate(models.items()):\n",
    "        # Update main progress bar description\n",
    "        pbar_main.set_description(f\"Training {name}\")\n",
    "        \n",
    "        print(f\"\\nüîÑ Model {model_idx + 1}/{len(models)}: {name}\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Create a custom callback for GridSearchCV if using sklearn >= 0.24\n",
    "        try:\n",
    "            from sklearn.model_selection._search import _VerboseReporter\n",
    "            \n",
    "            class TqdmGridSearchCV(GridSearchCV):\n",
    "                def _run_search(self, evaluate_candidates):\n",
    "                    \"\"\"Run the search with tqdm progress bar\"\"\"\n",
    "                    with tqdm(total=len(self.param_grid) if hasattr(self, 'param_grid') else total_params[name], \n",
    "                             desc=f\"  Grid Search\", \n",
    "                             position=1, \n",
    "                             leave=False) as pbar:\n",
    "                        \n",
    "                        def evaluate_candidates_progress(candidate_params):\n",
    "                            results = evaluate_candidates(candidate_params)\n",
    "                            pbar.update(len(candidate_params))\n",
    "                            return results\n",
    "                        \n",
    "                        return super()._run_search(evaluate_candidates_progress)\n",
    "            \n",
    "            # Use custom GridSearchCV with progress\n",
    "            grid = TqdmGridSearchCV(\n",
    "                model, \n",
    "                param_grids[name], \n",
    "                cv=cv_strategy, \n",
    "                scoring=scoring, \n",
    "                n_jobs=-1,\n",
    "                verbose=0  # Disable sklearn's verbose output\n",
    "            )\n",
    "        except:\n",
    "            # Fallback to regular GridSearchCV\n",
    "            grid = GridSearchCV(\n",
    "                model, \n",
    "                param_grids[name], \n",
    "                cv=cv_strategy, \n",
    "                scoring=scoring, \n",
    "                n_jobs=-1,\n",
    "                verbose=0\n",
    "            )\n",
    "        \n",
    "        # Show parameter search info\n",
    "        print(f\"  üìä Searching {total_params[name]} parameter combinations...\")\n",
    "        print(f\"  üîÑ Using {cv_strategy.n_splits}-fold cross-validation\")\n",
    "        \n",
    "        # Fit with progress indication\n",
    "        try:\n",
    "            grid.fit(X_train, y_train)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\n‚ö†Ô∏è Training interrupted! Saving results so far...\")\n",
    "            break\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        training_times[name] = training_time\n",
    "        \n",
    "        # Store best model\n",
    "        best_models[name] = grid.best_estimator_\n",
    "        \n",
    "        # Predictions with mini progress\n",
    "        print(\"  üìà Making predictions...\", end='')\n",
    "        y_pred = grid.best_estimator_.predict(X_test)\n",
    "        y_pred_proba = grid.best_estimator_.predict_proba(X_test)[:, 1] if hasattr(grid.best_estimator_, 'predict_proba') else None\n",
    "        y_train_pred = grid.best_estimator_.predict(X_train)\n",
    "        print(\" Done!\")\n",
    "        \n",
    "        # Calculate metrics\n",
    "        print(\"  üìä Calculating metrics...\", end='')\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'balanced_accuracy': balanced_accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred, average='binary'),\n",
    "            'recall': recall_score(y_test, y_pred, average='binary'),\n",
    "            'f1_score': f1_score(y_test, y_pred, average='binary'),\n",
    "            'cv_score': grid.best_score_,\n",
    "            'train_accuracy': accuracy_score(y_train, y_train_pred),\n",
    "            'train_balanced_accuracy': balanced_accuracy_score(y_train, y_train_pred),\n",
    "            'training_time': training_time\n",
    "        }\n",
    "        \n",
    "        if y_pred_proba is not None:\n",
    "            metrics['auc_roc'] = roc_auc_score(y_test, y_pred_proba)\n",
    "        print(\" Done!\")\n",
    "        \n",
    "        results[name] = {\n",
    "            'metrics': metrics,\n",
    "            'best_params': grid.best_params_,\n",
    "            'predictions': y_pred,\n",
    "            'probabilities': y_pred_proba\n",
    "        }\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\n  ‚úÖ Results for {name}:\")\n",
    "        print(f\"     ‚Ä¢ Best CV Score: {grid.best_score_:.4f}\")\n",
    "        print(f\"     ‚Ä¢ Test Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"     ‚Ä¢ Test Balanced Accuracy: {metrics['balanced_accuracy']:.4f}\")\n",
    "        print(f\"     ‚Ä¢ Training Time: {training_time:.2f}s\")\n",
    "        \n",
    "        # Special reporting for Neural Network\n",
    "        if name == 'Neural Network':\n",
    "            print(f\"\\n  üß† Best Neural Network Configuration:\")\n",
    "            for param, value in grid.best_params_.items():\n",
    "                print(f\"     ‚Ä¢ {param}: {value}\")\n",
    "        \n",
    "        # Update main progress bar\n",
    "        pbar_main.update(1)\n",
    "        \n",
    "        # Estimate remaining time\n",
    "        elapsed_time = sum(training_times.values())\n",
    "        avg_time_per_model = elapsed_time / (model_idx + 1)\n",
    "        remaining_models = len(models) - (model_idx + 1)\n",
    "        estimated_remaining = avg_time_per_model * remaining_models\n",
    "        \n",
    "        if remaining_models > 0:\n",
    "            print(f\"\\n  ‚è±Ô∏è Estimated time remaining: {estimated_remaining:.1f}s ({remaining_models} models left)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ All models trained successfully!\")\n",
    "print(f\"‚è±Ô∏è Total training time: {sum(training_times.values()):.2f}s\")\n",
    "\n",
    "# Summary table\n",
    "print(\"\\nüìä Quick Summary:\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'Model':<20} {'CV Score':>10} {'Test Acc':>10} {'Time (s)':>10}\")\n",
    "print(\"-\"*80)\n",
    "for name in models.keys():\n",
    "    if name in results:\n",
    "        print(f\"{name:<20} {results[name]['metrics']['cv_score']:>10.4f} \"\n",
    "              f\"{results[name]['metrics']['accuracy']:>10.4f} \"\n",
    "              f\"{results[name]['metrics']['training_time']:>10.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 8. Model Comparison and Overfitting Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison dataframe\n",
    "comparison_data = []\n",
    "for name, result in results.items():\n",
    "    row = {'Model': name}\n",
    "    row.update(result['metrics'])\n",
    "    \n",
    "    # Calculate overfitting metrics\n",
    "    row['overfitting_score'] = row['train_balanced_accuracy'] - row['balanced_accuracy']\n",
    "    row['is_overfitting'] = row['overfitting_score'] > CONFIG['overfitting_threshold']\n",
    "    \n",
    "    comparison_data.append(row)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('balanced_accuracy', ascending=False)\n",
    "\n",
    "print(\"\\nüìä Model Performance Comparison:\")\n",
    "display_cols = ['Model', 'balanced_accuracy', 'accuracy', 'precision', 'recall', 'f1_score', \n",
    "                'train_balanced_accuracy', 'overfitting_score', 'training_time']\n",
    "print(comparison_df[display_cols].to_string(index=False, float_format='%.4f'))\n",
    "\n",
    "# Overfitting analysis\n",
    "print(\"\\nüîç OVERFITTING ANALYSIS:\")\n",
    "print(\"=\"*60)\n",
    "overfitting_models = comparison_df[comparison_df['is_overfitting']]\n",
    "if len(overfitting_models) > 0:\n",
    "    print(f\"‚ö†Ô∏è Models showing overfitting (train-test difference > {CONFIG['overfitting_threshold']*100}%):\")\n",
    "    for _, model in overfitting_models.iterrows():\n",
    "        print(f\"   ‚Ä¢ {model['Model']}: {model['overfitting_score']*100:.2f}% difference\")\n",
    "else:\n",
    "    print(\"‚úÖ No models show significant overfitting!\")\n",
    "\n",
    "# Complex model analysis\n",
    "complex_models = ['Neural Network', 'Random Forest', 'Gradient Boosting', 'SVM']\n",
    "print(\"\\nüìà Complex Model Analysis:\")\n",
    "for model_name in complex_models:\n",
    "    if model_name in comparison_df['Model'].values:\n",
    "        model_data = comparison_df[comparison_df['Model'] == model_name].iloc[0]\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"   ‚Ä¢ Test Accuracy: {model_data['balanced_accuracy']:.4f}\")\n",
    "        print(f\"   ‚Ä¢ Overfitting: {'Yes' if model_data['is_overfitting'] else 'No'} ({model_data['overfitting_score']*100:.2f}%)\")\n",
    "        print(f\"   ‚Ä¢ Training Time: {model_data['training_time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåç 9. Multi-Station Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform station-wise analysis if station information is available\n",
    "if station_info is not None and CONFIG['station_column'] in df.columns:\n",
    "    print(\"\\nüåç STATION-WISE ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Get best model\n",
    "    best_model_name = comparison_df.iloc[0]['Model']\n",
    "    best_model = best_models[best_model_name]\n",
    "    best_pred = results[best_model_name]['predictions']\n",
    "    \n",
    "    # Calculate per-station accuracy\n",
    "    unique_stations = np.unique(station_test)\n",
    "    station_accuracies = {}\n",
    "    high_accuracy_stations = []\n",
    "    \n",
    "    for station in unique_stations:\n",
    "        station_mask = station_test == station\n",
    "        if np.sum(station_mask) > 10:  # Only analyze stations with sufficient test samples\n",
    "            station_acc = accuracy_score(y_test[station_mask], best_pred[station_mask])\n",
    "            station_accuracies[station] = {\n",
    "                'accuracy': station_acc,\n",
    "                'n_samples': np.sum(station_mask),\n",
    "                'n_correct': np.sum(y_test[station_mask] == best_pred[station_mask])\n",
    "            }\n",
    "            \n",
    "            if station_acc >= CONFIG['high_accuracy_threshold']:\n",
    "                high_accuracy_stations.append((station, station_acc))\n",
    "    \n",
    "    # Report high-accuracy stations\n",
    "    print(f\"\\nüèÜ Stations with ‚â•{CONFIG['high_accuracy_threshold']*100:.0f}% accuracy:\")\n",
    "    if high_accuracy_stations:\n",
    "        high_accuracy_stations.sort(key=lambda x: x[1], reverse=True)\n",
    "        for station, acc in high_accuracy_stations[:10]:\n",
    "            n_samples = station_accuracies[station]['n_samples']\n",
    "            print(f\"   ‚Ä¢ {station}: {acc*100:.2f}% ({n_samples} samples)\")\n",
    "        if len(high_accuracy_stations) > 10:\n",
    "            print(f\"   ... and {len(high_accuracy_stations) - 10} more stations\")\n",
    "    else:\n",
    "        print(\"   No stations achieved this accuracy threshold.\")\n",
    "    \n",
    "    # Overall station statistics\n",
    "    acc_values = [v['accuracy'] for v in station_accuracies.values()]\n",
    "    print(f\"\\nüìä Station Accuracy Statistics:\")\n",
    "    print(f\"   ‚Ä¢ Mean accuracy: {np.mean(acc_values)*100:.2f}%\")\n",
    "    print(f\"   ‚Ä¢ Std deviation: {np.std(acc_values)*100:.2f}%\")\n",
    "    print(f\"   ‚Ä¢ Min accuracy: {np.min(acc_values)*100:.2f}%\")\n",
    "    print(f\"   ‚Ä¢ Max accuracy: {np.max(acc_values)*100:.2f}%\")\n",
    "    print(f\"   ‚Ä¢ Stations analyzed: {len(station_accuracies)}\")\n",
    "    \n",
    "    # Store station results\n",
    "    station_results_df = pd.DataFrame([\n",
    "        {'station': k, 'accuracy': v['accuracy'], 'n_samples': v['n_samples']} \n",
    "        for k, v in station_accuracies.items()\n",
    "    ]).sort_values('accuracy', ascending=False)\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Station-wise analysis not available (no station information in dataset)\")\n",
    "    station_results_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà 10. Comprehensive Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "best_model = best_models[best_model_name]\n",
    "best_pred = results[best_model_name]['predictions']\n",
    "\n",
    "# Create subplots\n",
    "if station_results_df is not None:\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Model Performance Comparison', 'Overall Confusion Matrix',\n",
    "                       'Station Accuracy Heatmap', 'Overfitting Analysis'),\n",
    "        specs=[[{'type': 'bar'}, {'type': 'heatmap'}],\n",
    "               [{'type': 'bar'}, {'type': 'scatter'}]]\n",
    "    )\n",
    "else:\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Model Performance Comparison', 'Overall Confusion Matrix',\n",
    "                       'Training Time Comparison', 'Overfitting Analysis'),\n",
    "        specs=[[{'type': 'bar'}, {'type': 'heatmap'}],\n",
    "               [{'type': 'bar'}, {'type': 'scatter'}]]\n",
    "    )\n",
    "\n",
    "# 1. Model Performance Comparison\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=comparison_df['Model'],\n",
    "        y=comparison_df['balanced_accuracy'],\n",
    "        name='Balanced Accuracy',\n",
    "        marker_color='lightblue'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# 2. Confusion Matrix\n",
    "cm = confusion_matrix(y_test, best_pred)\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=cm,\n",
    "        text=cm,\n",
    "        texttemplate='%{text}',\n",
    "        colorscale='Blues',\n",
    "        showscale=False\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# 3. Station Accuracy or Training Time\n",
    "if station_results_df is not None:\n",
    "    # Station accuracy bar chart\n",
    "    top_stations = station_results_df.head(20)\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=top_stations['station'],\n",
    "            y=top_stations['accuracy'],\n",
    "            name='Station Accuracy',\n",
    "            marker_color='lightgreen'\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "else:\n",
    "    # Training time comparison\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=comparison_df['Model'],\n",
    "            y=comparison_df['training_time'],\n",
    "            name='Training Time (s)',\n",
    "            marker_color='lightcoral'\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "# 4. Overfitting Analysis\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=comparison_df['train_balanced_accuracy'],\n",
    "        y=comparison_df['balanced_accuracy'],\n",
    "        mode='markers+text',\n",
    "        text=comparison_df['Model'],\n",
    "        textposition='top center',\n",
    "        marker=dict(size=10),\n",
    "        name='Models'\n",
    "    ),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "# Add diagonal line for overfitting plot\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[0, 1],\n",
    "        y=[0, 1],\n",
    "        mode='lines',\n",
    "        line=dict(dash='dash', color='red'),\n",
    "        name='No Overfitting Line',\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_xaxes(title_text='Model', row=1, col=1)\n",
    "fig.update_yaxes(title_text='Balanced Accuracy', row=1, col=1)\n",
    "fig.update_xaxes(title_text='Predicted', row=1, col=2)\n",
    "fig.update_yaxes(title_text='Actual', row=1, col=2)\n",
    "\n",
    "if station_results_df is not None:\n",
    "    fig.update_xaxes(title_text='Station', row=2, col=1, tickangle=45)\n",
    "    fig.update_yaxes(title_text='Accuracy', row=2, col=1)\n",
    "else:\n",
    "    fig.update_xaxes(title_text='Model', row=2, col=1)\n",
    "    fig.update_yaxes(title_text='Training Time (s)', row=2, col=1)\n",
    "\n",
    "fig.update_xaxes(title_text='Train Balanced Accuracy', row=2, col=2)\n",
    "fig.update_yaxes(title_text='Test Balanced Accuracy', row=2, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    title_text='Comprehensive Model Analysis',\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance visualization\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False).head(20)\n",
    "    \n",
    "    fig = px.bar(importance_df, y='feature', x='importance', orientation='h',\n",
    "                title=f'Top 20 Feature Importances - {best_model_name}')\n",
    "    fig.update_layout(height=600)\n",
    "    fig.show()\n",
    "elif hasattr(best_model, 'coef_'):\n",
    "    # For linear models\n",
    "    coef = best_model.coef_[0] if len(best_model.coef_.shape) > 1 else best_model.coef_\n",
    "    \n",
    "    coef_df = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'coefficient': np.abs(coef)\n",
    "    }).sort_values('coefficient', ascending=False).head(20)\n",
    "    \n",
    "    fig = px.bar(coef_df, y='feature', x='coefficient', orientation='h',\n",
    "                title=f'Top 20 Feature Coefficients - {best_model_name}')\n",
    "    fig.update_layout(height=600)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã 11. Comprehensive Analysis Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã COMPREHENSIVE ANALYSIS REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Station Accuracy Report\n",
    "print(\"\\n1Ô∏è‚É£ STATION ACCURACY ANALYSIS\")\n",
    "print(\"-\"*60)\n",
    "if station_results_df is not None:\n",
    "    high_acc_stations = station_results_df[station_results_df['accuracy'] >= CONFIG['high_accuracy_threshold']]\n",
    "    print(f\"Stations achieving ‚â•{CONFIG['high_accuracy_threshold']*100:.0f}% accuracy: {len(high_acc_stations)}\")\n",
    "    if len(high_acc_stations) > 0:\n",
    "        print(\"\\nTop performing stations:\")\n",
    "        for _, row in high_acc_stations.head(5).iterrows():\n",
    "            print(f\"  ‚Ä¢ {row['station']}: {row['accuracy']*100:.2f}% ({row['n_samples']} samples)\")\n",
    "else:\n",
    "    print(\"Station-wise analysis not available.\")\n",
    "\n",
    "# 2. Overfitting Detection\n",
    "print(\"\\n2Ô∏è‚É£ OVERFITTING DETECTION\")\n",
    "print(\"-\"*60)\n",
    "for _, model in comparison_df.iterrows():\n",
    "    if model['is_overfitting']:\n",
    "        print(f\"‚ö†Ô∏è {model['Model']}:\")\n",
    "        print(f\"   ‚Ä¢ Train accuracy: {model['train_balanced_accuracy']*100:.2f}%\")\n",
    "        print(f\"   ‚Ä¢ Test accuracy: {model['balanced_accuracy']*100:.2f}%\")\n",
    "        print(f\"   ‚Ä¢ Difference: {model['overfitting_score']*100:.2f}%\")\n",
    "\n",
    "# Check complex models specifically\n",
    "print(\"\\nComplex model overfitting analysis:\")\n",
    "complex_models = ['Neural Network', 'Random Forest', 'Gradient Boosting']\n",
    "complex_overfitting = comparison_df[comparison_df['Model'].isin(complex_models) & comparison_df['is_overfitting']]\n",
    "if len(complex_overfitting) > 0:\n",
    "    print(f\"‚ö†Ô∏è {len(complex_overfitting)} out of {len(complex_models)} complex models show overfitting\")\n",
    "else:\n",
    "    print(\"‚úÖ No complex models show significant overfitting\")\n",
    "\n",
    "# 3. Model Recommendation\n",
    "print(\"\\n3Ô∏è‚É£ MODEL RECOMMENDATION\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Score models based on multiple criteria\n",
    "recommendation_scores = []\n",
    "for _, model in comparison_df.iterrows():\n",
    "    score = 0\n",
    "    reasons = []\n",
    "    \n",
    "    # Balanced accuracy (40% weight)\n",
    "    acc_score = model['balanced_accuracy'] * 40\n",
    "    score += acc_score\n",
    "    \n",
    "    # Computational efficiency (30% weight)\n",
    "    if model['training_time'] < np.percentile(comparison_df['training_time'], 25):\n",
    "        score += 30\n",
    "        reasons.append(\"fast training\")\n",
    "    elif model['training_time'] < np.percentile(comparison_df['training_time'], 50):\n",
    "        score += 20\n",
    "        reasons.append(\"moderate training time\")\n",
    "    elif model['training_time'] < np.percentile(comparison_df['training_time'], 75):\n",
    "        score += 10\n",
    "    \n",
    "    # Overfitting risk (30% weight)\n",
    "    if not model['is_overfitting']:\n",
    "        score += 30\n",
    "        reasons.append(\"no overfitting\")\n",
    "    elif model['overfitting_score'] < 0.03:\n",
    "        score += 20\n",
    "        reasons.append(\"minimal overfitting\")\n",
    "    elif model['overfitting_score'] < 0.05:\n",
    "        score += 10\n",
    "        reasons.append(\"moderate overfitting\")\n",
    "    \n",
    "    recommendation_scores.append({\n",
    "        'Model': model['Model'],\n",
    "        'Score': score,\n",
    "        'Accuracy': model['balanced_accuracy'],\n",
    "        'Training_Time': model['training_time'],\n",
    "        'Overfitting': model['overfitting_score'],\n",
    "        'Reasons': ', '.join(reasons)\n",
    "    })\n",
    "\n",
    "recommendation_df = pd.DataFrame(recommendation_scores).sort_values('Score', ascending=False)\n",
    "\n",
    "print(\"\\nüèÜ TOP RECOMMENDATIONS:\")\n",
    "for i, row in recommendation_df.head(3).iterrows():\n",
    "    print(f\"\\n{i+1}. {row['Model']} (Score: {row['Score']:.1f}/100)\")\n",
    "    print(f\"   ‚Ä¢ Balanced Accuracy: {row['Accuracy']*100:.2f}%\")\n",
    "    print(f\"   ‚Ä¢ Training Time: {row['Training_Time']:.2f}s\")\n",
    "    print(f\"   ‚Ä¢ Overfitting: {row['Overfitting']*100:.2f}%\")\n",
    "    print(f\"   ‚Ä¢ Strengths: {row['Reasons']}\")\n",
    "\n",
    "# Final recommendation\n",
    "best_overall = recommendation_df.iloc[0]\n",
    "print(f\"\\nüìå FINAL RECOMMENDATION: {best_overall['Model']}\")\n",
    "print(f\"   This model provides the best balance of accuracy ({best_overall['Accuracy']*100:.2f}%),\")\n",
    "print(f\"   computational efficiency ({best_overall['Training_Time']:.2f}s), and\")\n",
    "print(f\"   generalization ability (overfitting: {best_overall['Overfitting']*100:.2f}%).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ 12. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "output_dir = output_path / f'pleasant_weather_analysis_{timestamp}'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save comprehensive results\n",
    "summary = {\n",
    "    'analysis_date': datetime.now().strftime('%Y-%m-%d %H:%M'),\n",
    "    'configuration': CONFIG,\n",
    "    'dataset_info': {\n",
    "        'total_samples': len(df),\n",
    "        'features_used': len(selected_features),\n",
    "        'target_variable': target_col,\n",
    "        'stations_analyzed': len(np.unique(station_test)) if station_info is not None else 'N/A'\n",
    "    },\n",
    "    'model_comparison': comparison_df.to_dict('records'),\n",
    "    'best_model': {\n",
    "        'name': best_model_name,\n",
    "        'parameters': results[best_model_name]['best_params'],\n",
    "        'metrics': results[best_model_name]['metrics']\n",
    "    },\n",
    "    'neural_network_best_config': results.get('Neural Network', {}).get('best_params', {}),\n",
    "    'high_accuracy_stations': len(high_accuracy_stations) if 'high_accuracy_stations' in locals() else 0,\n",
    "    'overfitting_models': list(comparison_df[comparison_df['is_overfitting']]['Model'].values),\n",
    "    'recommendation': {\n",
    "        'model': best_overall['Model'],\n",
    "        'score': best_overall['Score'],\n",
    "        'reasons': best_overall['Reasons']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save JSON summary\n",
    "with open(output_dir / 'analysis_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=4)\n",
    "\n",
    "# Save detailed DataFrames\n",
    "comparison_df.to_csv(output_dir / 'model_comparison.csv', index=False)\n",
    "if station_results_df is not None:\n",
    "    station_results_df.to_csv(output_dir / 'station_accuracies.csv', index=False)\n",
    "\n",
    "# Save best model\n",
    "joblib.dump(best_models[best_model_name], output_dir / f'best_model_{best_model_name.lower().replace(\" \", \"_\")}.pkl')\n",
    "\n",
    "# Save all models if requested\n",
    "save_all = input(\"\\nüíæ Save all trained models? (y/n): \").strip().lower()\n",
    "if save_all == 'y':\n",
    "    models_dir = output_dir / 'all_models'\n",
    "    models_dir.mkdir(exist_ok=True)\n",
    "    for name, model in best_models.items():\n",
    "        joblib.dump(model, models_dir / f\"{name.lower().replace(' ', '_')}.pkl\")\n",
    "\n",
    "# Save predictions\n",
    "predictions_df = pd.DataFrame({\n",
    "    'actual': y_test,\n",
    "    'predicted': results[best_model_name]['predictions']\n",
    "})\n",
    "if station_info is not None:\n",
    "    predictions_df['station'] = station_test\n",
    "predictions_df.to_csv(output_dir / 'predictions.csv', index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Results saved to: {output_dir}\")\n",
    "print(f\"\\nüéâ Analysis complete!\")\n",
    "print(f\"   Best model: {best_model_name}\")\n",
    "print(f\"   Recommended model: {best_overall['Model']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 13. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä FINAL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüéØ Problem Type: {problem_type.upper()}\")\n",
    "print(f\"   Target Variable: {target_col}\")\n",
    "if 'selected_station' in locals() and selected_station:\n",
    "    print(f\"   Selected Station: {selected_station}\")\n",
    "    print(f\"   Mode: {'Single Station' if single_station_mode else 'Multi-Station'}\")\n",
    "print(f\"   Features Used: {len(selected_features)}\")\n",
    "print(f\"   Training Samples: {X_train.shape[0]:,}\")\n",
    "print(f\"   Test Samples: {X_test.shape[0]:,}\")\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name}\")\n",
    "for metric, value in results[best_model_name]['metrics'].items():\n",
    "    if metric != 'training_time':\n",
    "        print(f\"   {metric}: {value:.4f}\")\n",
    "\n",
    "print(f\"\\nüìÅ All results saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
