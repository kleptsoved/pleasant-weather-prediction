{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66fd2bff-f991-4ae5-92a1-47c20d5c1257",
   "metadata": {},
   "source": [
    "# Machine Learning Model Training and Evaluation\n",
    "This notebook extends the data loading template to train and evaluate multiple ML models using the pre-scaled dataset.\n",
    "\n",
    "## üìÅ Expected Project Structure\n",
    "```plaintext\n",
    "Your Project/\n",
    "‚îú‚îÄ‚îÄ 01_project_management/\n",
    "‚îú‚îÄ‚îÄ 02_data/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Original_data/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ Processed_data/    ‚Üê Pre-scaled data should be here\n",
    "‚îú‚îÄ‚îÄ 03_notebooks/          ‚Üê Run notebooks from here\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ src/               ‚Üê Custom modules live here\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ file_handler.py\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ml_model_training.ipynb  ‚Üê This notebook\n",
    "‚îú‚îÄ‚îÄ 04_analyses/\n",
    "‚îî‚îÄ‚îÄ 05_results/\n",
    "```\n",
    "\n",
    "## üéØ Objectives\n",
    "1. Load pre-scaled dataset\n",
    "2. Split data into training and test sets\n",
    "3. Train multiple ML models with cross-validation\n",
    "4. Optimize hyperparameters using GridSearchCV\n",
    "5. Compare model performance and select the best model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": [
    "## üìö 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import sys\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src folder to Python path\n",
    "sys.path.append(str(Path.cwd() / 'src'))\n",
    "\n",
    "# Import custom modules\n",
    "from file_handler import setup_paths, load_data_with_detection_enhanced\n",
    "from data_exporter import export_data_interactive, quick_export\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    cross_val_score, \n",
    "    GridSearchCV,\n",
    "    StratifiedKFold\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    classification_report, \n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve\n",
    ")\n",
    "\n",
    "# ML Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üìÖ Analysis date: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-data-section",
   "metadata": {},
   "source": [
    "## üì• 2. Load Pre-Scaled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up project paths interactively\n",
    "# You'll be prompted to select the folder containing your pre-scaled data\n",
    "project_root, input_path, output_path = setup_paths()\n",
    "\n",
    "# Load the pre-scaled dataset\n",
    "print(\"\\nüìä Loading pre-scaled dataset...\")\n",
    "df = load_data_with_detection_enhanced(input_path)[0]\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(f\"\\nüìã Dataset Information:\")\n",
    "print(f\"   Shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "print(f\"   Memory usage: {df.memory_usage().sum() / 1024**2:.2f} MB\")\n",
    "print(f\"\\n   Column types:\")\n",
    "print(df.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36602dcf-fab5-4e12-8f46-167c061ccb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduce your dataset to a single year\n",
    "dfyear = df[df['DATE'].astype(str).str.contains('1960')] #<-----INSERT YEAR HERE\n",
    "dfyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f09bd7e-98c9-4ebb-843d-2f959c04a463",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfyear.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fa2151-ff56-4fd5-91d0-f3dcdeb2d28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the DATE and MONTH data as those numbers are not scaled with the rest.\n",
    "notempyear = dfyear.drop(['DATE','MONTH'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepare-data-section",
   "metadata": {},
   "source": [
    "## üîÑ 3. Prepare Data for Machine Learning\n",
    "\n",
    "### Define Target Variable\n",
    "First, we need to identify or create a target variable for classification. Adjust this based on project's specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac6605e-cb2e-4403-9719-520f0579e1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Comprehensive column analysis\n",
    "print(\"üìä COLUMN ANALYSIS AND GROUPING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create a DataFrame to display column information\n",
    "column_info = pd.DataFrame({\n",
    "    'Column Name': notempyear.columns,\n",
    "    'Data Type': notempyear.dtypes.values,\n",
    "    'Non-Null Count': notempyear.count().values,\n",
    "    'Null %': (notempyear.isnull().sum() / len(notempyear) * 100).round(2).values\n",
    "})\n",
    "\n",
    "# Display all columns in a nice table format\n",
    "print(\"\\nüìã All Available Columns:\")\n",
    "print(column_info.to_string(index=False))\n",
    "\n",
    "# Group columns by common patterns\n",
    "print(\"\\n\\nüîç COLUMN GROUPING BY PATTERNS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Define pattern groups\n",
    "patterns = {\n",
    "    'Statistical Measures': ['mean', 'max', 'min', 'std', 'avg', 'median', 'sum', 'count'],\n",
    "    'Temperature Related': ['temp', 'temperature', 'celsius', 'fahrenheit'],\n",
    "    'Time Related': ['date', 'time', 'year', 'month', 'day', 'hour', 'minute'],\n",
    "    'Percentage/Ratio': ['pct', 'percent', 'ratio', 'rate'],\n",
    "    'Categorical Likely': ['id', 'name', 'type', 'category', 'class', 'group'],\n",
    "    'Measurement Values': ['value', 'amount', 'quantity', 'level', 'size', 'volume']\n",
    "}\n",
    "\n",
    "# Group columns by patterns\n",
    "grouped_columns = {}\n",
    "unmatched_columns = list(notempyear.columns)\n",
    "\n",
    "for group_name, keywords in patterns.items():\n",
    "    matched = []\n",
    "    for col in notempyear.columns:\n",
    "        col_lower = col.lower()\n",
    "        if any(keyword in col_lower for keyword in keywords):\n",
    "            matched.append(col)\n",
    "            if col in unmatched_columns:\n",
    "                unmatched_columns.remove(col)\n",
    "    if matched:\n",
    "        grouped_columns[group_name] = matched\n",
    "\n",
    "# Display grouped columns with numbers for selection\n",
    "print(\"\\nColumn Groups Found:\")\n",
    "group_list = list(grouped_columns.keys())\n",
    "for i, (group_name, cols) in enumerate(grouped_columns.items(), 1):\n",
    "    print(f\"\\n{i}. üè∑Ô∏è {group_name} ({len(cols)} columns):\")\n",
    "    # Show first 5 columns as examples\n",
    "    for col in cols[:5]:\n",
    "        dtype = notempyear[col].dtype\n",
    "        print(f\"   ‚Ä¢ {col} ({dtype})\")\n",
    "    if len(cols) > 5:\n",
    "        print(f\"   ... and {len(cols) - 5} more\")\n",
    "\n",
    "if unmatched_columns:\n",
    "    group_list.append(\"Other Columns\")\n",
    "    print(f\"\\n{len(group_list)}. ‚ùì Other Columns ({len(unmatched_columns)}):\")\n",
    "    for col in unmatched_columns[:5]:\n",
    "        print(f\"   ‚Ä¢ {col} ({notempyear[col].dtype})\")\n",
    "    if len(unmatched_columns) > 5:\n",
    "        print(f\"   ... and {len(unmatched_columns) - 5} more\")\n",
    "    grouped_columns[\"Other Columns\"] = unmatched_columns\n",
    "\n",
    "# INTERACTIVE SELECTION SECTION\n",
    "print(\"\\n\\nüéØ SELECT COLUMNS FOR ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüìù Instructions:\")\n",
    "print(\"1. First, select which column groups to include\")\n",
    "print(\"2. Then, optionally filter by specific keywords within those groups\")\n",
    "print(\"3. Finally, choose a column for target variable creation\")\n",
    "\n",
    "# Prompt for group selection\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Which column groups do you want to include?\")\n",
    "print(\"Enter numbers separated by commas (e.g., 1,3,5) or 'all' for all groups:\")\n",
    "for i, group in enumerate(group_list, 1):\n",
    "    num_cols = len(grouped_columns.get(group, []))\n",
    "    print(f\"  {i}. {group} ({num_cols} columns)\")\n",
    "\n",
    "# Get user input for groups\n",
    "user_groups = input(\"\\nüëâ Your selection: \").strip()\n",
    "\n",
    "# Process group selection\n",
    "selected_groups = []\n",
    "if user_groups.lower() == 'all':\n",
    "    selected_groups = group_list\n",
    "    selected_columns = list(notempyear.columns)\n",
    "else:\n",
    "    try:\n",
    "        group_indices = [int(x.strip()) - 1 for x in user_groups.split(',')]\n",
    "        selected_groups = [group_list[i] for i in group_indices if 0 <= i < len(group_list)]\n",
    "        selected_columns = []\n",
    "        for group in selected_groups:\n",
    "            selected_columns.extend(grouped_columns.get(group, []))\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è Invalid input. Using all columns.\")\n",
    "        selected_groups = group_list\n",
    "        selected_columns = list(notempyear.columns)\n",
    "\n",
    "print(f\"\\n‚úÖ Selected groups: {', '.join(selected_groups)}\")\n",
    "print(f\"   Total columns selected: {len(selected_columns)}\")\n",
    "\n",
    "# Prompt for keyword filtering\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Do you want to filter columns by specific keywords?\")\n",
    "print(\"For example: 'mean' for only mean values, 'temp,mean' for temperature means\")\n",
    "print(\"Or press Enter to skip filtering\")\n",
    "\n",
    "keyword_filter = input(\"\\nüëâ Enter keywords (comma-separated) or press Enter: \").strip()\n",
    "\n",
    "# Apply keyword filtering if provided\n",
    "if keyword_filter:\n",
    "    keywords = [k.strip().lower() for k in keyword_filter.split(',')]\n",
    "    filtered_columns = []\n",
    "    for col in selected_columns:\n",
    "        col_lower = col.lower()\n",
    "        if all(keyword in col_lower for keyword in keywords):\n",
    "            filtered_columns.append(col)\n",
    "    \n",
    "    if filtered_columns:\n",
    "        print(f\"\\n‚úÖ Filtered to {len(filtered_columns)} columns containing all keywords: {keywords}\")\n",
    "        selected_columns = filtered_columns\n",
    "        # Show filtered columns\n",
    "        print(\"\\nFiltered columns:\")\n",
    "        for col in filtered_columns[:10]:\n",
    "            print(f\"   ‚Ä¢ {col}\")\n",
    "        if len(filtered_columns) > 10:\n",
    "            print(f\"   ... and {len(filtered_columns) - 10} more\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è No columns found with all keywords: {keywords}. Using original selection.\")\n",
    "\n",
    "# TARGET VARIABLE CREATION\n",
    "print(\"\\n\\nüéØ CREATE TARGET VARIABLE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Filter for numerical columns only\n",
    "numerical_selected = [col for col in selected_columns if notempyear[col].dtype in ['int64', 'float64']]\n",
    "\n",
    "if numerical_selected:\n",
    "    print(f\"\\nFound {len(numerical_selected)} numerical columns for potential targets:\")\n",
    "    for i, col in enumerate(numerical_selected[:15], 1):  # Show up to 15\n",
    "        print(f\"  {i}. {col}\")\n",
    "    if len(numerical_selected) > 15:\n",
    "        print(f\"  ... and {len(numerical_selected) - 15} more\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"Target variable creation options:\")\n",
    "    print(\"‚Ä¢ Press Enter to use ALL filtered columns as features (no target creation)\")\n",
    "    print(\"‚Ä¢ Enter a number to select a specific column for target creation\")\n",
    "    print(\"‚Ä¢ Enter 'skip' to proceed without creating a target\")\n",
    "    \n",
    "    target_choice = input(\"\\nüëâ Your choice: \").strip()\n",
    "    \n",
    "    # Process target selection\n",
    "    if target_choice == '':  # Enter pressed - use all columns as features\n",
    "        print(f\"\\n‚úÖ Using all {len(selected_columns)} filtered columns as features\")\n",
    "        print(\"   No target variable created - suitable for unsupervised learning\")\n",
    "        target_column = None\n",
    "        feature_columns = selected_columns.copy()\n",
    "        \n",
    "    elif target_choice.lower() == 'skip':\n",
    "        print(\"\\n‚úÖ Skipping target creation\")\n",
    "        target_column = None\n",
    "        feature_columns = selected_columns.copy()\n",
    "        \n",
    "    else:  # Number entered - create target from specific column\n",
    "        try:\n",
    "            target_idx = int(target_choice) - 1\n",
    "            if 0 <= target_idx < len(numerical_selected):\n",
    "                target_column = numerical_selected[target_idx]\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Invalid number. Using first column.\")\n",
    "                target_column = numerical_selected[0]\n",
    "        except:\n",
    "            print(f\"‚ö†Ô∏è Invalid input. Using first column.\")\n",
    "            target_column = numerical_selected[0]\n",
    "        \n",
    "        # Create target variable\n",
    "        print(f\"\\n‚úÖ Using column for target: {target_column}\")\n",
    "        \n",
    "        # Ask for target type\n",
    "        print(\"\\nHow should the target be created?\")\n",
    "        print(\"  1. Binary (above/below median)\")\n",
    "        print(\"  2. Binary (above/below mean)\")\n",
    "        print(\"  3. Multi-class (3 equal groups)\")\n",
    "        print(\"  4. Multi-class (5 equal groups)\")\n",
    "        \n",
    "        target_type = input(\"\\nüëâ Your choice (1-4, default=1): \").strip()\n",
    "        \n",
    "        if target_type == '2':\n",
    "            threshold = notempyear[target_column].mean()\n",
    "            notempyear['target'] = (notempyear[target_column] > threshold).astype(int)\n",
    "            print(f\"\\n‚úÖ Created binary target (above/below mean: {threshold:.2f})\")\n",
    "        elif target_type == '3':\n",
    "            notempyear['target'] = pd.qcut(notempyear[target_column], q=3, labels=[0,1,2], duplicates='drop')\n",
    "            print(f\"\\n‚úÖ Created 3-class target (equal groups)\")\n",
    "        elif target_type == '4':\n",
    "            notempyear['target'] = pd.qcut(notempyear[target_column], q=5, labels=[0,1,2,3,4], duplicates='drop')\n",
    "            print(f\"\\n‚úÖ Created 5-class target (equal groups)\")\n",
    "        else:  # Default: binary median\n",
    "            threshold = notempyear[target_column].median()\n",
    "            notempyear['target'] = (notempyear[target_column] > threshold).astype(int)\n",
    "            print(f\"\\n‚úÖ Created binary target (above/below median: {threshold:.2f})\")\n",
    "        \n",
    "        print(f\"   Class distribution: {notempyear['target'].value_counts().sort_index().to_dict()}\")\n",
    "        \n",
    "        # Define feature columns (exclude target and source column)\n",
    "        feature_columns = [col for col in selected_columns if col not in ['target', target_column]]\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No numerical columns found in selection.\")\n",
    "    target_column = None\n",
    "    feature_columns = selected_columns.copy()\n",
    "\n",
    "# Look for temperature columns (for compatibility)\n",
    "temp_columns = [col for col in notempyear.columns if 'temp' in col.lower()]\n",
    "\n",
    "# SUMMARY\n",
    "print(\"\\n\\nüìå ANALYSIS CONFIGURATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"‚úì Selected column groups: {', '.join(selected_groups)}\")\n",
    "if keyword_filter:\n",
    "    print(f\"‚úì Keyword filters applied: {keyword_filter}\")\n",
    "print(f\"‚úì Total columns for analysis: {len(feature_columns)} features\")\n",
    "if 'target' in notempyear.columns:\n",
    "    print(f\"‚úì Target variable created from: {target_column}\")\n",
    "    print(f\"‚úì Target type: {notempyear['target'].nunique()} classes\")\n",
    "else:\n",
    "    print(f\"‚úì No target variable - ready for unsupervised learning\")\n",
    "\n",
    "print(\"\\nüìä VARIABLES READY FOR NEXT CELLS:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"‚Ä¢ notempyear - DataFrame {'with target column' if 'target' in notempyear.columns else '(no target)'}\")\n",
    "print(f\"‚Ä¢ feature_columns - List of {len(feature_columns)} selected features\")\n",
    "if target_column:\n",
    "    print(f\"‚Ä¢ target_column = '{target_column}'\")\n",
    "else:\n",
    "    print(f\"‚Ä¢ target_column = None\")\n",
    "print(f\"‚Ä¢ temp_columns - Temperature columns found: {len(temp_columns)}\")\n",
    "\n",
    "# Save configuration for reference\n",
    "config_summary = {\n",
    "    'selected_groups': selected_groups,\n",
    "    'keyword_filter': keyword_filter if keyword_filter else None,\n",
    "    'feature_count': len(feature_columns),\n",
    "    'target_column': target_column,\n",
    "    'target_classes': notempyear['target'].nunique() if 'target' in notempyear.columns else None,\n",
    "    'analysis_type': 'supervised' if 'target' in notempyear.columns else 'unsupervised'\n",
    "}\n",
    "\n",
    "print(\"\\nüíæ Configuration saved in 'config_summary' dictionary\")\n",
    "print(f\"   Analysis type: {config_summary['analysis_type']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61385413-2287-4899-ae92-f7900084d112",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = notempyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare-target",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create a binary target variable based on temperature\n",
    "# We should modify this based on specific prediction task\n",
    "\n",
    "# Check available columns\n",
    "print(\"üìã Available columns:\")\n",
    "print(df.columns.tolist()[:20], \"...\")  # Show first 20 columns\n",
    "\n",
    "# Example: Create binary target (modify based on your needs)\n",
    "# This example creates a target based on whether it's above/below median temperature\n",
    "# Replace this with your actual target variable\n",
    "\n",
    "# Look for temperature columns\n",
    "temp_columns = [col for col in df.columns if 'temp' in col.lower()]\n",
    "print(f\"\\nüå°Ô∏è Temperature columns found: {temp_columns}\")\n",
    "\n",
    "if temp_columns:\n",
    "    # Use the first temperature column found\n",
    "    target_column = temp_columns[0]\n",
    "    median_temp = df[target_column].median()\n",
    "    df['target'] = (df[target_column] > median_temp).astype(int)\n",
    "    print(f\"\\n‚úÖ Created binary target based on {target_column}\")\n",
    "    print(f\"   Class distribution: {df['target'].value_counts().to_dict()}\")\n",
    "    \n",
    "    # Remove the original temperature column from features\n",
    "    feature_columns = [col for col in df.columns if col not in ['target', target_column]]\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No temperature columns found. Please create your target variable manually.\")\n",
    "    # Manual target creation example:\n",
    "    # df['target'] = your_target_creation_logic_here\n",
    "    # feature_columns = [col for col in df.columns if col != 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = df[feature_columns]\n",
    "y = df['target']\n",
    "\n",
    "print(f\"\\nüìä Feature matrix shape: {X.shape}\")\n",
    "print(f\"üéØ Target vector shape: {y.shape}\")\n",
    "print(f\"\\nüìà Target distribution:\")\n",
    "print(y.value_counts(normalize=True).round(3))\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÇÔ∏è Data split completed:\")\n",
    "print(f\"   Training set: {X_train.shape[0]:,} samples\")\n",
    "print(f\"   Test set: {X_test.shape[0]:,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "models-section",
   "metadata": {},
   "source": [
    "## ü§ñ 4. Define Models and Hyperparameter Grids\n",
    "\n",
    "We'll use efficient parameter grids optimized for consumer-grade laptops.\n",
    "\n",
    "### Models included:\n",
    "- **Logistic Regression**: Linear model for baseline performance\n",
    "- **Decision Tree**: Single tree for interpretability\n",
    "- **Random Forest**: Ensemble of trees for better accuracy\n",
    "- **Gradient Boosting**: Sequential ensemble that builds trees to correct previous errors\n",
    "- **Support Vector Machine**: For complex non-linear boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models and their parameter grids\n",
    "# Using smaller grids for efficiency on consumer laptops\n",
    "\n",
    "models = {\n",
    "    'Logistic Regression': {\n",
    "        'model': LogisticRegression(max_iter=1000, random_state=42),\n",
    "        'params': {\n",
    "            'C': [0.01, 0.1, 1, 10],\n",
    "            'penalty': ['l2'],\n",
    "            'solver': ['lbfgs', 'liblinear']\n",
    "        }\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'model': DecisionTreeClassifier(random_state=42),\n",
    "        'params': {\n",
    "            'max_depth': [3, 5, 7, 10],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [5, 10, 15],\n",
    "            'min_samples_split': [2, 5],\n",
    "            'min_samples_leaf': [1, 2]\n",
    "        }\n",
    "    },\n",
    "    'Support Vector Machine': {\n",
    "        'model': SVC(random_state=42, probability=True),\n",
    "        'params': {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'kernel': ['rbf', 'linear'],\n",
    "            'gamma': ['scale', 'auto']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Models and parameter grids defined!\")\n",
    "print(f\"\\nüìã Models to train: {list(models.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-section",
   "metadata": {},
   "source": [
    "## üèÉ‚Äç‚ôÇÔ∏è 5. Train Models with Cross-Validation and GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize results storage\n",
    "results = {}\n",
    "best_models = {}\n",
    "\n",
    "# Define cross-validation strategy\n",
    "cv_folds = 5\n",
    "cv_strategy = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "\n",
    "print(f\"üîÑ Starting model training with {cv_folds}-fold cross-validation...\\n\")\n",
    "\n",
    "# Train each model\n",
    "for model_name, model_info in models.items():\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"ü§ñ Training {model_name}...\")\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # Perform GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model_info['model'],\n",
    "        param_grid=model_info['params'],\n",
    "        cv=cv_strategy,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit the model\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Store the best model\n",
    "    best_models[model_name] = grid_search.best_estimator_\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    y_pred_proba = grid_search.predict_proba(X_test)[:, 1] if hasattr(grid_search.best_estimator_, 'predict_proba') else None\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    cv_scores = cross_val_score(grid_search.best_estimator_, X_train, y_train, cv=cv_strategy)\n",
    "    \n",
    "    # Store results\n",
    "    results[model_name] = {\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'best_cv_score': grid_search.best_score_,\n",
    "        'cv_scores': cv_scores,\n",
    "        'test_accuracy': accuracy,\n",
    "        'predictions': y_pred,\n",
    "        'pred_proba': y_pred_proba,\n",
    "        'training_time': (datetime.now() - start_time).total_seconds()\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n‚úÖ {model_name} training completed!\")\n",
    "    print(f\"   Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"   Best CV score: {grid_search.best_score_:.4f}\")\n",
    "    print(f\"   Test accuracy: {accuracy:.4f}\")\n",
    "    print(f\"   Training time: {results[model_name]['training_time']:.2f} seconds\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"‚úÖ All models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison-section",
   "metadata": {},
   "source": [
    "## üìä 6. Compare Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_data = []\n",
    "for model_name, result in results.items():\n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'CV Mean Score': result['cv_scores'].mean(),\n",
    "        'CV Std': result['cv_scores'].std(),\n",
    "        'Test Accuracy': result['test_accuracy'],\n",
    "        'Training Time (s)': result['training_time']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('Test Accuracy', ascending=False)\n",
    "\n",
    "print(\"üìä Model Performance Comparison:\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Identify best model\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name} (Test Accuracy: {comparison_df.iloc[0]['Test Accuracy']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Test Accuracy Comparison\n",
    "ax1 = axes[0, 0]\n",
    "bars1 = ax1.bar(comparison_df['Model'], comparison_df['Test Accuracy'])\n",
    "ax1.set_title('Model Test Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Test Accuracy')\n",
    "ax1.set_ylim(0, 1.1)\n",
    "for bar, acc in zip(bars1, comparison_df['Test Accuracy']):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{acc:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# 2. CV Score with Error Bars\n",
    "ax2 = axes[0, 1]\n",
    "ax2.errorbar(comparison_df['Model'], comparison_df['CV Mean Score'], \n",
    "             yerr=comparison_df['CV Std'], fmt='o-', capsize=5, capthick=2)\n",
    "ax2.set_title('Cross-Validation Scores (with std)', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('CV Score')\n",
    "ax2.set_ylim(0, 1.1)\n",
    "\n",
    "# 3. Training Time Comparison\n",
    "ax3 = axes[1, 0]\n",
    "bars3 = ax3.bar(comparison_df['Model'], comparison_df['Training Time (s)'])\n",
    "ax3.set_title('Training Time Comparison', fontsize=14, fontweight='bold')\n",
    "ax3.set_ylabel('Training Time (seconds)')\n",
    "for bar, time in zip(bars3, comparison_df['Training Time (s)']):\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "             f'{time:.1f}s', ha='center', va='bottom')\n",
    "\n",
    "# 4. Accuracy vs Training Time Trade-off\n",
    "ax4 = axes[1, 1]\n",
    "scatter = ax4.scatter(comparison_df['Training Time (s)'], comparison_df['Test Accuracy'], \n",
    "                      s=200, alpha=0.6)\n",
    "for idx, row in comparison_df.iterrows():\n",
    "    ax4.annotate(row['Model'], (row['Training Time (s)'], row['Test Accuracy']), \n",
    "                 xytext=(5, 5), textcoords='offset points')\n",
    "ax4.set_title('Accuracy vs Training Time Trade-off', fontsize=14, fontweight='bold')\n",
    "ax4.set_xlabel('Training Time (seconds)')\n",
    "ax4.set_ylabel('Test Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ensemble-comparison",
   "metadata": {},
   "source": [
    "### üìä Ensemble Methods Comparison\n",
    "\n",
    "Let's specifically compare Random Forest vs Gradient Boosting performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-ensembles",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare ensemble methods if both were trained\n",
    "ensemble_models = ['Random Forest', 'Gradient Boosting']\n",
    "if all(model in results for model in ensemble_models):\n",
    "    print(\"\\nüå≤ ENSEMBLE METHODS COMPARISON\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create comparison\n",
    "    ensemble_comparison = comparison_df[comparison_df['Model'].isin(ensemble_models)]\n",
    "    \n",
    "    for _, row in ensemble_comparison.iterrows():\n",
    "        print(f\"\\n{row['Model']}:\")\n",
    "        print(f\"  - Test Accuracy: {row['Test Accuracy']:.4f}\")\n",
    "        print(f\"  - CV Score: {row['CV Mean Score']:.4f} (¬±{row['CV Std']:.4f})\")\n",
    "        print(f\"  - Training Time: {row['Training Time (s)']:.2f}s\")\n",
    "    \n",
    "    # Visualize ensemble comparison\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    ensemble_data = ensemble_comparison.set_index('Model')\n",
    "    x = range(len(ensemble_models))\n",
    "    \n",
    "    ax1.bar(x, ensemble_data['Test Accuracy'], alpha=0.7, label='Test Accuracy')\n",
    "    ax1.bar(x, ensemble_data['CV Mean Score'], alpha=0.7, label='CV Score')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(ensemble_models)\n",
    "    ax1.set_ylabel('Score')\n",
    "    ax1.set_title('Ensemble Methods: Accuracy Comparison')\n",
    "    ax1.legend()\n",
    "    ax1.set_ylim(0, 1.1)\n",
    "    \n",
    "    # Training time vs accuracy trade-off\n",
    "    ax2.scatter(ensemble_data['Training Time (s)'], ensemble_data['Test Accuracy'], \n",
    "                s=300, alpha=0.7)\n",
    "    for model, row in ensemble_data.iterrows():\n",
    "        ax2.annotate(model, (row['Training Time (s)'], row['Test Accuracy']), \n",
    "                     xytext=(5, 5), textcoords='offset points')\n",
    "    ax2.set_xlabel('Training Time (seconds)')\n",
    "    ax2.set_ylabel('Test Accuracy')\n",
    "    ax2.set_title('Ensemble Methods: Efficiency vs Performance')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Explain the difference\n",
    "    print(\"\\nüìö Key Differences:\")\n",
    "    print(\"  ‚Ä¢ Random Forest: Builds trees in parallel (faster)\")\n",
    "    print(\"  ‚Ä¢ Gradient Boosting: Builds trees sequentially (often more accurate)\")\n",
    "    print(\"  ‚Ä¢ Random Forest: Less prone to overfitting\")\n",
    "    print(\"  ‚Ä¢ Gradient Boosting: Can achieve higher accuracy with tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "best-model-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model\n",
    "best_model = best_models[best_model_name]\n",
    "best_result = results[best_model_name]\n",
    "\n",
    "print(f\"üèÜ Detailed Analysis of {best_model_name}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nüìã Classification Report:\")\n",
    "print(classification_report(y_test, best_result['predictions']))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, best_result['predictions'])\n",
    "print(\"\\nüìä Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-best-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix and ROC curve for best model\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# 1. Confusion Matrix Heatmap\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1)\n",
    "ax1.set_title(f'Confusion Matrix - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Predicted')\n",
    "ax1.set_ylabel('Actual')\n",
    "\n",
    "# 2. ROC Curve (if probability predictions available)\n",
    "if best_result['pred_proba'] is not None:\n",
    "    fpr, tpr, _ = roc_curve(y_test, best_result['pred_proba'])\n",
    "    auc_score = roc_auc_score(y_test, best_result['pred_proba'])\n",
    "    \n",
    "    ax2.plot(fpr, tpr, 'b-', linewidth=2, label=f'ROC Curve (AUC = {auc_score:.3f})')\n",
    "    ax2.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "    ax2.set_xlabel('False Positive Rate')\n",
    "    ax2.set_ylabel('True Positive Rate')\n",
    "    ax2.set_title(f'ROC Curve - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "    ax2.legend(loc='lower right')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "else:\n",
    "    ax2.text(0.5, 0.5, 'ROC Curve not available\\nfor this model', \n",
    "             ha='center', va='center', transform=ax2.transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature-importance-section",
   "metadata": {},
   "source": [
    "## üìà 8. Feature Importance Analysis (if applicable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for tree-based models\n",
    "if best_model_name in ['Decision Tree', 'Random Forest']:\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Display top 20 features\n",
    "    print(f\"\\nüìä Top 20 Most Important Features ({best_model_name}):\")\n",
    "    print(feature_importance.head(20).to_string(index=False))\n",
    "    \n",
    "    # Visualize feature importance\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    top_features = feature_importance.head(20)\n",
    "    plt.barh(top_features['feature'], top_features['importance'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title(f'Top 20 Feature Importances - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "elif best_model_name == 'Logistic Regression':\n",
    "    # Get coefficients for logistic regression\n",
    "    coefficients = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'coefficient': best_model.coef_[0]\n",
    "    })\n",
    "    coefficients['abs_coefficient'] = coefficients['coefficient'].abs()\n",
    "    coefficients = coefficients.sort_values('abs_coefficient', ascending=False)\n",
    "    \n",
    "    print(f\"\\nüìä Top 20 Most Important Features ({best_model_name}):\")\n",
    "    print(coefficients.head(20)[['feature', 'coefficient']].to_string(index=False))\n",
    "    \n",
    "    # Visualize coefficients\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    top_coef = coefficients.head(20)\n",
    "    colors = ['red' if x < 0 else 'blue' for x in top_coef['coefficient']]\n",
    "    plt.barh(top_coef['feature'], top_coef['coefficient'], color=colors)\n",
    "    plt.xlabel('Coefficient Value')\n",
    "    plt.title(f'Top 20 Feature Coefficients - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"\\nüìä Feature importance not directly available for {best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-results-section",
   "metadata": {},
   "source": [
    "## üíæ 9. Save Results and Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare results summary\n",
    "results_summary = {\n",
    "    'analysis_date': datetime.now().strftime('%Y-%m-%d %H:%M'),\n",
    "    'dataset_shape': df.shape,\n",
    "    'train_size': X_train.shape[0],\n",
    "    'test_size': X_test.shape[0],\n",
    "    'model_comparison': comparison_df.to_dict('records'),\n",
    "    'best_model': {\n",
    "        'name': best_model_name,\n",
    "        'parameters': results[best_model_name]['best_params'],\n",
    "        'test_accuracy': results[best_model_name]['test_accuracy'],\n",
    "        'cv_mean_score': results[best_model_name]['cv_scores'].mean(),\n",
    "        'cv_std_score': results[best_model_name]['cv_scores'].std()\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save results summary as JSON\n",
    "import json\n",
    "results_filename = f\"ml_results_{datetime.now().strftime('%Y%m%d_%H%M')}.json\"\n",
    "results_path = output_path / results_filename\n",
    "\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results_summary, f, indent=4)\n",
    "\n",
    "print(f\"‚úÖ Results summary saved to: {results_path}\")\n",
    "\n",
    "# Save the best model\n",
    "import joblib\n",
    "model_filename = f\"best_model_{best_model_name.lower().replace(' ', '_')}_{datetime.now().strftime('%Y%m%d_%H%M')}.pkl\"\n",
    "model_path = output_path / model_filename\n",
    "\n",
    "joblib.dump(best_model, model_path)\n",
    "print(f\"‚úÖ Best model saved to: {model_path}\")\n",
    "\n",
    "# Save predictions\n",
    "predictions_df = pd.DataFrame({\n",
    "    'actual': y_test,\n",
    "    'predicted': best_result['predictions']\n",
    "})\n",
    "if best_result['pred_proba'] is not None:\n",
    "    predictions_df['probability'] = best_result['pred_proba']\n",
    "\n",
    "predictions_filename = f\"predictions_{best_model_name.lower().replace(' ', '_')}_{datetime.now().strftime('%Y%m%d_%H%M')}.csv\"\n",
    "predictions_df.to_csv(output_path / predictions_filename, index=False)\n",
    "print(f\"‚úÖ Predictions saved to: {output_path / predictions_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusions-section",
   "metadata": {},
   "source": [
    "## üéØ 10. Conclusions and Next Steps\n",
    "\n",
    "### Summary of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä FINAL ANALYSIS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüéØ Best Model: {best_model_name}\")\n",
    "print(f\"   - Test Accuracy: {results[best_model_name]['test_accuracy']:.4f}\")\n",
    "print(f\"   - CV Mean Score: {results[best_model_name]['cv_scores'].mean():.4f} (¬±{results[best_model_name]['cv_scores'].std():.4f})\")\n",
    "print(f\"   - Training Time: {results[best_model_name]['training_time']:.2f} seconds\")\n",
    "print(f\"\\nüîß Optimal Parameters:\")\n",
    "for param, value in results[best_model_name]['best_params'].items():\n",
    "    print(f\"   - {param}: {value}\")\n",
    "\n",
    "print(\"\\nüìà All Models Performance Ranking:\")\n",
    "for idx, row in comparison_df.iterrows():\n",
    "    print(f\"   {idx+1}. {row['Model']}: {row['Test Accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\nüí° Recommendations for Next Steps:\")\n",
    "print(\"   1. Consider ensemble methods combining top models\")\n",
    "print(\"   2. Perform feature engineering to improve performance\")\n",
    "print(\"   3. Collect more data if possible\")\n",
    "print(\"   4. Try deep learning approaches for complex patterns\")\n",
    "print(\"   5. Deploy the best model for real-world testing\")\n",
    "\n",
    "print(\"\\n‚úÖ Analysis completed successfully!\")\n",
    "print(f\"üìÅ All results saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fa6995-d3cc-41ba-922e-5bb914dc17c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
