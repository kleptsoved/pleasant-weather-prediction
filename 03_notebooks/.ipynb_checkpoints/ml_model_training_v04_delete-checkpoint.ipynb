{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Universal ML Model Training - Classification & Regression\n",
    "\n",
    "This notebook automatically detects whether your problem is classification or regression and applies appropriate models and metrics.\n",
    "\n",
    "## üìÅ Expected Structure\n",
    "```\n",
    "Your Project/\n",
    "‚îú‚îÄ‚îÄ 02_data/Processed_data/    ‚Üê Pre-scaled data\n",
    "‚îú‚îÄ‚îÄ 03_notebooks/              ‚Üê Run from here\n",
    "‚îî‚îÄ‚îÄ 05_results/                ‚Üê Output files\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "# ML imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold, StratifiedKFold\n",
    "from sklearn.metrics import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, f_regression, mutual_info_classif, mutual_info_regression\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso, ElasticNet, SGDClassifier, SGDRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.offline as pyo\n",
    "pyo.init_notebook_mode(connected=True)\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üìÖ Analysis date: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• 2. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the module (adjust path as needed if not in sys.path)\n",
    "import sys\n",
    "sys.path.append('./src')  # Optional: adapt if running outside src\n",
    "from file_handler import setup_paths, load_data_with_detection_enhanced\n",
    "\n",
    "# 1. Interactive project path setup (choose input/output folder)\n",
    "project_root, input_path, output_path = setup_paths()\n",
    "\n",
    "# 2. File selection and loading (choose file & format)\n",
    "df, filename = load_data_with_detection_enhanced(input_path)\n",
    "\n",
    "print(f\"\\nüìä Dataset loaded: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "print(f\"   Memory usage: {df.memory_usage().sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# --- Handle missing data  ---\n",
    "missing_pct = df.isnull().sum() / len(df) * 100\n",
    "cols_to_drop = missing_pct[missing_pct > 50].index\n",
    "if len(cols_to_drop) > 0:\n",
    "    df = df.drop(columns=cols_to_drop)\n",
    "    print(f\"   Dropped {len(cols_to_drop)} columns with >50% missing data\")\n",
    "\n",
    "# Fill remaining missing values for numeric columns\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÖ 3. Time Period Selection (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for date columns\n",
    "date_cols = [col for col in df.columns if 'date' in col.lower()]\n",
    "if date_cols:\n",
    "    date_col = date_cols[0]\n",
    "    df['_year'] = pd.to_datetime(df[date_col], format='%Y%m%d').dt.year\n",
    "    year_counts = df['_year'].value_counts().sort_index()\n",
    "    \n",
    "    print(\"\\nüìÖ Available years:\")\n",
    "    for year, count in year_counts.items():\n",
    "        print(f\"  {year}: {count:,} records\")\n",
    "    \n",
    "    time_selection = input(\"\\nüëâ Enter year/range/all/last5: \").strip().lower()\n",
    "    \n",
    "    if time_selection == 'all':\n",
    "        pass\n",
    "    elif time_selection == 'last5':\n",
    "        df = df[df['_year'] >= df['_year'].max() - 4]\n",
    "    elif '-' in time_selection:\n",
    "        start, end = map(int, time_selection.split('-'))\n",
    "        df = df[(df['_year'] >= start) & (df['_year'] <= end)]\n",
    "    elif time_selection.isdigit():\n",
    "        df = df[df['_year'] == int(time_selection)]\n",
    "    \n",
    "    df = df.drop('_year', axis=1)\n",
    "    print(f\"   Selected dataset size: {len(df):,} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ 4. Feature and Target Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Group columns by pattern\n",
    "patterns = {\n",
    "    'Statistical': ['mean', 'max', 'min', 'std', 'avg'],\n",
    "    'Temperature': ['temp', 'temperature'],\n",
    "    'Weather': ['humid', 'pressure', 'wind', 'rain'],\n",
    "    'Time': ['date', 'year', 'month', 'day']\n",
    "}\n",
    "\n",
    "grouped_cols = {}\n",
    "for group, keywords in patterns.items():\n",
    "    cols = [c for c in numeric_cols if any(k in c.lower() for k in keywords)]\n",
    "    if cols:\n",
    "        grouped_cols[group] = cols\n",
    "\n",
    "print(\"\\nüéØ Feature Groups:\")\n",
    "for i, (group, cols) in enumerate(grouped_cols.items(), 1):\n",
    "    print(f\"  {i}. {group} ({len(cols)} columns)\")\n",
    "\n",
    "selection = input(\"\\nüëâ Select groups (e.g., 1,3) or 'all': \").strip()\n",
    "if selection.lower() == 'all':\n",
    "    selected_features = numeric_cols\n",
    "else:\n",
    "    selected_features = []\n",
    "    for idx in selection.split(','):\n",
    "        group_name = list(grouped_cols.keys())[int(idx.strip())-1]\n",
    "        selected_features.extend(grouped_cols[group_name])\n",
    "\n",
    "print(f\"‚úÖ Selected {len(selected_features)} features\")\n",
    "\n",
    "# Keyword filtering (optional)\n",
    "keyword_filter = input(\"\\nüëâ Filter by keywords (optional, press Enter to skip): \").strip()\n",
    "if keyword_filter:\n",
    "    keywords = [k.strip().lower() for k in keyword_filter.split(',')]\n",
    "    filtered_columns = [col for col in selected_features \n",
    "                       if all(kw in col.lower() for kw in keywords)]\n",
    "    if filtered_columns:\n",
    "        selected_features = filtered_columns\n",
    "        print(f\"‚úÖ Filtered to {len(selected_features)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target variable creation\n",
    "print(\"\\nüéØ Target Variable Selection:\")\n",
    "print(\"1. Use existing column\")\n",
    "print(\"2. Create binary target (threshold)\")\n",
    "print(\"3. Create multi-class target (binning)\")\n",
    "\n",
    "target_option = input(\"\\nüëâ Your choice (1-3): \").strip()\n",
    "\n",
    "# Show available columns\n",
    "print(\"\\nAvailable columns:\")\n",
    "for i, col in enumerate(selected_features[:20], 1):\n",
    "    stats = df[col].describe()[['mean', '50%', 'std']]\n",
    "    print(f\"  {i}. {col} (mean={stats['mean']:.2f}, median={stats['50%']:.2f})\")\n",
    "\n",
    "col_idx = int(input(\"\\nüëâ Select column number: \")) - 1\n",
    "target_col = selected_features[col_idx]\n",
    "selected_features.remove(target_col)\n",
    "\n",
    "# Create target based on option\n",
    "if target_option == '1':\n",
    "    df['target'] = df[target_col]\n",
    "    # Detect problem type\n",
    "    n_unique = df['target'].nunique()\n",
    "    if n_unique <= 20 or df['target'].dtype == 'object':\n",
    "        problem_type = 'classification'\n",
    "    else:\n",
    "        problem_type = 'regression'\n",
    "elif target_option == '2':\n",
    "    threshold = df[target_col].median()\n",
    "    df['target'] = (df[target_col] > threshold).astype(int)\n",
    "    problem_type = 'classification'\n",
    "else:\n",
    "    n_classes = int(input(\"\\nüëâ Number of classes (3-10): \"))\n",
    "    df['target'] = pd.qcut(df[target_col], q=n_classes, labels=range(n_classes), duplicates='drop')\n",
    "    problem_type = 'classification'\n",
    "\n",
    "print(f\"\\n‚úÖ Problem type detected: {problem_type.upper()}\")\n",
    "print(f\"   Target variable: {target_col}\")\n",
    "print(f\"   Unique values: {df['target'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì 5. Manual Gradient Descent Implementation\n",
    "\n",
    "This section demonstrates gradient descent optimization from scratch, showing how ML algorithms learn parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we should run gradient descent demo\n",
    "# Initialize results dictionaries if they don't exist\n",
    "if 'results' not in locals():\n",
    "    results = {}\n",
    "if 'best_models' not in locals():\n",
    "    best_models = {}\n",
    "\n",
    "if problem_type == 'regression' and len(selected_features) >= 1:\n",
    "    print(\"\\nüéì GRADIENT DESCENT\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"We'll use a simple linear regression with visualization.\\n\")\n",
    "    print(\"üìù Note: This implementation uses only ONE feature for educational purposes,\")\n",
    "    print(\"   while the sklearn models will use ALL selected features.\\n\")\n",
    "    \n",
    "    # Select feature for demonstration\n",
    "    print(\"Select a feature for gradient descent visualization:\")\n",
    "    for i, feat in enumerate(selected_features[:20], 1):\n",
    "        corr = df[feat].corr(df['target'])\n",
    "        print(f\"  {i}. {feat} (correlation with target: {corr:.3f})\")\n",
    "    \n",
    "    feat_idx = int(input(\"\\nüëâ Select feature number (or 0 to skip): \"))\n",
    "    \n",
    "    if feat_idx > 0:\n",
    "        selected_feature = selected_features[feat_idx - 1]\n",
    "        run_gradient_descent = True\n",
    "    else:\n",
    "        run_gradient_descent = False\n",
    "else:\n",
    "    run_gradient_descent = False\n",
    "    print(\"\\nüí° Gradient descent demo is available for regression problems.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'run_gradient_descent' in locals() and run_gradient_descent:\n",
    "    # Prepare data for gradient descent\n",
    "    X_gd = df[selected_feature].values.reshape(-1, 1)\n",
    "    y_gd = df['target'].values\n",
    "    \n",
    "    # Standardize the data for better convergence\n",
    "    X_mean, X_std = X_gd.mean(), X_gd.std()\n",
    "    y_mean, y_std = y_gd.mean(), y_gd.std()\n",
    "    X_norm = (X_gd - X_mean) / X_std\n",
    "    y_norm = (y_gd - y_mean) / y_std\n",
    "    \n",
    "    # Add bias term (intercept)\n",
    "    X_norm = np.c_[np.ones(X_norm.shape[0]), X_norm]\n",
    "    \n",
    "    print(f\"\\nüìä Using feature: {selected_feature}\")\n",
    "    print(f\"   Data points: {len(X_norm)}\")\n",
    "    print(f\"   Feature range: [{X_gd.min():.2f}, {X_gd.max():.2f}]\")\n",
    "    print(f\"   Target range: [{y_gd.min():.2f}, {y_gd.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'run_gradient_descent' in locals() and run_gradient_descent:\n",
    "    \"\"\"\n",
    "    GRADIENT DESCENT IMPLEMENTATION\n",
    "    \n",
    "    This is a manual implementation of gradient descent for linear regression.\n",
    "    The algorithm iteratively updates parameters (theta) to minimize the cost function.\n",
    "    \n",
    "    Key concepts:\n",
    "    - Cost Function: J(Œ∏) = (1/2m) * Œ£(h(x) - y)¬≤ where h(x) = Œ∏‚ÇÄ + Œ∏‚ÇÅ*x\n",
    "    - Gradient: ‚àÇJ/‚àÇŒ∏‚±º = (1/m) * Œ£(h(x) - y) * x‚±º\n",
    "    - Update Rule: Œ∏‚±º := Œ∏‚±º - Œ± * ‚àÇJ/‚àÇŒ∏‚±º\n",
    "    \"\"\"\n",
    "    \n",
    "    def compute_cost(X, y, theta):\n",
    "        \"\"\"\n",
    "        Compute the cost (Mean Squared Error) for given parameters.\n",
    "        \n",
    "        Args:\n",
    "            X: Feature matrix with bias term (m x 2)\n",
    "            y: Target values (m x 1)\n",
    "            theta: Parameters [Œ∏‚ÇÄ, Œ∏‚ÇÅ] (2 x 1)\n",
    "        \n",
    "        Returns:\n",
    "            cost: Scalar cost value\n",
    "        \"\"\"\n",
    "        m = len(y)\n",
    "        predictions = X.dot(theta)\n",
    "        errors = predictions - y\n",
    "        cost = (1 / (2 * m)) * np.sum(errors ** 2)\n",
    "        return cost\n",
    "    \n",
    "    def gradient_descent(X, y, theta_init, alpha, iterations):\n",
    "        \"\"\"\n",
    "        Perform gradient descent to optimize parameters.\n",
    "        \n",
    "        Args:\n",
    "            X: Feature matrix with bias term\n",
    "            y: Target values\n",
    "            theta_init: Initial parameter values\n",
    "            alpha: Learning rate (step size)\n",
    "            iterations: Number of iterations\n",
    "        \n",
    "        Returns:\n",
    "            theta: Final parameters\n",
    "            cost_history: Cost at each iteration\n",
    "            theta_history: Parameters at each iteration\n",
    "        \"\"\"\n",
    "        m = len(y)\n",
    "        theta = theta_init.copy()\n",
    "        cost_history = []\n",
    "        theta_history = []\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            # Compute predictions\n",
    "            predictions = X.dot(theta)\n",
    "            \n",
    "            # Compute errors\n",
    "            errors = predictions - y\n",
    "            \n",
    "            # Compute gradients\n",
    "            # ‚àÇJ/‚àÇŒ∏‚ÇÄ = (1/m) * Œ£(errors)\n",
    "            # ‚àÇJ/‚àÇŒ∏‚ÇÅ = (1/m) * Œ£(errors * x)\n",
    "            gradients = (1 / m) * X.T.dot(errors)\n",
    "            \n",
    "            # Update parameters\n",
    "            theta = theta - alpha * gradients\n",
    "            \n",
    "            # Store history\n",
    "            cost = compute_cost(X, y, theta)\n",
    "            cost_history.append(cost)\n",
    "            theta_history.append(theta.copy())\n",
    "            \n",
    "            # Print progress every 100 iterations\n",
    "            if i % 100 == 0:\n",
    "                print(f\"   Iteration {i}: Cost = {cost:.6f}, Œ∏‚ÇÄ = {theta[0]:.4f}, Œ∏‚ÇÅ = {theta[1]:.4f}\")\n",
    "        \n",
    "        return theta, cost_history, np.array(theta_history)\n",
    "    \n",
    "    # Interactive parameter selection\n",
    "    print(\"\\n‚öôÔ∏è GRADIENT DESCENT PARAMETERS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Initial theta values\n",
    "    print(\"\\nInitial parameter values (Œ∏‚ÇÄ, Œ∏‚ÇÅ):\")\n",
    "    print(\"  1. Random initialization\")\n",
    "    print(\"  2. Zero initialization\")\n",
    "    print(\"  3. Custom values\")\n",
    "    \n",
    "    init_choice = input(\"\\nüëâ Your choice (1-3): \").strip()\n",
    "    \n",
    "    if init_choice == '1':\n",
    "        theta_init = np.random.randn(2) * 0.5\n",
    "    elif init_choice == '2':\n",
    "        theta_init = np.zeros(2)\n",
    "    else:\n",
    "        theta0 = float(input(\"  Enter Œ∏‚ÇÄ (intercept): \"))\n",
    "        theta1 = float(input(\"  Enter Œ∏‚ÇÅ (slope): \"))\n",
    "        theta_init = np.array([theta0, theta1])\n",
    "    \n",
    "    print(f\"\\n  Initial values: Œ∏‚ÇÄ = {theta_init[0]:.4f}, Œ∏‚ÇÅ = {theta_init[1]:.4f}\")\n",
    "    \n",
    "    # Learning rate\n",
    "    print(\"\\nLearning rate (Œ±) suggestions:\")\n",
    "    print(\"  ‚Ä¢ 0.001: Very slow but stable\")\n",
    "    print(\"  ‚Ä¢ 0.01:  Moderate speed (recommended)\")\n",
    "    print(\"  ‚Ä¢ 0.1:   Fast but may overshoot\")\n",
    "    print(\"  ‚Ä¢ 1.0:   Very fast, risk of divergence\")\n",
    "    \n",
    "    alpha = float(input(\"\\nüëâ Enter learning rate: \"))\n",
    "    \n",
    "    # Number of iterations\n",
    "    iterations = int(input(\"üëâ Number of iterations (e.g., 1000): \"))\n",
    "    \n",
    "    print(\"\\nüöÄ Running gradient descent...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Run gradient descent\n",
    "    theta_final, cost_history, theta_history = gradient_descent(\n",
    "        X_norm, y_norm, theta_init, alpha, iterations\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Gradient descent complete!\")\n",
    "    print(f\"   Final parameters: Œ∏‚ÇÄ = {theta_final[0]:.4f}, Œ∏‚ÇÅ = {theta_final[1]:.4f}\")\n",
    "    print(f\"   Final cost: {cost_history[-1]:.6f}\")\n",
    "    print(f\"   Cost reduction: {cost_history[0]:.6f} ‚Üí {cost_history[-1]:.6f} ({(1 - cost_history[-1]/cost_history[0])*100:.1f}% improvement)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ 6. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = df[selected_features]\n",
    "y = df['target']\n",
    "\n",
    "# Feature selection if too many features\n",
    "if X.shape[1] > 50:\n",
    "    k = min(30, X.shape[1] // 2)\n",
    "    if problem_type == 'classification':\n",
    "        selector = SelectKBest(f_classif, k=k)\n",
    "    else:\n",
    "        selector = SelectKBest(f_regression, k=k)\n",
    "    \n",
    "    X_selected = selector.fit_transform(X, y)\n",
    "    selected_features = X.columns[selector.get_support()].tolist()\n",
    "    X = pd.DataFrame(X_selected, columns=selected_features)\n",
    "    print(f\"\\n‚úÖ Reduced features from {df[selected_features].shape[1]} to {k}\")\n",
    "\n",
    "# Train-test split\n",
    "if problem_type == 'classification':\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "print(f\"\\n‚úÇÔ∏è Data split: {X_train.shape[0]:,} train, {X_test.shape[0]:,} test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'run_gradient_descent' in locals() and run_gradient_descent and 'theta_final' in locals():\n",
    "    # Create 3D visualization of the loss function\n",
    "    print(\"\\nüìä Creating 3D Loss Function Visualization...\")\n",
    "    \n",
    "    # Create grid of theta values for 3D plot\n",
    "    theta0_range = np.linspace(theta_final[0] - 2, theta_final[0] + 2, 50)\n",
    "    theta1_range = np.linspace(theta_final[1] - 2, theta_final[1] + 2, 50)\n",
    "    theta0_grid, theta1_grid = np.meshgrid(theta0_range, theta1_range)\n",
    "    \n",
    "    # Compute cost for each combination\n",
    "    cost_grid = np.zeros_like(theta0_grid)\n",
    "    for i in range(len(theta0_range)):\n",
    "        for j in range(len(theta1_range)):\n",
    "            theta_temp = np.array([theta0_grid[i, j], theta1_grid[i, j]])\n",
    "            cost_grid[i, j] = compute_cost(X_norm, y_norm, theta_temp)\n",
    "    \n",
    "    # Create 3D surface plot\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        specs=[[{'type': 'surface', 'rowspan': 2}, {'type': 'scatter'}],\n",
    "               [None, {'type': 'scatter'}]],\n",
    "        subplot_titles=('3D Loss Function', 'Loss vs Iterations', 'Parameter Evolution')\n",
    "    )\n",
    "    \n",
    "    # 3D Surface\n",
    "    fig.add_trace(\n",
    "        go.Surface(\n",
    "            x=theta0_range,\n",
    "            y=theta1_range,\n",
    "            z=cost_grid,\n",
    "            colorscale='Viridis',\n",
    "            opacity=0.7,\n",
    "            name='Loss Surface'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Add gradient descent path on 3D surface\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=theta_history[:, 0],\n",
    "            y=theta_history[:, 1],\n",
    "            z=cost_history,\n",
    "            mode='lines+markers',\n",
    "            line=dict(color='red', width=4),\n",
    "            marker=dict(size=4),\n",
    "            name='GD Path'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Loss vs Iterations\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=list(range(len(cost_history))),\n",
    "            y=cost_history,\n",
    "            mode='lines',\n",
    "            name='Cost',\n",
    "            line=dict(color='blue')\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Parameter evolution\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=list(range(len(theta_history))),\n",
    "            y=theta_history[:, 0],\n",
    "            mode='lines',\n",
    "            name='Œ∏‚ÇÄ',\n",
    "            line=dict(color='green')\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=list(range(len(theta_history))),\n",
    "            y=theta_history[:, 1],\n",
    "            mode='lines',\n",
    "            name='Œ∏‚ÇÅ',\n",
    "            line=dict(color='orange')\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title='Gradient Descent Visualization',\n",
    "        height=800,\n",
    "        showlegend=True,\n",
    "        legend=dict(\n",
    "            x=0,               # X position, 0 (left) to 1 (right)\n",
    "            y=0,               # Y position, 0 (bottom) to 1 (top)\n",
    "            xanchor='left',    # or 'right', 'center'\n",
    "            yanchor='bottom',  # or 'left', 'middle'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Update axes\n",
    "    fig.update_xaxes(title_text='Œ∏‚ÇÄ', row=1, col=1)\n",
    "    fig.update_yaxes(title_text='Œ∏‚ÇÅ', row=1, col=1)\n",
    "    fig.update_xaxes(title_text='Iteration', row=1, col=2)\n",
    "    fig.update_yaxes(title_text='Cost', row=1, col=2)\n",
    "    fig.update_xaxes(title_text='Iteration', row=2, col=2)\n",
    "    fig.update_yaxes(title_text='Parameter Value', row=2, col=2)\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Create contour plot with gradient descent path\n",
    "    fig2 = go.Figure()\n",
    "    \n",
    "    # Add contour\n",
    "    fig2.add_trace(\n",
    "        go.Contour(\n",
    "            x=theta0_range,\n",
    "            y=theta1_range,\n",
    "            z=cost_grid,\n",
    "            colorscale='Viridis',\n",
    "            showscale=True,\n",
    "            name='Loss Contours'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Add gradient descent path\n",
    "    fig2.add_trace(\n",
    "        go.Scatter(\n",
    "            x=theta_history[:, 0],\n",
    "            y=theta_history[:, 1],\n",
    "            mode='lines+markers',\n",
    "            line=dict(color='red', width=3),\n",
    "            marker=dict(size=8, color='red'),\n",
    "            name='GD Path'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Mark start and end points\n",
    "    fig2.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[theta_init[0]],\n",
    "            y=[theta_init[1]],\n",
    "            mode='markers',\n",
    "            marker=dict(size=15, color='green', symbol='star'),\n",
    "            name='Start'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig2.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[theta_final[0]],\n",
    "            y=[theta_final[1]],\n",
    "            mode='markers',\n",
    "            marker=dict(size=15, color='blue', symbol='star'),\n",
    "            name='End'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig2.update_layout(\n",
    "        title='Gradient Descent Path on Loss Contours',\n",
    "        xaxis_title='Œ∏‚ÇÄ',\n",
    "        yaxis_title='Œ∏‚ÇÅ',\n",
    "        height=600,\n",
    "        width=800,\n",
    "        showlegend=True,\n",
    "        legend=dict(\n",
    "            x=0,               # X position, 0 (left) to 1 (right)\n",
    "            y=0,               # Y position, 0 (bottom) to 1 (top)\n",
    "            xanchor='left',    # or 'right', 'center'\n",
    "            yanchor='bottom',  # or 'left', 'middle'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig2.show()\n",
    "    \n",
    "    # Visualize the fitted line\n",
    "    fig3 = go.Figure()\n",
    "    \n",
    "    # Scatter plot of original data\n",
    "    sample_size = min(500, len(X_gd))  # Sample for better visualization\n",
    "    sample_idx = np.random.choice(len(X_gd), sample_size, replace=False)\n",
    "    \n",
    "    fig3.add_trace(\n",
    "        go.Scatter(\n",
    "            x=X_gd[sample_idx].flatten(),\n",
    "            y=y_gd[sample_idx],\n",
    "            mode='markers',\n",
    "            marker=dict(size=5, opacity=0.5),\n",
    "            name='Data Points'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Fitted line (need to transform back from normalized space)\n",
    "    x_line = np.linspace(X_gd.min(), X_gd.max(), 100)\n",
    "    x_line_norm = (x_line - X_mean) / X_std\n",
    "    X_line_norm = np.c_[np.ones(len(x_line)), x_line_norm]\n",
    "    y_line_norm = X_line_norm.dot(theta_final)\n",
    "    y_line = y_line_norm * y_std + y_mean\n",
    "    \n",
    "    fig3.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x_line,\n",
    "            y=y_line,\n",
    "            mode='lines',\n",
    "            line=dict(color='red', width=3),\n",
    "            name='Fitted Line'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig3.update_layout(\n",
    "        title=f'Gradient Descent Result: {selected_feature} vs {target_col}',\n",
    "        xaxis_title=selected_feature,\n",
    "        yaxis_title=target_col,\n",
    "        height=500,\n",
    "        showlegend=True,\n",
    "        legend=dict(\n",
    "            x=0,               # X position, 0 (left) to 1 (right)\n",
    "            y=0,               # Y position, 0 (bottom) to 1 (top)\n",
    "            xanchor='left',    # or 'right', 'center'\n",
    "            yanchor='bottom',  # or 'left', 'middle'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig3.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\nüìä GRADIENT DESCENT SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nAlgorithm Performance:\")\n",
    "    print(f\"  ‚Ä¢ Initial cost: {cost_history[0]:.6f}\")\n",
    "    print(f\"  ‚Ä¢ Final cost: {cost_history[-1]:.6f}\")\n",
    "    print(f\"  ‚Ä¢ Cost reduction: {(1 - cost_history[-1]/cost_history[0])*100:.2f}%\")\n",
    "    print(f\"  ‚Ä¢ Convergence: {'Yes' if abs(cost_history[-1] - cost_history[-2]) < 1e-6 else 'No'}\")\n",
    "    \n",
    "    print(f\"\\nLearning Insights:\")\n",
    "    if alpha >= 1.0 and cost_history[-1] > cost_history[0]:\n",
    "        print(f\"  ‚ö†Ô∏è Learning rate too high! The cost increased.\")\n",
    "    elif alpha <= 0.001 and iterations < 1000:\n",
    "        print(f\"  ‚ö†Ô∏è Learning rate too low! May need more iterations.\")\n",
    "    else:\n",
    "        print(f\"  ‚úÖ Learning rate appears appropriate.\")\n",
    "    \n",
    "    # Compare with sklearn\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_gd, y_gd)\n",
    "    \n",
    "    print(f\"\\nComparison with sklearn LinearRegression:\")\n",
    "    print(f\"  ‚Ä¢ Our Œ∏‚ÇÄ: {theta_final[0] * y_std - theta_final[1] * y_std * X_mean / X_std + y_mean:.4f}\")\n",
    "    print(f\"  ‚Ä¢ sklearn intercept: {lr.intercept_:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Our Œ∏‚ÇÅ: {theta_final[1] * y_std / X_std:.4f}\")\n",
    "    print(f\"  ‚Ä¢ sklearn coefficient: {lr.coef_[0]:.4f}\")\n",
    "    \n",
    "    # Store gradient descent results for comparison\n",
    "    # Transform predictions back to original scale\n",
    "    X_test_gd = X_test[selected_feature].values.reshape(-1, 1)\n",
    "    X_test_norm = (X_test_gd - X_mean) / X_std\n",
    "    X_test_norm = np.c_[np.ones(X_test_norm.shape[0]), X_test_norm]\n",
    "    y_pred_norm = X_test_norm.dot(theta_final)\n",
    "    y_pred_gd = y_pred_norm * y_std + y_mean\n",
    "    \n",
    "    # Calculate metrics for gradient descent\n",
    "    gd_metrics = {\n",
    "        'r2': r2_score(y_test, y_pred_gd),\n",
    "        'mse': mean_squared_error(y_test, y_pred_gd),\n",
    "        'rmse': np.sqrt(mean_squared_error(y_test, y_pred_gd)),\n",
    "        'mae': mean_absolute_error(y_test, y_pred_gd),\n",
    "        'cv_score': 0.0  # No CV for manual implementation\n",
    "    }\n",
    "    \n",
    "    # Add to results\n",
    "    results['Gradient Descent (Manual)'] = {\n",
    "        'metrics': gd_metrics,\n",
    "        'best_params': {\n",
    "            'learning_rate': alpha,\n",
    "            'iterations': iterations,\n",
    "            'feature': selected_feature\n",
    "        },\n",
    "        'predictions': y_pred_gd\n",
    "    }\n",
    "    \n",
    "    best_models['Gradient Descent (Manual)'] = {\n",
    "        'theta': theta_final,\n",
    "        'feature': selected_feature,\n",
    "        'normalization': {'X_mean': X_mean, 'X_std': X_std, 'y_mean': y_mean, 'y_std': y_std}\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n‚úÖ Gradient Descent results added to model comparison!\")\n",
    "    print(f\"   Test R¬≤: {gd_metrics['r2']:.4f}\")\n",
    "    print(f\"   Test RMSE: {gd_metrics['rmse']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ 7. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models based on problem type\n",
    "if problem_type == 'classification':\n",
    "    # Check class balance\n",
    "    class_props = y_train.value_counts(normalize=True)\n",
    "    is_balanced = class_props.min() >= 0.2\n",
    "    class_weight = None if is_balanced else 'balanced'\n",
    "    \n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000, class_weight=class_weight),\n",
    "        'Decision Tree': DecisionTreeClassifier(max_depth=10, class_weight=class_weight),\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=10, class_weight=class_weight, n_jobs=-1),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, max_depth=5),\n",
    "        'SVM': SVC(kernel='rbf', probability=True, class_weight=class_weight),\n",
    "        'SGD Classifier': SGDClassifier(max_iter=1000, tol=1e-3, class_weight=class_weight, random_state=42)\n",
    "    }\n",
    "    \n",
    "    # Simplified parameter grids\n",
    "    param_grids = {\n",
    "        'Logistic Regression': {'C': [0.1, 1, 10]},\n",
    "        'Decision Tree': {'max_depth': [5, 10, 15], 'min_samples_split': [2, 10]},\n",
    "        'Random Forest': {'n_estimators': [50, 100], 'max_depth': [10, 20]},\n",
    "        'Gradient Boosting': {'n_estimators': [50, 100], 'learning_rate': [0.1, 0.2]},\n",
    "        'SVM': {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto']},\n",
    "        'SGD Classifier': {'alpha': [0.0001, 0.001, 0.01], 'penalty': ['l2', 'l1', 'elasticnet']}\n",
    "    }\n",
    "    \n",
    "    cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scoring = 'balanced_accuracy' if not is_balanced else 'accuracy'\n",
    "    \n",
    "else:  # regression\n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Ridge': Ridge(),\n",
    "        'Lasso': Lasso(max_iter=2000),\n",
    "        'Decision Tree': DecisionTreeRegressor(max_depth=10),\n",
    "        'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=10, n_jobs=-1),\n",
    "        'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, max_depth=5),\n",
    "        'SVR': SVR(kernel='rbf'),\n",
    "        'SGD Regressor': SGDRegressor(max_iter=1000, tol=1e-3, random_state=42)\n",
    "    }\n",
    "    \n",
    "    param_grids = {\n",
    "        'Linear Regression': {},\n",
    "        'Ridge': {'alpha': [0.1, 1, 10]},\n",
    "        'Lasso': {'alpha': [0.01, 0.1, 1]},\n",
    "        'Decision Tree': {'max_depth': [5, 10, 15], 'min_samples_split': [2, 10]},\n",
    "        'Random Forest': {'n_estimators': [50, 100], 'max_depth': [10, 20]},\n",
    "        'Gradient Boosting': {'n_estimators': [50, 100], 'learning_rate': [0.1, 0.2]},\n",
    "        'SVR': {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto']},\n",
    "        'SGD Regressor': {'alpha': [0.0001, 0.001, 0.01], 'penalty': ['l2', 'l1', 'elasticnet']}\n",
    "    }\n",
    "    \n",
    "    cv_strategy = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scoring = 'r2'\n",
    "\n",
    "print(f\"\\nü§ñ Training {len(models)} models for {problem_type}...\")\n",
    "print(f\"   Scoring metric: {scoring}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models\n",
    "# Initialize results - check if gradient descent was already run\n",
    "if 'results' not in locals():\n",
    "    results = {}\n",
    "if 'best_models' not in locals():\n",
    "    best_models = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n Training {name}...\", end=' ')\n",
    "    \n",
    "    # Grid search\n",
    "    grid = GridSearchCV(model, param_grids[name], cv=cv_strategy, scoring=scoring, n_jobs=-1)\n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    # Store best model\n",
    "    best_models[name] = grid.best_estimator_\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = grid.best_estimator_.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    if problem_type == 'classification':\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'balanced_accuracy': balanced_accuracy_score(y_test, y_pred),\n",
    "            'f1_score': f1_score(y_test, y_pred, average='weighted'),\n",
    "            'cv_score': grid.best_score_\n",
    "        }\n",
    "    else:\n",
    "        metrics = {\n",
    "            'r2': r2_score(y_test, y_pred),\n",
    "            'mse': mean_squared_error(y_test, y_pred),\n",
    "            'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "            'mae': mean_absolute_error(y_test, y_pred),\n",
    "            'cv_score': grid.best_score_\n",
    "        }\n",
    "    \n",
    "    results[name] = {\n",
    "        'metrics': metrics,\n",
    "        'best_params': grid.best_params_,\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úì CV Score: {grid.best_score_:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ All models trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 8. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_data = []\n",
    "for name, result in results.items():\n",
    "    row = {'Model': name}\n",
    "    row.update(result['metrics'])\n",
    "    comparison_data.append(row)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Handle different metrics for gradient descent\n",
    "if 'Gradient Descent (Manual)' in comparison_df['Model'].values:\n",
    "    # For gradient descent, CV score is not available, so fill with NaN\n",
    "    comparison_df.loc[comparison_df['Model'] == 'Gradient Descent (Manual)', 'cv_score'] = np.nan\n",
    "\n",
    "# Sort by primary metric\n",
    "if problem_type == 'classification':\n",
    "    comparison_df = comparison_df.sort_values('balanced_accuracy', ascending=False)\n",
    "    primary_metric = 'balanced_accuracy'\n",
    "else:\n",
    "    comparison_df = comparison_df.sort_values('r2', ascending=False)\n",
    "    primary_metric = 'r2'\n",
    "\n",
    "print(\"\\nüìä Model Performance Comparison:\")\n",
    "print(comparison_df.to_string(index=False, float_format='%.4f'))\n",
    "\n",
    "# Get best model (excluding gradient descent for best model selection if it's just a demo)\n",
    "models_for_best = comparison_df[comparison_df['Model'] != 'Gradient Descent (Manual)']\n",
    "if len(models_for_best) > 0:\n",
    "    best_model_name = models_for_best.iloc[0]['Model']\n",
    "else:\n",
    "    best_model_name = comparison_df.iloc[0]['Model']\n",
    "    \n",
    "print(f\"\\nüèÜ Best Model: {best_model_name}\")\n",
    "\n",
    "# Note about gradient descent if it was run\n",
    "if 'Gradient Descent (Manual)' in comparison_df['Model'].values:\n",
    "    gd_rank = comparison_df[comparison_df['Model'] == 'Gradient Descent (Manual)'].index[0] + 1\n",
    "    print(f\"\\nüìù Note: Gradient Descent (Manual) ranked #{gd_rank} - uses only one feature ({results['Gradient Descent (Manual)']['best_params']['feature']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà 9. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison visualization\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=('Model Performance', 'Best Model Analysis'))\n",
    "\n",
    "# Performance comparison\n",
    "fig.add_trace(\n",
    "    go.Bar(x=comparison_df['Model'], y=comparison_df[primary_metric], name=primary_metric),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Best model analysis\n",
    "best_model = best_models[best_model_name]\n",
    "best_pred = results[best_model_name]['predictions']\n",
    "\n",
    "if problem_type == 'classification':\n",
    "    # Confusion matrix for classification\n",
    "    cm = confusion_matrix(y_test, best_pred)\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(z=cm, text=cm, texttemplate='%{text}', colorscale='Blues'),\n",
    "        row=1, col=2\n",
    "    )\n",
    "else:\n",
    "    # Actual vs Predicted for regression\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=y_test, y=best_pred, mode='markers', name='Predictions'),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    # Add diagonal line\n",
    "    min_val = min(y_test.min(), best_pred.min())\n",
    "    max_val = max(y_test.max(), best_pred.max())\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=[min_val, max_val], y=[min_val, max_val], \n",
    "                  mode='lines', name='Perfect Prediction', line=dict(dash='dash')),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "fig.update_layout(height=500, showlegend=True, title_text=f\"Model Analysis - {problem_type.title()}\")\n",
    "fig.show()\n",
    "\n",
    "# Feature importance (if available)\n",
    "if best_model_name == 'Gradient Descent (Manual)':\n",
    "    # For gradient descent, show the single feature coefficient\n",
    "    print(f\"\\nüìä Gradient Descent Coefficient:\")\n",
    "    print(f\"   Feature: {best_models[best_model_name]['feature']}\")\n",
    "    print(f\"   Œ∏‚ÇÅ (slope): {best_models[best_model_name]['theta'][1]:.4f}\")\n",
    "    print(f\"   Œ∏‚ÇÄ (intercept): {best_models[best_model_name]['theta'][0]:.4f}\")\n",
    "elif hasattr(best_model, 'feature_importances_'):\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False).head(15)\n",
    "    \n",
    "    fig = px.bar(importance_df, y='feature', x='importance', orientation='h',\n",
    "                title=f'Top 15 Feature Importances - {best_model_name}')\n",
    "    fig.update_layout(height=500)\n",
    "    fig.show()\n",
    "elif hasattr(best_model, 'coef_'):\n",
    "    # For linear models\n",
    "    if problem_type == 'classification' and len(np.unique(y_train)) == 2:\n",
    "        coef = best_model.coef_[0]\n",
    "    else:\n",
    "        coef = best_model.coef_\n",
    "    \n",
    "    coef_df = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'coefficient': np.abs(coef)\n",
    "    }).sort_values('coefficient', ascending=False).head(15)\n",
    "    \n",
    "    fig = px.bar(coef_df, y='feature', x='coefficient', orientation='h',\n",
    "                title=f'Top 15 Feature Coefficients - {best_model_name}')\n",
    "    fig.update_layout(height=500)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ 10. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = Path('model_results') / datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save results summary\n",
    "summary = {\n",
    "    'analysis_date': datetime.now().strftime('%Y-%m-%d %H:%M'),\n",
    "    'problem_type': problem_type,\n",
    "    'dataset_info': {\n",
    "        'total_samples': len(df),\n",
    "        'features_used': len(selected_features),\n",
    "        'target_variable': target_col\n",
    "    },\n",
    "    'model_comparison': comparison_df.to_dict('records'),\n",
    "    'best_model': {\n",
    "        'name': best_model_name,\n",
    "        'parameters': results[best_model_name]['best_params'],\n",
    "        'metrics': results[best_model_name]['metrics']\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(output_dir / 'results_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=4)\n",
    "\n",
    "# Save models\n",
    "for name, model in best_models.items():\n",
    "    joblib.dump(model, output_dir / f\"{name.lower().replace(' ', '_')}.pkl\")\n",
    "\n",
    "# Save predictions\n",
    "predictions_df = pd.DataFrame({\n",
    "    'actual': y_test,\n",
    "    'predicted': results[best_model_name]['predictions']\n",
    "})\n",
    "predictions_df.to_csv(output_dir / 'predictions.csv', index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Results saved to: {output_dir}\")\n",
    "print(f\"\\nüéâ Analysis complete! Best model: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 11. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä FINAL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüéØ Problem Type: {problem_type.upper()}\")\n",
    "print(f\"   Target Variable: {target_col}\")\n",
    "print(f\"   Features Used: {len(selected_features)}\")\n",
    "print(f\"   Training Samples: {X_train.shape[0]:,}\")\n",
    "print(f\"   Test Samples: {X_test.shape[0]:,}\")\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name}\")\n",
    "for metric, value in results[best_model_name]['metrics'].items():\n",
    "    print(f\"   {metric}: {value:.4f}\")\n",
    "\n",
    "print(f\"\\nüìÅ All results saved to: {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
