{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Enhanced Machine Learning Model Training and Evaluation\n",
    "\n",
    "This notebook provides a comprehensive framework for training and evaluating multiple ML models with advanced features including:\n",
    "- Interactive data selection and exploration\n",
    "- Automated handling of class imbalance\n",
    "- Advanced visualizations (2D and 3D)\n",
    "- Feature importance and selection\n",
    "- Model optimization tracking\n",
    "- Performance comparison and ensemble analysis\n",
    "\n",
    "## üìÅ Expected Project Structure\n",
    "```\n",
    "Your Project/\n",
    "‚îú‚îÄ‚îÄ 02_data/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ Processed_data/             ‚Üê Pre-scaled data\n",
    "‚îú‚îÄ‚îÄ 03_notebooks/                   ‚Üê Run notebooks from here\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ src/                        ‚Üê Custom modules\n",
    "‚îî‚îÄ‚îÄ 05_results/                     ‚Üê Output files\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import sys\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import joblib\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    cross_val_score, \n",
    "    GridSearchCV,\n",
    "    StratifiedKFold,\n",
    "    learning_curve,\n",
    "    validation_curve\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    classification_report, \n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    f1_score,\n",
    "    balanced_accuracy_score,\n",
    "    make_scorer\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import (\n",
    "    SelectKBest, \n",
    "    f_classif,\n",
    "    mutual_info_classif,\n",
    "    RFE\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# ML Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    VotingClassifier\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.offline as pyo\n",
    "\n",
    "# Initialize plotly for notebook\n",
    "pyo.init_notebook_mode(connected=True)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üìÖ Analysis date: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_scaling(df: pd.DataFrame, sample_cols: int = 5) -> Dict:\n",
    "    \"\"\"Check if data appears to be scaled by examining statistical properties.\"\"\"\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    sample_cols = min(sample_cols, len(numeric_cols))\n",
    "    sampled_cols = np.random.choice(numeric_cols, sample_cols, replace=False)\n",
    "    \n",
    "    scaling_info = {\n",
    "        'appears_scaled': True,\n",
    "        'details': {}\n",
    "    }\n",
    "    \n",
    "    for col in sampled_cols:\n",
    "        col_stats = {\n",
    "            'mean': df[col].mean(),\n",
    "            'std': df[col].std(),\n",
    "            'min': df[col].min(),\n",
    "            'max': df[col].max()\n",
    "        }\n",
    "        scaling_info['details'][col] = col_stats\n",
    "        \n",
    "        # Check if data appears to be scaled (common patterns)\n",
    "        if abs(col_stats['mean']) > 10 or col_stats['std'] > 10:\n",
    "            scaling_info['appears_scaled'] = False\n",
    "    \n",
    "    return scaling_info\n",
    "\n",
    "def handle_missing_data(df: pd.DataFrame, strategy: str = 'auto') -> pd.DataFrame:\n",
    "    \"\"\"Handle missing data with various strategies.\"\"\"\n",
    "    missing_info = df.isnull().sum()\n",
    "    missing_cols = missing_info[missing_info > 0]\n",
    "    \n",
    "    if len(missing_cols) == 0:\n",
    "        print(\"‚úÖ No missing data detected!\")\n",
    "        return df\n",
    "    \n",
    "    print(f\"‚ö†Ô∏è Missing data found in {len(missing_cols)} columns:\")\n",
    "    print(missing_cols.head(10))\n",
    "    \n",
    "    if strategy == 'auto':\n",
    "        # Automatic handling based on missing percentage\n",
    "        for col in missing_cols.index:\n",
    "            missing_pct = missing_cols[col] / len(df) * 100\n",
    "            \n",
    "            if missing_pct > 50:\n",
    "                print(f\"  ‚ùå Dropping {col} ({missing_pct:.1f}% missing)\")\n",
    "                df = df.drop(columns=[col])\n",
    "            elif missing_pct > 20:\n",
    "                print(f\"  üìä Filling {col} with median ({missing_pct:.1f}% missing)\")\n",
    "                df[col] = df[col].fillna(df[col].median())\n",
    "            else:\n",
    "                print(f\"  üìà Forward filling {col} ({missing_pct:.1f}% missing)\")\n",
    "                df[col] = df[col].fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def check_class_balance(y: pd.Series) -> Dict:\n",
    "    \"\"\"Check for class imbalance and suggest strategies.\"\"\"\n",
    "    class_counts = y.value_counts()\n",
    "    class_props = y.value_counts(normalize=True)\n",
    "    \n",
    "    imbalance_info = {\n",
    "        'balanced': True,\n",
    "        'class_counts': class_counts.to_dict(),\n",
    "        'class_proportions': class_props.to_dict(),\n",
    "        'minority_class': class_props.idxmin(),\n",
    "        'majority_class': class_props.idxmax(),\n",
    "        'imbalance_ratio': class_props.max() / class_props.min(),\n",
    "        'suggested_strategy': None\n",
    "    }\n",
    "    \n",
    "    # Check for imbalance (less than 20% for any class)\n",
    "    if class_props.min() < 0.2:\n",
    "        imbalance_info['balanced'] = False\n",
    "        \n",
    "        if class_props.min() < 0.1:\n",
    "            imbalance_info['suggested_strategy'] = 'SMOTE or class weights'\n",
    "        else:\n",
    "            imbalance_info['suggested_strategy'] = 'class weights'\n",
    "    \n",
    "    return imbalance_info\n",
    "\n",
    "def create_ensemble_model(best_models: Dict, voting: str = 'soft') -> VotingClassifier:\n",
    "    \"\"\"Create an ensemble from the best performing models.\"\"\"\n",
    "    estimators = [(name, model) for name, model in best_models.items()]\n",
    "    ensemble = VotingClassifier(estimators=estimators, voting=voting)\n",
    "    return ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• 3. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Please select your data file:\n",
      "Enter the full path to your scaled dataset, or press Enter to use file browser\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " üëâ File path:  \n"
     ]
    }
   ],
   "source": [
    "# Interactive file selection\n",
    "print(\"üìÅ Please select your data file:\")\n",
    "print(\"Enter the full path to your scaled dataset, or press Enter to use file browser\")\n",
    "\n",
    "file_path = input(\"\\n üëâ File path: \").strip()\n",
    "\n",
    "if not file_path:\n",
    "    from tkinter import filedialog\n",
    "    import tkinter as tk\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    file_path = filedialog.askopenfilename(\n",
    "        title=\"Select your scaled dataset\",\n",
    "        filetypes=[(\"CSV files\", \"*.csv\"), (\"All files\", \"*.*\")]\n",
    "    )\n",
    "\n",
    "# Load the dataset\n",
    "print(f\"\\nüìä Loading dataset from: {file_path}\")\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display basic information\n",
    "print(f\"\\nüìã Dataset Information:\")\n",
    "print(f\"   Shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "print(f\"   Memory usage: {df.memory_usage().sum() / 1024**2:.2f} MB\")\n",
    "print(f\"\\n   Column types:\")\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "# Check for missing data\n",
    "missing_summary = df.isnull().sum().sum()\n",
    "print(f\"\\n   Missing values: {missing_summary:,} total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if data is scaled\n",
    "print(\"\\nüîç Checking if data appears to be scaled...\")\n",
    "scaling_info = check_scaling(df)\n",
    "\n",
    "if scaling_info['appears_scaled']:\n",
    "    print(\"‚úÖ Data appears to be properly scaled\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Data may not be scaled. Consider scaling before model training.\")\n",
    "    \n",
    "print(\"\\nSample statistics from random columns:\")\n",
    "for col, stats in list(scaling_info['details'].items())[:3]:\n",
    "    print(f\"  {col}: mean={stats['mean']:.3f}, std={stats['std']:.3f}, \"\n",
    "          f\"range=[{stats['min']:.3f}, {stats['max']:.3f}]\")\n",
    "\n",
    "# Handle missing data\n",
    "df = handle_missing_data(df, strategy='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìÖ Interactive Time Period Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for date columns\n",
    "date_cols = [col for col in df.columns if 'date' in col.lower()]\n",
    "if date_cols:\n",
    "    date_col = date_cols[0]\n",
    "    \n",
    "    # Extract years\n",
    "    df['_year'] = pd.to_datetime(df[date_col], format='%Y%m%d').dt.year\n",
    "    year_counts = df['_year'].value_counts().sort_index()\n",
    "    \n",
    "    print(\"\\nüìÖ TIME PERIOD SELECTION\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Date column found: {date_col}\")\n",
    "    print(f\"\\nAvailable years and data points:\")\n",
    "    \n",
    "    # Create a visual representation\n",
    "    fig = px.bar(x=year_counts.index.tolist(), y=year_counts.values.tolist(),\n",
    "                 labels={'x': 'Year', 'y': 'Number of Records'},\n",
    "                 title='Data Distribution by Year')\n",
    "    fig.show()\n",
    "    \n",
    "    # Print year options\n",
    "    for year, count in year_counts.items():\n",
    "        print(f\"  {year}: {count:,} records\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"Options:\")\n",
    "    print(\"  ‚Ä¢ Enter a specific year (e.g., 2020)\")\n",
    "    print(\"  ‚Ä¢ Enter a range (e.g., 2018-2020)\")\n",
    "    print(\"  ‚Ä¢ Enter 'all' to use entire dataset\")\n",
    "    print(\"  ‚Ä¢ Enter 'last5' for last 5 years\")\n",
    "    \n",
    "    time_selection = input(\"\\nüëâ Your selection: \").strip().lower()\n",
    "    \n",
    "    # Process selection\n",
    "    if time_selection == 'all':\n",
    "        print(\"‚úÖ Using entire dataset\")\n",
    "    elif time_selection == 'last5':\n",
    "        last_year = df['_year'].max()\n",
    "        df = df[df['_year'] >= last_year - 4]\n",
    "        print(f\"‚úÖ Using data from {last_year-4} to {last_year}\")\n",
    "    elif '-' in time_selection:\n",
    "        start, end = time_selection.split('-')\n",
    "        df = df[(df['_year'] >= int(start)) & (df['_year'] <= int(end))]\n",
    "        print(f\"‚úÖ Using data from {start} to {end}\")\n",
    "    else:\n",
    "        df = df[df['_year'] == int(time_selection)]\n",
    "        print(f\"‚úÖ Using data from year {time_selection}\")\n",
    "    \n",
    "    # Clean up temporary column\n",
    "    df = df.drop('_year', axis=1)\n",
    "    print(f\"   Final dataset size: {len(df):,} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ 4. Interactive Feature and Target Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive column analysis\n",
    "print(\"\\nüìä COLUMN ANALYSIS AND GROUPING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create column info DataFrame\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "column_info = pd.DataFrame({\n",
    "    'Column Name': numeric_cols,\n",
    "    'Data Type': df[numeric_cols].dtypes.values,\n",
    "    'Non-Null Count': df[numeric_cols].count().values,\n",
    "    'Null %': (df[numeric_cols].isnull().sum() / len(df) * 100).round(2).values,\n",
    "    'Mean': df[numeric_cols].mean().round(3).values,\n",
    "    'Std': df[numeric_cols].std().round(3).values\n",
    "})\n",
    "\n",
    "# Group columns by patterns\n",
    "patterns = {\n",
    "    'Statistical Measures': ['mean', 'max', 'min', 'std', 'avg', 'median', 'sum', 'count'],\n",
    "    'Temperature Related': ['temp', 'temperature', 'celsius', 'fahrenheit'],\n",
    "    'Humidity/Pressure': ['humid', 'pressure', 'precip', 'rain'],\n",
    "    'Wind Related': ['wind', 'gust', 'speed'],\n",
    "    'Time Related': ['date', 'time', 'year', 'month', 'day', 'hour'],\n",
    "    'Location Related': ['station', 'city', 'location', 'lat', 'lon']\n",
    "}\n",
    "\n",
    "grouped_columns = {}\n",
    "unmatched_columns = list(numeric_cols)\n",
    "\n",
    "for group_name, keywords in patterns.items():\n",
    "    matched = []\n",
    "    for col in numeric_cols:\n",
    "        col_lower = col.lower()\n",
    "        if any(keyword in col_lower for keyword in keywords):\n",
    "            matched.append(col)\n",
    "            if col in unmatched_columns:\n",
    "                unmatched_columns.remove(col)\n",
    "    if matched:\n",
    "        grouped_columns[group_name] = matched\n",
    "\n",
    "if unmatched_columns:\n",
    "    grouped_columns[\"Other Columns\"] = unmatched_columns\n",
    "\n",
    "# Display grouped columns\n",
    "print(\"\\nColumn Groups Found:\")\n",
    "for i, (group_name, cols) in enumerate(grouped_columns.items(), 1):\n",
    "    print(f\"\\n{i}. üè∑Ô∏è {group_name} ({len(cols)} columns):\")\n",
    "    for col in cols[:5]:\n",
    "        print(f\"   ‚Ä¢ {col}\")\n",
    "    if len(cols) > 5:\n",
    "        print(f\"   ... and {len(cols) - 5} more\")\n",
    "\n",
    "# Interactive feature selection\n",
    "print(\"\\n\\nüéØ FEATURE SELECTION\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nSelect column groups to include as features:\")\n",
    "print(\"Enter numbers separated by commas (e.g., 1,3,5) or 'all':\")\n",
    "\n",
    "for i, group in enumerate(grouped_columns.keys(), 1):\n",
    "    print(f\"  {i}. {group} ({len(grouped_columns[group])} columns)\")\n",
    "\n",
    "user_groups = input(\"\\nüëâ Your selection: \").strip()\n",
    "\n",
    "# Process selection\n",
    "if user_groups.lower() == 'all':\n",
    "    selected_columns = list(numeric_cols)\n",
    "else:\n",
    "    selected_groups = []\n",
    "    selected_columns = []\n",
    "    try:\n",
    "        indices = [int(x.strip()) - 1 for x in user_groups.split(',')]\n",
    "        group_list = list(grouped_columns.keys())\n",
    "        for idx in indices:\n",
    "            if 0 <= idx < len(group_list):\n",
    "                group = group_list[idx]\n",
    "                selected_groups.append(group)\n",
    "                selected_columns.extend(grouped_columns[group])\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è Invalid input. Using all columns.\")\n",
    "        selected_columns = list(numeric_cols)\n",
    "\n",
    "print(f\"\\n‚úÖ Selected {len(selected_columns)} features\")\n",
    "\n",
    "# Keyword filtering\n",
    "keyword_filter = input(\"\\nüëâ Filter by keywords (optional, press Enter to skip): \").strip()\n",
    "if keyword_filter:\n",
    "    keywords = [k.strip().lower() for k in keyword_filter.split(',')]\n",
    "    filtered_columns = [col for col in selected_columns \n",
    "                       if all(kw in col.lower() for kw in keywords)]\n",
    "    if filtered_columns:\n",
    "        selected_columns = filtered_columns\n",
    "        print(f\"‚úÖ Filtered to {len(selected_columns)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Target Variable Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive target variable creation\n",
    "print(\"\\n\\nüéØ TARGET VARIABLE CREATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nOptions for target variable:\")\n",
    "print(\"1. Select an existing column to predict\")\n",
    "print(\"2. Create a binary target from a column (above/below threshold)\")\n",
    "print(\"3. Create multi-class target (binning)\")\n",
    "print(\"4. Skip (for unsupervised learning)\")\n",
    "\n",
    "target_option = input(\"\\nüëâ Your choice (1-4): \").strip()\n",
    "\n",
    "if target_option == '1':\n",
    "    # Show available columns for target\n",
    "    print(\"\\nAvailable columns for target:\")\n",
    "    for i, col in enumerate(selected_columns[:20], 1):\n",
    "        print(f\"  {i}. {col}\")\n",
    "    if len(selected_columns) > 20:\n",
    "        print(f\"  ... and {len(selected_columns) - 20} more\")\n",
    "    \n",
    "    target_idx = int(input(\"\\nüëâ Select target column number: \")) - 1\n",
    "    target_column = selected_columns[target_idx]\n",
    "    df['target'] = df[target_column]\n",
    "    selected_columns.remove(target_column)\n",
    "    \n",
    "elif target_option == '2':\n",
    "    # Binary target creation\n",
    "    print(\"\\nSelect column for binary target creation:\")\n",
    "    for i, col in enumerate(selected_columns[:20], 1):\n",
    "        sample_vals = df[col].describe()[['25%', '50%', '75%']].round(3)\n",
    "        print(f\"  {i}. {col} (Q1={sample_vals['25%']}, Median={sample_vals['50%']}, Q3={sample_vals['75%']})\")\n",
    "    \n",
    "    col_idx = int(input(\"\\nüëâ Select column number: \")) - 1\n",
    "    target_column = selected_columns[col_idx]\n",
    "    \n",
    "    print(\"\\nThreshold options:\")\n",
    "    print(\"1. Median\")\n",
    "    print(\"2. Mean\")\n",
    "    print(\"3. Custom value\")\n",
    "    \n",
    "    threshold_option = input(\"\\nüëâ Your choice (1-3): \").strip()\n",
    "    \n",
    "    if threshold_option == '1':\n",
    "        threshold = df[target_column].median()\n",
    "    elif threshold_option == '2':\n",
    "        threshold = df[target_column].mean()\n",
    "    else:\n",
    "        threshold = float(input(\"üëâ Enter threshold value: \"))\n",
    "    \n",
    "    df['target'] = (df[target_column] > threshold).astype(int)\n",
    "    selected_columns.remove(target_column)\n",
    "    print(f\"\\n‚úÖ Created binary target (above/below {threshold:.3f})\")\n",
    "    \n",
    "elif target_option == '3':\n",
    "    # Multi-class target\n",
    "    print(\"\\nSelect column for multi-class target:\")\n",
    "    for i, col in enumerate(selected_columns[:20], 1):\n",
    "        print(f\"  {i}. {col}\")\n",
    "    \n",
    "    col_idx = int(input(\"\\nüëâ Select column number: \")) - 1\n",
    "    target_column = selected_columns[col_idx]\n",
    "    \n",
    "    n_classes = int(input(\"\\nüëâ Number of classes (3-10): \"))\n",
    "    df['target'] = pd.qcut(df[target_column], q=n_classes, \n",
    "                          labels=range(n_classes), duplicates='drop')\n",
    "    selected_columns.remove(target_column)\n",
    "    print(f\"\\n‚úÖ Created {n_classes}-class target\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n‚úÖ No target variable created - ready for unsupervised learning\")\n",
    "    target_column = None\n",
    "\n",
    "# Display target distribution if created\n",
    "if 'target' in df.columns:\n",
    "    print(\"\\nüìä Target Distribution:\")\n",
    "    target_dist = df['target'].value_counts().sort_index()\n",
    "    for class_val, count in target_dist.items():\n",
    "        pct = count / len(df) * 100\n",
    "        print(f\"   Class {class_val}: {count:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Check class balance\n",
    "    balance_info = check_class_balance(df['target'])\n",
    "    if not balance_info['balanced']:\n",
    "        print(f\"\\n‚ö†Ô∏è Class imbalance detected!\")\n",
    "        print(f\"   Imbalance ratio: {balance_info['imbalance_ratio']:.2f}\")\n",
    "        print(f\"   Suggested strategy: {balance_info['suggested_strategy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà 5. Data Visualization and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D visualization of feature relationships\n",
    "if 'target' in df.columns and len(selected_columns) >= 2:\n",
    "    print(\"\\nüìä Creating 3D visualization of feature relationships...\")\n",
    "    \n",
    "    # Select top 2 features by correlation with target\n",
    "    correlations = df[selected_columns].corrwith(df['target']).abs().sort_values(ascending=False)\n",
    "    top_features = correlations.head(2).index.tolist()\n",
    "    \n",
    "    # Create 3D scatter plot\n",
    "    fig = go.Figure(data=[go.Scatter3d(\n",
    "        x=df[top_features[0]],\n",
    "        y=df[top_features[1]],\n",
    "        z=df['target'],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=5,\n",
    "            color=df['target'],\n",
    "            colorscale='Viridis',\n",
    "            showscale=True,\n",
    "            colorbar=dict(title=\"Target\")\n",
    "        ),\n",
    "        text=[f\"Target: {t}\" for t in df['target']],\n",
    "        hovertemplate='%{text}<br>' +\n",
    "                      f'{top_features[0]}: %{{x:.3f}}<br>' +\n",
    "                      f'{top_features[1]}: %{{y:.3f}}<extra></extra>'\n",
    "    )])\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f'3D Visualization: Target vs Top 2 Features',\n",
    "        scene=dict(\n",
    "            xaxis_title=top_features[0],\n",
    "            yaxis_title=top_features[1],\n",
    "            zaxis_title='Target'\n",
    "        ),\n",
    "        height=600\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "# Feature correlation heatmap\n",
    "if len(selected_columns) > 1:\n",
    "    print(\"\\nüìä Creating feature correlation heatmap...\")\n",
    "    \n",
    "    # Sample features if too many\n",
    "    if len(selected_columns) > 20:\n",
    "        # Select diverse features based on correlation\n",
    "        sample_features = selected_columns[:20]\n",
    "    else:\n",
    "        sample_features = selected_columns\n",
    "    \n",
    "    corr_matrix = df[sample_features].corr()\n",
    "    \n",
    "    fig = px.imshow(corr_matrix,\n",
    "                    text_auto='.2f',\n",
    "                    color_continuous_scale='RdBu_r',\n",
    "                    title='Feature Correlation Heatmap')\n",
    "    fig.update_layout(width=800, height=600)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ 6. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = df[selected_columns]\n",
    "y = df['target'] if 'target' in df.columns else None\n",
    "\n",
    "print(f\"\\nüìä Data shapes:\")\n",
    "print(f\"   Features (X): {X.shape}\")\n",
    "if y is not None:\n",
    "    print(f\"   Target (y): {y.shape}\")\n",
    "\n",
    "# Feature selection option\n",
    "if X.shape[1] > 50:\n",
    "    print(f\"\\n‚ö†Ô∏è You have {X.shape[1]} features. Consider feature selection?\")\n",
    "    do_feature_selection = input(\"üëâ Perform feature selection? (y/n): \").strip().lower()\n",
    "    \n",
    "    if do_feature_selection == 'y':\n",
    "        print(\"\\nFeature selection methods:\")\n",
    "        print(\"1. SelectKBest (statistical)\")\n",
    "        print(\"2. Mutual Information\")\n",
    "        print(\"3. Recursive Feature Elimination (slower)\")\n",
    "        \n",
    "        method = input(\"üëâ Select method (1-3): \").strip()\n",
    "        k = int(input(\"üëâ Number of features to keep: \"))\n",
    "        \n",
    "        if method == '1':\n",
    "            selector = SelectKBest(f_classif, k=k)\n",
    "        elif method == '2':\n",
    "            selector = SelectKBest(mutual_info_classif, k=k)\n",
    "        else:\n",
    "            # Use a simple model for RFE\n",
    "            estimator = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "            selector = RFE(estimator, n_features_to_select=k)\n",
    "        \n",
    "        X_selected = selector.fit_transform(X, y)\n",
    "        selected_features = X.columns[selector.get_support()]\n",
    "        X = pd.DataFrame(X_selected, columns=selected_features)\n",
    "        print(f\"\\n‚úÖ Selected {k} features\")\n",
    "        selected_columns = list(selected_features)\n",
    "\n",
    "# Train-test split\n",
    "test_size = float(input(\"\\nüëâ Test set size (0.1-0.4, default=0.2): \") or \"0.2\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_size, random_state=42, stratify=y\n",
    "\n",
    "print(f\"\\n‚úÇÔ∏è Data split completed:\")\n",
    "print(f\"   Training set: {X_train.shape[0]:,} samples\")\n",
    "print(f\"   Test set: {X_test.shape[0]:,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ 7. Model Definition and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models with class weight handling for imbalanced data\n",
    "if y is not None:\n",
    "    class_weights = None\n",
    "    if not balance_info['balanced']:\n",
    "        # Calculate class weights\n",
    "        classes = np.unique(y_train)\n",
    "        weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
    "        class_weights = dict(zip(classes, weights))\n",
    "        print(f\"\\n‚öñÔ∏è Using class weights: {class_weights}\")\n",
    "\n",
    "# Performance mode selection\n",
    "print(\"\\nüöÄ PERFORMANCE MODE SELECTION\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. Quick Mode (fewer hyperparameters, faster)\")\n",
    "print(\"2. Standard Mode (balanced)\")\n",
    "print(\"3. Thorough Mode (more hyperparameters, slower)\")\n",
    "\n",
    "mode = input(\"\\nüëâ Select mode (1-3, default=2): \").strip() or \"2\"\n",
    "\n",
    "# Define models based on mode\n",
    "if mode == '1':\n",
    "    # Quick mode - minimal parameters\n",
    "    models = {\n",
    "        'Logistic Regression': {\n",
    "            'model': LogisticRegression(max_iter=1000, random_state=42, \n",
    "                                      class_weight=class_weights if 'class_weights' in locals() else None),\n",
    "            'params': {\n",
    "                'C': [0.1, 1, 10],\n",
    "                'solver': ['lbfgs']\n",
    "            }\n",
    "        },\n",
    "        'Decision Tree': {\n",
    "            'model': DecisionTreeClassifier(random_state=42,\n",
    "                                          class_weight=class_weights if 'class_weights' in locals() else None),\n",
    "            'params': {\n",
    "                'max_depth': [5, 10],\n",
    "                'min_samples_split': [2, 10]\n",
    "            }\n",
    "        },\n",
    "        'Random Forest': {\n",
    "            'model': RandomForestClassifier(random_state=42, n_jobs=-1,\n",
    "                                          class_weight=class_weights if 'class_weights' in locals() else None),\n",
    "            'params': {\n",
    "                'n_estimators': [50, 100],\n",
    "                'max_depth': [10, 20]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "elif mode == '3':\n",
    "    # Thorough mode - extensive parameters\n",
    "    models = {\n",
    "        'Logistic Regression': {\n",
    "            'model': LogisticRegression(max_iter=2000, random_state=42,\n",
    "                                      class_weight=class_weights if 'class_weights' in locals() else None),\n",
    "            'params': {\n",
    "                'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "                'penalty': ['l1', 'l2'],\n",
    "                'solver': ['liblinear', 'saga']\n",
    "            }\n",
    "        },\n",
    "        'Decision Tree': {\n",
    "            'model': DecisionTreeClassifier(random_state=42,\n",
    "                                          class_weight=class_weights if 'class_weights' in locals() else None),\n",
    "            'params': {\n",
    "                'max_depth': [3, 5, 7, 10, 15, None],\n",
    "                'min_samples_split': [2, 5, 10, 20],\n",
    "                'min_samples_leaf': [1, 2, 4, 8],\n",
    "                'criterion': ['gini', 'entropy']\n",
    "            }\n",
    "        },\n",
    "        'Random Forest': {\n",
    "            'model': RandomForestClassifier(random_state=42, n_jobs=-1,\n",
    "                                          class_weight=class_weights if 'class_weights' in locals() else None),\n",
    "            'params': {\n",
    "                'n_estimators': [50, 100, 200, 300],\n",
    "                'max_depth': [5, 10, 15, 20, None],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'min_samples_leaf': [1, 2, 4],\n",
    "                'max_features': ['sqrt', 'log2', None]\n",
    "            }\n",
    "        },\n",
    "        'Gradient Boosting': {\n",
    "            'model': GradientBoostingClassifier(random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [50, 100, 200],\n",
    "                'learning_rate': [0.01, 0.1, 0.2],\n",
    "                'max_depth': [3, 5, 7],\n",
    "                'subsample': [0.8, 1.0]\n",
    "            }\n",
    "        },\n",
    "        'SVM': {\n",
    "            'model': SVC(random_state=42, probability=True,\n",
    "                        class_weight=class_weights if 'class_weights' in locals() else None),\n",
    "            'params': {\n",
    "                'C': [0.1, 1, 10, 100],\n",
    "                'kernel': ['rbf', 'linear', 'poly'],\n",
    "                'gamma': ['scale', 'auto', 0.001, 0.01]\n",
    "            }\n",
    "        },\n",
    "        'Neural Network': {\n",
    "            'model': MLPClassifier(random_state=42, max_iter=1000),\n",
    "            'params': {\n",
    "                'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],\n",
    "                'activation': ['relu', 'tanh'],\n",
    "                'alpha': [0.0001, 0.001, 0.01],\n",
    "                'learning_rate': ['constant', 'adaptive']\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "else:\n",
    "    # Standard mode - balanced parameters\n",
    "    models = {\n",
    "        'Logistic Regression': {\n",
    "            'model': LogisticRegression(max_iter=1000, random_state=42,\n",
    "                                      class_weight=class_weights if 'class_weights' in locals() else None),\n",
    "            'params': {\n",
    "                'C': [0.01, 0.1, 1, 10],\n",
    "                'penalty': ['l2'],\n",
    "                'solver': ['lbfgs', 'liblinear']\n",
    "            }\n",
    "        },\n",
    "        'Decision Tree': {\n",
    "            'model': DecisionTreeClassifier(random_state=42,\n",
    "                                          class_weight=class_weights if 'class_weights' in locals() else None),\n",
    "            'params': {\n",
    "                'max_depth': [3, 5, 7, 10],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'min_samples_leaf': [1, 2, 4]\n",
    "            }\n",
    "        },\n",
    "        'Random Forest': {\n",
    "            'model': RandomForestClassifier(random_state=42, n_jobs=-1,\n",
    "                                          class_weight=class_weights if 'class_weights' in locals() else None),\n",
    "            'params': {\n",
    "                'n_estimators': [50, 100, 200],\n",
    "                'max_depth': [5, 10, 15],\n",
    "                'min_samples_split': [2, 5],\n",
    "                'min_samples_leaf': [1, 2]\n",
    "            }\n",
    "        },\n",
    "        'Gradient Boosting': {\n",
    "            'model': GradientBoostingClassifier(random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [50, 100],\n",
    "                'learning_rate': [0.01, 0.1],\n",
    "                'max_depth': [3, 5]\n",
    "            }\n",
    "        },\n",
    "        'SVM': {\n",
    "            'model': SVC(random_state=42, probability=True,\n",
    "                        class_weight=class_weights if 'class_weights' in locals() else None),\n",
    "            'params': {\n",
    "                'C': [0.1, 1, 10],\n",
    "                'kernel': ['rbf', 'linear'],\n",
    "                'gamma': ['scale', 'auto']\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(f\"\\n‚úÖ {len(models)} models configured for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÉ‚Äç‚ôÇÔ∏è 8. Model Training with Optimization Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize storage\n",
    "results = {}\n",
    "best_models = {}\n",
    "training_histories = {}\n",
    "\n",
    "# Define scoring metrics based on class balance\n",
    "if 'balance_info' in locals() and not balance_info['balanced']:\n",
    "    scoring_metrics = ['balanced_accuracy', 'f1_weighted', 'roc_auc_ovr_weighted']\n",
    "    primary_metric = 'balanced_accuracy'\n",
    "else:\n",
    "    scoring_metrics = ['accuracy', 'f1_weighted', 'roc_auc_ovr']\n",
    "    primary_metric = 'accuracy'\n",
    "\n",
    "# Cross-validation setup\n",
    "cv_folds = 5\n",
    "cv_strategy = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "\n",
    "print(f\"\\nüîÑ Starting model training with {cv_folds}-fold cross-validation...\")\n",
    "print(f\"   Primary metric: {primary_metric}\")\n",
    "print(f\"   Additional metrics: {', '.join(scoring_metrics[1:])}\")\n",
    "\n",
    "# Train each model\n",
    "for model_name, model_info in models.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ü§ñ Training {model_name}...\")\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # GridSearchCV with multiple metrics\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model_info['model'],\n",
    "        param_grid=model_info['params'],\n",
    "        cv=cv_strategy,\n",
    "        scoring=primary_metric,\n",
    "        n_jobs=-1,\n",
    "        verbose=1,\n",
    "        return_train_score=True\n",
    "    )\n",
    "    \n",
    "    # Fit the model\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Store the best model\n",
    "    best_models[model_name] = grid_search.best_estimator_\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    y_pred_proba = None\n",
    "    if hasattr(grid_search.best_estimator_, 'predict_proba'):\n",
    "        y_pred_proba = grid_search.predict_proba(X_test)\n",
    "    \n",
    "    # Calculate comprehensive metrics\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # Cross-validation scores\n",
    "    cv_scores = cross_val_score(grid_search.best_estimator_, X_train, y_train, \n",
    "                               cv=cv_strategy, scoring=primary_metric)\n",
    "    \n",
    "    # Store results\n",
    "    results[model_name] = {\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'best_cv_score': grid_search.best_score_,\n",
    "        'cv_scores': cv_scores,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'balanced_accuracy': balanced_acc,\n",
    "        'f1_score': f1,\n",
    "        'predictions': y_pred,\n",
    "        'pred_proba': y_pred_proba,\n",
    "        'training_time': (datetime.now() - start_time).total_seconds(),\n",
    "        'grid_search_results': grid_search.cv_results_\n",
    "    }\n",
    "    \n",
    "    # Store training history\n",
    "    training_histories[model_name] = {\n",
    "        'mean_train_scores': grid_search.cv_results_['mean_train_score'],\n",
    "        'mean_test_scores': grid_search.cv_results_['mean_test_score'],\n",
    "        'params': grid_search.cv_results_['params']\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n‚úÖ {model_name} training completed!\")\n",
    "    print(f\"   Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"   Best CV score: {grid_search.best_score_:.4f}\")\n",
    "    print(f\"   Test accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"   Balanced accuracy: {balanced_acc:.4f}\")\n",
    "    print(f\"   F1 score: {f1:.4f}\")\n",
    "    print(f\"   Training time: {results[model_name]['training_time']:.2f} seconds\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"‚úÖ All models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 9. Model Comparison and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison\n",
    "comparison_data = []\n",
    "for model_name, result in results.items():\n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'CV Mean Score': result['cv_scores'].mean(),\n",
    "        'CV Std': result['cv_scores'].std(),\n",
    "        'Test Accuracy': result['test_accuracy'],\n",
    "        'Balanced Accuracy': result['balanced_accuracy'],\n",
    "        'F1 Score': result['f1_score'],\n",
    "        'Training Time (s)': result['training_time']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('Balanced Accuracy', ascending=False)\n",
    "\n",
    "print(\"\\nüìä Model Performance Comparison:\")\n",
    "print(\"=\" * 100)\n",
    "print(comparison_df.to_string(index=False, float_format='%.4f'))\n",
    "\n",
    "# Identify best model\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìà Advanced Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive comparison plots\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('Model Accuracy Comparison', 'Training Time vs Performance',\n",
    "                    'Cross-Validation Consistency', 'Metric Comparison'),\n",
    "    specs=[[{\"type\": \"bar\"}, {\"type\": \"scatter\"}],\n",
    "           [{\"type\": \"box\"}, {\"type\": \"bar\"}]]\n",
    ")\n",
    "\n",
    "# 1. Accuracy comparison\n",
    "fig.add_trace(\n",
    "    go.Bar(x=comparison_df['Model'], y=comparison_df['Test Accuracy'],\n",
    "           name='Test Accuracy', marker_color='lightblue'),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Bar(x=comparison_df['Model'], y=comparison_df['Balanced Accuracy'],\n",
    "           name='Balanced Accuracy', marker_color='darkblue'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# 2. Training time vs performance\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=comparison_df['Training Time (s)'], \n",
    "               y=comparison_df['Balanced Accuracy'],\n",
    "               mode='markers+text',\n",
    "               text=comparison_df['Model'],\n",
    "               textposition=\"top center\",\n",
    "               marker=dict(size=10)),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# 3. CV consistency (box plot)\n",
    "for model_name in results.keys():\n",
    "    fig.add_trace(\n",
    "        go.Box(y=results[model_name]['cv_scores'],\n",
    "               name=model_name),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "# 4. Multiple metrics comparison\n",
    "metrics_data = []\n",
    "for model in comparison_df['Model']:\n",
    "    metrics_data.extend([\n",
    "        go.Bar(name=f'{model} - Accuracy', \n",
    "               x=['Accuracy'], y=[comparison_df[comparison_df['Model']==model]['Test Accuracy'].values[0]]),\n",
    "        go.Bar(name=f'{model} - F1', \n",
    "               x=['F1 Score'], y=[comparison_df[comparison_df['Model']==model]['F1 Score'].values[0]])\n",
    "    ])\n",
    "\n",
    "fig.update_layout(height=800, showlegend=True, \n",
    "                  title_text=\"Comprehensive Model Performance Analysis\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ GridSearch Optimization Landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize hyperparameter optimization for the best model\n",
    "best_model_history = training_histories[best_model_name]\n",
    "\n",
    "# Create optimization landscape visualization\n",
    "if len(models[best_model_name]['params']) >= 2:\n",
    "    print(f\"\\nüìä Hyperparameter Optimization Landscape for {best_model_name}\")\n",
    "    \n",
    "    # Get parameter names\n",
    "    param_names = list(models[best_model_name]['params'].keys())[:2]\n",
    "    \n",
    "    # Create a pivot table for visualization\n",
    "    results_df = pd.DataFrame({\n",
    "        'param_' + param_names[0]: [p[param_names[0]] for p in best_model_history['params']],\n",
    "        'param_' + param_names[1]: [p[param_names[1]] for p in best_model_history['params']],\n",
    "        'score': best_model_history['mean_test_scores']\n",
    "    })\n",
    "    \n",
    "    # Create unique values for each parameter\n",
    "    param1_values = sorted(results_df['param_' + param_names[0]].unique())\n",
    "    param2_values = sorted(results_df['param_' + param_names[1]].unique())\n",
    "    \n",
    "    # Create heatmap data\n",
    "    heatmap_data = []\n",
    "    for p1 in param1_values:\n",
    "        row = []\n",
    "        for p2 in param2_values:\n",
    "            score = results_df[(results_df['param_' + param_names[0]] == p1) & \n",
    "                              (results_df['param_' + param_names[1]] == p2)]['score']\n",
    "            row.append(score.values[0] if len(score) > 0 else None)\n",
    "        heatmap_data.append(row)\n",
    "    \n",
    "    # Create 3D surface plot\n",
    "    fig = go.Figure(data=[go.Surface(\n",
    "        x=param2_values,\n",
    "        y=param1_values,\n",
    "        z=heatmap_data,\n",
    "        colorscale='Viridis'\n",
    "    )])\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f'Hyperparameter Optimization Landscape - {best_model_name}',\n",
    "        scene=dict(\n",
    "            xaxis_title=param_names[1],\n",
    "            yaxis_title=param_names[0],\n",
    "            zaxis_title='CV Score'\n",
    "        ),\n",
    "        height=600\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç 10. Best Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model\n",
    "best_model = best_models[best_model_name]\n",
    "best_result = results[best_model_name]\n",
    "\n",
    "print(f\"\\nüèÜ Detailed Analysis of {best_model_name}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nüìã Classification Report:\")\n",
    "print(classification_report(y_test, best_result['predictions']))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, best_result['predictions'])\n",
    "print(\"\\nüìä Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Confusion Matrix Heatmap\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0])\n",
    "axes[0, 0].set_title(f'Confusion Matrix - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Predicted')\n",
    "axes[0, 0].set_ylabel('Actual')\n",
    "\n",
    "# 2. ROC Curves (for binary classification)\n",
    "if len(np.unique(y_test)) == 2 and best_result['pred_proba'] is not None:\n",
    "    fpr, tpr, _ = roc_curve(y_test, best_result['pred_proba'][:, 1])\n",
    "    auc_score = roc_auc_score(y_test, best_result['pred_proba'][:, 1])\n",
    "    \n",
    "    axes[0, 1].plot(fpr, tpr, 'b-', linewidth=2, label=f'ROC Curve (AUC = {auc_score:.3f})')\n",
    "    axes[0, 1].plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "    axes[0, 1].set_xlabel('False Positive Rate')\n",
    "    axes[0, 1].set_ylabel('True Positive Rate')\n",
    "    axes[0, 1].set_title(f'ROC Curve - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].legend(loc='lower right')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Precision-Recall Curve\n",
    "    precision, recall, _ = precision_recall_curve(y_test, best_result['pred_proba'][:, 1])\n",
    "    axes[1, 0].plot(recall, precision, 'g-', linewidth=2)\n",
    "    axes[1, 0].set_xlabel('Recall')\n",
    "    axes[1, 0].set_ylabel('Precision')\n",
    "    axes[1, 0].set_title(f'Precision-Recall Curve - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "else:\n",
    "    axes[0, 1].text(0.5, 0.5, 'ROC Curve not available\\nfor multi-class', \n",
    "                    ha='center', va='center', transform=axes[0, 1].transAxes)\n",
    "    axes[1, 0].text(0.5, 0.5, 'Precision-Recall Curve\\nnot available for multi-class', \n",
    "                    ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "\n",
    "# 3. Learning Curves\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    best_model, X_train, y_train, cv=cv_strategy, \n",
    "    train_sizes=np.linspace(0.1, 1.0, 10), n_jobs=-1\n",
    ")\n",
    "\n",
    "axes[1, 1].plot(train_sizes, np.mean(train_scores, axis=1), 'o-', color='r', label='Training score')\n",
    "axes[1, 1].plot(train_sizes, np.mean(val_scores, axis=1), 'o-', color='g', label='Validation score')\n",
    "axes[1, 1].fill_between(train_sizes, \n",
    "                        np.mean(train_scores, axis=1) - np.std(train_scores, axis=1),\n",
    "                        np.mean(train_scores, axis=1) + np.std(train_scores, axis=1), \n",
    "                        alpha=0.1, color='r')\n",
    "axes[1, 1].fill_between(train_sizes, \n",
    "                        np.mean(val_scores, axis=1) - np.std(val_scores, axis=1),\n",
    "                        np.mean(val_scores, axis=1) + np.std(val_scores, axis=1), \n",
    "                        alpha=0.1, color='g')\n",
    "axes[1, 1].set_xlabel('Training Set Size')\n",
    "axes[1, 1].set_ylabel('Score')\n",
    "axes[1, 1].set_title(f'Learning Curves - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].legend(loc='best')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåü 11. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for applicable models\n",
    "if best_model_name in ['Random Forest', 'Gradient Boosting', 'Decision Tree']:\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Display top features\n",
    "    print(f\"\\nüìä Top 20 Most Important Features ({best_model_name}):\")\n",
    "    print(feature_importance.head(20).to_string(index=False))\n",
    "    \n",
    "    # Visualize feature importance\n",
    "    fig = px.bar(feature_importance.head(20), \n",
    "                 y='feature', x='importance',\n",
    "                 orientation='h',\n",
    "                 title=f'Top 20 Feature Importances - {best_model_name}',\n",
    "                 labels={'importance': 'Feature Importance', 'feature': 'Feature'})\n",
    "    fig.update_layout(height=600, yaxis={'categoryorder':'total ascending'})\n",
    "    fig.show()\n",
    "    \n",
    "elif best_model_name == 'Logistic Regression':\n",
    "    # Get coefficients\n",
    "    if len(np.unique(y_train)) == 2:\n",
    "        coefficients = pd.DataFrame({\n",
    "            'feature': X_train.columns,\n",
    "            'coefficient': best_model.coef_[0]\n",
    "        })\n",
    "    else:\n",
    "        # Multi-class: average absolute coefficients\n",
    "        coefficients = pd.DataFrame({\n",
    "            'feature': X_train.columns,\n",
    "            'coefficient': np.mean(np.abs(best_model.coef_), axis=0)\n",
    "        })\n",
    "    \n",
    "    coefficients['abs_coefficient'] = coefficients['coefficient'].abs()\n",
    "    coefficients = coefficients.sort_values('abs_coefficient', ascending=False)\n",
    "    \n",
    "    print(f\"\\nüìä Top 20 Most Important Features ({best_model_name}):\")\n",
    "    print(coefficients.head(20)[['feature', 'coefficient']].to_string(index=False))\n",
    "    \n",
    "    # Visualize coefficients\n",
    "    fig = px.bar(coefficients.head(20), \n",
    "                 y='feature', x='coefficient',\n",
    "                 orientation='h',\n",
    "                 title=f'Top 20 Feature Coefficients - {best_model_name}',\n",
    "                 color='coefficient',\n",
    "                 color_continuous_scale='RdBu_r',\n",
    "                 color_continuous_midpoint=0)\n",
    "    fig.update_layout(height=600, yaxis={'categoryorder':'total ascending'})\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ù 12. Ensemble Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ensemble from top performing models\n",
    "print(\"\\nü§ù Creating Ensemble Model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Select top 3 models\n",
    "top_models = comparison_df.head(3)['Model'].tolist()\n",
    "print(f\"Selected models for ensemble: {', '.join(top_models)}\")\n",
    "\n",
    "# Create ensemble\n",
    "ensemble_estimators = [(name, best_models[name]) for name in top_models]\n",
    "ensemble_model = VotingClassifier(estimators=ensemble_estimators, voting='soft')\n",
    "\n",
    "# Train ensemble\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate ensemble\n",
    "ensemble_pred = ensemble_model.predict(X_test)\n",
    "ensemble_accuracy = accuracy_score(y_test, ensemble_pred)\n",
    "ensemble_balanced = balanced_accuracy_score(y_test, ensemble_pred)\n",
    "ensemble_f1 = f1_score(y_test, ensemble_pred, average='weighted')\n",
    "\n",
    "print(f\"\\n‚úÖ Ensemble Model Performance:\")\n",
    "print(f\"   Test Accuracy: {ensemble_accuracy:.4f}\")\n",
    "print(f\"   Balanced Accuracy: {ensemble_balanced:.4f}\")\n",
    "print(f\"   F1 Score: {ensemble_f1:.4f}\")\n",
    "\n",
    "# Compare with best single model\n",
    "improvement = (ensemble_balanced - comparison_df.iloc[0]['Balanced Accuracy']) * 100\n",
    "print(f\"\\nüìä Improvement over best single model: {improvement:+.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ 13. Save Results and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = Path('model_results') / datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Prepare comprehensive results\n",
    "final_results = {\n",
    "    'analysis_date': datetime.now().strftime('%Y-%m-%d %H:%M'),\n",
    "    'dataset_info': {\n",
    "        'shape': df.shape,\n",
    "        'features_used': len(selected_columns),\n",
    "        'train_size': X_train.shape[0],\n",
    "        'test_size': X_test.shape[0],\n",
    "        'class_distribution': y.value_counts().to_dict() if y is not None else None,\n",
    "        'class_balance': balance_info if 'balance_info' in locals() else None\n",
    "    },\n",
    "    'model_comparison': comparison_df.to_dict('records'),\n",
    "    'best_model': {\n",
    "        'name': best_model_name,\n",
    "        'parameters': results[best_model_name]['best_params'],\n",
    "        'test_accuracy': results[best_model_name]['test_accuracy'],\n",
    "        'balanced_accuracy': results[best_model_name]['balanced_accuracy'],\n",
    "        'f1_score': results[best_model_name]['f1_score'],\n",
    "        'cv_mean_score': results[best_model_name]['cv_scores'].mean(),\n",
    "        'cv_std_score': results[best_model_name]['cv_scores'].std()\n",
    "    },\n",
    "    'ensemble_performance': {\n",
    "        'models': top_models,\n",
    "        'test_accuracy': ensemble_accuracy,\n",
    "        'balanced_accuracy': ensemble_balanced,\n",
    "        'f1_score': ensemble_f1,\n",
    "        'improvement': improvement\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save results\n",
    "results_file = output_dir / 'analysis_results.json'\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(final_results, f, indent=4)\n",
    "print(f\"\\n‚úÖ Results saved to: {results_file}\")\n",
    "\n",
    "# Save models\n",
    "for model_name, model in best_models.items():\n",
    "    model_file = output_dir / f\"{model_name.lower().replace(' ', '_')}_model.pkl\"\n",
    "    joblib.dump(model, model_file)\n",
    "print(f\"‚úÖ Individual models saved to: {output_dir}\")\n",
    "\n",
    "# Save ensemble model\n",
    "ensemble_file = output_dir / 'ensemble_model.pkl'\n",
    "joblib.dump(ensemble_model, ensemble_file)\n",
    "print(f\"‚úÖ Ensemble model saved to: {ensemble_file}\")\n",
    "\n",
    "# Save predictions\n",
    "predictions_df = pd.DataFrame({\n",
    "    'actual': y_test,\n",
    "    'predicted_best': best_result['predictions'],\n",
    "    'predicted_ensemble': ensemble_pred\n",
    "})\n",
    "predictions_file = output_dir / 'predictions.csv'\n",
    "predictions_df.to_csv(predictions_file, index=False)\n",
    "print(f\"‚úÖ Predictions saved to: {predictions_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ 14. Final Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä FINAL ANALYSIS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüéØ Best Single Model: {best_model_name}\")\n",
    "print(f\"   - Balanced Accuracy: {results[best_model_name]['balanced_accuracy']:.4f}\")\n",
    "print(f\"   - F1 Score: {results[best_model_name]['f1_score']:.4f}\")\n",
    "print(f\"   - Training Time: {results[best_model_name]['training_time']:.2f}s\")\n",
    "\n",
    "print(f\"\\nü§ù Ensemble Model Performance:\")\n",
    "print(f\"   - Balanced Accuracy: {ensemble_balanced:.4f}\")\n",
    "print(f\"   - Improvement: {improvement:+.2f}%\")\n",
    "\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "if 'balance_info' in locals() and not balance_info['balanced']:\n",
    "    print(f\"   ‚ö†Ô∏è Class imbalance handled with class weights\")\n",
    "    print(f\"   - Imbalance ratio: {balance_info['imbalance_ratio']:.2f}\")\n",
    "\n",
    "if 'feature_importance' in locals():\n",
    "    top_features = feature_importance.head(5)['feature'].tolist()\n",
    "    print(f\"   üìä Most important features: {', '.join(top_features)}\")\n",
    "\n",
    "print(\"\\nüöÄ Recommendations for Next Steps:\")\n",
    "print(\"   1. Deploy the ensemble model for best performance\")\n",
    "print(\"   2. Monitor model performance on new data\")\n",
    "print(\"   3. Consider collecting more data for minority classes\")\n",
    "print(\"   4. Experiment with feature engineering based on importance\")\n",
    "print(\"   5. Set up automated retraining pipeline\")\n",
    "\n",
    "print(f\"\\n‚úÖ Analysis completed successfully!\")\n",
    "print(f\"üìÅ All results saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö 15. Manual vs Automated Optimization Comparison\n",
    "\n",
    "This section provides an educational comparison between manual parameter tuning (as in gradient descent) and automated methods like GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüéØ OPTIMIZATION METHODS COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comparison visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Manual optimization illustration\n",
    "x = np.linspace(-10, 10, 100)\n",
    "y = np.linspace(-10, 10, 100)\n",
    "X_mesh, Y_mesh = np.meshgrid(x, y)\n",
    "Z = np.sin(np.sqrt(X_mesh**2 + Y_mesh**2))\n",
    "\n",
    "# Manual path\n",
    "manual_path_x = [8, 6, 4, 2, 0.5, 0.1]\n",
    "manual_path_y = [8, 5, 3, 1, 0.3, 0.1]\n",
    "\n",
    "ax1.contour(X_mesh, Y_mesh, Z, levels=20, alpha=0.6)\n",
    "ax1.plot(manual_path_x, manual_path_y, 'ro-', linewidth=2, markersize=8)\n",
    "ax1.set_title('Manual Optimization\\n(Like Gradient Descent)', fontsize=14)\n",
    "ax1.set_xlabel('Parameter 1')\n",
    "ax1.set_ylabel('Parameter 2')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Grid search illustration\n",
    "grid_x = np.linspace(-10, 10, 10)\n",
    "grid_y = np.linspace(-10, 10, 10)\n",
    "grid_points_x, grid_points_y = np.meshgrid(grid_x, grid_y)\n",
    "\n",
    "ax2.contour(X_mesh, Y_mesh, Z, levels=20, alpha=0.6)\n",
    "ax2.scatter(grid_points_x, grid_points_y, c='red', s=50, alpha=0.8)\n",
    "ax2.set_title('Grid Search Optimization\\n(Systematic Exploration)', fontsize=14)\n",
    "ax2.set_xlabel('Parameter 1')\n",
    "ax2.set_ylabel('Parameter 2')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Comparison Summary:\")\n",
    "print(\"\\nManual Optimization (Gradient Descent Style):\")\n",
    "print(\"  ‚úì Provides intuition about parameter space\")\n",
    "print(\"  ‚úì Can be faster for simple problems\")\n",
    "print(\"  ‚úì Allows for custom optimization strategies\")\n",
    "print(\"  ‚úó Requires expertise and manual tuning\")\n",
    "print(\"  ‚úó May get stuck in local optima\")\n",
    "print(\"  ‚úó Time-consuming for multiple parameters\")\n",
    "\n",
    "print(\"\\nAutomated Optimization (GridSearchCV):\")\n",
    "print(\"  ‚úì Systematic and reproducible\")\n",
    "print(\"  ‚úì Explores entire parameter space\")\n",
    "print(\"  ‚úì Finds global optimum within search space\")\n",
    "print(\"  ‚úì Handles multiple parameters easily\")\n",
    "print(\"  ‚úó Can be computationally expensive\")\n",
    "print(\"  ‚úó Limited to predefined parameter grid\")\n",
    "\n",
    "print(\"\\nüí° Best Practice: Use GridSearchCV for model selection, \")\n",
    "print(\"   then fine-tune with manual methods if needed!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ Enhanced ML Training Notebook Complete!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
