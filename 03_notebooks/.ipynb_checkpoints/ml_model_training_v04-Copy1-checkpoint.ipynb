{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Universal ML Model Training - Classification & Regression\n",
    "\n",
    "This notebook automatically detects whether your problem is classification or regression and applies appropriate models and metrics.\n",
    "\n",
    "## üìÅ Expected Structure\n",
    "```\n",
    "Your Project/\n",
    "‚îú‚îÄ‚îÄ 02_data/Processed_data/    ‚Üê Pre-scaled data\n",
    "‚îú‚îÄ‚îÄ 03_notebooks/              ‚Üê Run from here\n",
    "‚îî‚îÄ‚îÄ 05_results/                ‚Üê Output files\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "# ML imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold, StratifiedKFold\n",
    "from sklearn.metrics import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, f_regression, mutual_info_classif, mutual_info_regression\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso, ElasticNet, SGDClassifier, SGDRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.offline as pyo\n",
    "pyo.init_notebook_mode(connected=True)\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üìÖ Analysis date: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• 2. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the module (adjust path as needed if not in sys.path)\n",
    "import sys\n",
    "sys.path.append('./src')  # Optional: adapt if running outside src\n",
    "from file_handler import setup_paths, load_data_with_detection_enhanced\n",
    "\n",
    "# 1. Interactive project path setup (choose input/output folder)\n",
    "project_root, input_path, output_path = setup_paths()\n",
    "\n",
    "# 2. File selection and loading (choose file & format)\n",
    "df, filename = load_data_with_detection_enhanced(input_path)\n",
    "\n",
    "print(f\"\\nüìä Dataset loaded: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "print(f\"   Memory usage: {df.memory_usage().sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# --- Handle missing data  ---\n",
    "missing_pct = df.isnull().sum() / len(df) * 100\n",
    "cols_to_drop = missing_pct[missing_pct > 50].index\n",
    "if len(cols_to_drop) > 0:\n",
    "    df = df.drop(columns=cols_to_drop)\n",
    "    print(f\"   Dropped {len(cols_to_drop)} columns with >50% missing data\")\n",
    "\n",
    "# Fill remaining missing values for numeric columns\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add these cells after the \"Load and Prepare Data\" section (Section 2)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## üå°Ô∏è 2.1 Weather Data Detection and Analysis (Optional)\n",
    "# \n",
    "# This section provides specialized analysis tools if your dataset contains weather/temperature data.\n",
    "\n",
    "# %%\n",
    "# Check if this is weather/temperature data\n",
    "temp_cols = [col for col in df.columns if 'temp' in col.lower() or 'temperature' in col.lower()]\n",
    "weather_cols = [col for col in df.columns if any(term in col.lower() for term in ['temp', 'humid', 'pressure', 'wind', 'rain', 'weather', 'climate'])]\n",
    "\n",
    "if temp_cols:\n",
    "    print(\"üå°Ô∏è WEATHER DATA DETECTED!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nFound {len(temp_cols)} temperature columns:\")\n",
    "    for i, col in enumerate(temp_cols[:10], 1):\n",
    "        print(f\"  {i}. {col}\")\n",
    "    if len(temp_cols) > 10:\n",
    "        print(f\"  ... and {len(temp_cols) - 10} more\")\n",
    "    \n",
    "    # Check for weather stations\n",
    "    station_pattern = []\n",
    "    for col in temp_cols:\n",
    "        # Extract station name (assuming format like STATION_temp_mean)\n",
    "        parts = col.split('_')\n",
    "        if len(parts) >= 2:\n",
    "            station = parts[0]\n",
    "            if station not in station_pattern:\n",
    "                station_pattern.append(station)\n",
    "    \n",
    "    if station_pattern:\n",
    "        print(f\"\\nüìç Found {len(station_pattern)} weather stations:\")\n",
    "        for i, station in enumerate(station_pattern[:10], 1):\n",
    "            print(f\"  {i}. {station}\")\n",
    "        if len(station_pattern) > 10:\n",
    "            print(f\"  ... and {len(station_pattern) - 10} more\")\n",
    "    \n",
    "    run_weather_analysis = input(\"\\nüëâ Run specialized weather analysis? (y/n): \").strip().lower() == 'y'\n",
    "else:\n",
    "    run_weather_analysis = False\n",
    "    print(\"‚ÑπÔ∏è No weather/temperature data detected. Skipping weather-specific analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## üå°Ô∏è 2.2 Temperature Data Preprocessing\n",
    "\n",
    "# %%\n",
    "if run_weather_analysis and temp_cols:\n",
    "    # Filter for mean temperature columns only\n",
    "    mean_temp_cols = [col for col in temp_cols if 'mean' in col.lower()]\n",
    "    \n",
    "    if mean_temp_cols:\n",
    "        print(f\"\\nüìä Focusing on {len(mean_temp_cols)} mean temperature columns\")\n",
    "        \n",
    "        # Create temperature-only dataframe\n",
    "        df_temp = df[mean_temp_cols].copy()\n",
    "        \n",
    "        # Add date information if available\n",
    "        date_cols_weather = [col for col in df.columns if 'date' in col.lower()]\n",
    "        if date_cols_weather:\n",
    "            df_temp['DATE'] = df[date_cols_weather[0]]\n",
    "            if any('month' in col.lower() for col in df.columns):\n",
    "                month_col = [col for col in df.columns if 'month' in col.lower()][0]\n",
    "                df_temp['MONTH'] = df[month_col]\n",
    "        \n",
    "        print(f\"‚úÖ Temperature dataset created: {df_temp.shape}\")\n",
    "        \n",
    "        # Box plot of temperature distributions\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        df_temp[mean_temp_cols].boxplot(rot=90)\n",
    "        plt.title('Temperature Distribution Across Weather Stations')\n",
    "        plt.ylabel('Temperature (scaled)')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Summary statistics\n",
    "        print(\"\\nüìä Temperature Statistics:\")\n",
    "        temp_stats = df_temp[mean_temp_cols].describe()\n",
    "        print(f\"  Overall mean: {temp_stats.loc['mean'].mean():.2f}\")\n",
    "        print(f\"  Overall std: {temp_stats.loc['std'].mean():.2f}\")\n",
    "        print(f\"  Coldest station (avg): {temp_stats.loc['mean'].idxmin()} ({temp_stats.loc['mean'].min():.2f})\")\n",
    "        print(f\"  Warmest station (avg): {temp_stats.loc['mean'].idxmax()} ({temp_stats.loc['mean'].max():.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## üåç 2.3 3D Temperature Visualization Across All Stations\n",
    "\n",
    "# %%\n",
    "if run_weather_analysis and 'df_temp' in locals():\n",
    "    # Year selection for 3D visualization\n",
    "    if 'DATE' in df_temp.columns:\n",
    "        df_temp['year'] = pd.to_datetime(df_temp['DATE'], format='%Y%m%d').dt.year\n",
    "        available_years = sorted(df_temp['year'].unique())\n",
    "        \n",
    "        print(\"\\nüìÖ Available years for 3D visualization:\")\n",
    "        for i, year in enumerate(available_years[:100], 1):\n",
    "            count = len(df_temp[df_temp['year'] == year])\n",
    "            print(f\"  {i}. {year} ({count} records)\")\n",
    "        \n",
    "        year_choice = input(\"\\nüëâ Enter year number (or 'skip' to skip): \").strip()\n",
    "        \n",
    "        if year_choice != 'skip' and year_choice.isdigit():\n",
    "            selected_year = available_years[int(year_choice) - 1]\n",
    "            df_year_3d = df_temp[df_temp['year'] == selected_year].copy()\n",
    "            \n",
    "            # Prepare data for 3D visualization\n",
    "            temp_matrix = df_year_3d[mean_temp_cols].values\n",
    "            \n",
    "            # Create station labels (shortened for better display)\n",
    "            station_labels = [col.replace('_temp_mean', '').replace('_', ' ') for col in mean_temp_cols]\n",
    "            \n",
    "            # Create 3D surface plot\n",
    "            fig = go.Figure(data=[go.Surface(\n",
    "                z=temp_matrix.T,  # Transpose so stations are on x-axis\n",
    "                x=list(range(len(df_year_3d))),  # Days\n",
    "                y=list(range(len(station_labels))),  # Stations\n",
    "                colorscale='RdBu_r',\n",
    "                name='Temperature'\n",
    "            )])\n",
    "            \n",
    "            # Update layout\n",
    "            fig.update_layout(\n",
    "                title=f'Temperature Patterns Across All Stations - Year {selected_year}',\n",
    "                scene=dict(\n",
    "                    xaxis_title='Day of Year',\n",
    "                    yaxis_title='Weather Station',\n",
    "                    zaxis_title='Temperature',\n",
    "                    yaxis=dict(\n",
    "                        tickmode='array',\n",
    "                        tickvals=list(range(0, len(station_labels), max(1, len(station_labels)//10))),\n",
    "                        ticktext=[station_labels[i] for i in range(0, len(station_labels), max(1, len(station_labels)//10))]\n",
    "                    )\n",
    "                ),\n",
    "                width=800,\n",
    "                height=600\n",
    "            )\n",
    "            \n",
    "            fig.show()\n",
    "            \n",
    "            print(f\"\\n‚úÖ 3D visualization created for {selected_year}\")\n",
    "            print(\"   üí° Tip: Click and drag to rotate the 3D plot!\")\n",
    "            \n",
    "            # Store year data for later use\n",
    "            df_year_temp = df_year_3d\n",
    "        else:\n",
    "            df_year_temp = None\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No date column found for year-based analysis\")\n",
    "        df_year_temp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## üéØ 2.4 Weather Station Selection for Detailed Analysis\n",
    "\n",
    "# %%\n",
    "if run_weather_analysis and 'mean_temp_cols' in locals():\n",
    "    print(\"\\nüéØ SELECT A WEATHER STATION FOR DETAILED ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Calculate correlation with other features if target exists\n",
    "    if 'target' in df.columns:\n",
    "        correlations = {}\n",
    "        for col in mean_temp_cols:\n",
    "            if col in df.columns:\n",
    "                corr = df[col].corr(df['target'])\n",
    "                correlations[col] = corr\n",
    "        \n",
    "        # Sort by absolute correlation\n",
    "        sorted_stations = sorted(correlations.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "        \n",
    "        print(\"\\nWeather stations by correlation with target:\")\n",
    "        for i, (station, corr) in enumerate(sorted_stations[:20], 1):\n",
    "            station_name = station.replace('_temp_mean', '')\n",
    "            print(f\"  {i}. {station_name}: {corr:.3f}\")\n",
    "    else:\n",
    "        sorted_stations = [(col, 0) for col in mean_temp_cols]\n",
    "        print(\"\\nAvailable weather stations:\")\n",
    "        for i, (station, _) in enumerate(sorted_stations[:20], 1):\n",
    "            station_name = station.replace('_temp_mean', '')\n",
    "            print(f\"  {i}. {station_name}\")\n",
    "    \n",
    "    station_choice = input(\"\\nüëâ Select station number (or 'skip'): \").strip()\n",
    "    \n",
    "    if station_choice != 'skip' and station_choice.isdigit():\n",
    "        selected_station_col = sorted_stations[int(station_choice) - 1][0]\n",
    "        selected_station_name = selected_station_col.replace('_temp_mean', '')\n",
    "        \n",
    "        print(f\"\\n‚úÖ Selected station: {selected_station_name}\")\n",
    "        \n",
    "        # Store for gradient descent analysis\n",
    "        weather_station_selected = selected_station_col\n",
    "        weather_station_name = selected_station_name\n",
    "    else:\n",
    "        weather_station_selected = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÖ 3. Time Period Selection (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for date columns\n",
    "date_cols = [col for col in df.columns if 'date' in col.lower()]\n",
    "if date_cols:\n",
    "    date_col = date_cols[0]\n",
    "    df['_year'] = pd.to_datetime(df[date_col], format='%Y%m%d').dt.year\n",
    "    year_counts = df['_year'].value_counts().sort_index()\n",
    "    \n",
    "    print(\"\\nüìÖ Available years:\")\n",
    "    for year, count in year_counts.items():\n",
    "        print(f\"  {year}: {count:,} records\")\n",
    "    \n",
    "    time_selection = input(\"\\nüëâ Enter year/range/all/last5: \").strip().lower()\n",
    "    \n",
    "    if time_selection == 'all':\n",
    "        pass\n",
    "    elif time_selection == 'last5':\n",
    "        df = df[df['_year'] >= df['_year'].max() - 4]\n",
    "    elif '-' in time_selection:\n",
    "        start, end = map(int, time_selection.split('-'))\n",
    "        df = df[(df['_year'] >= start) & (df['_year'] <= end)]\n",
    "    elif time_selection.isdigit():\n",
    "        df = df[df['_year'] == int(time_selection)]\n",
    "    \n",
    "    df = df.drop('_year', axis=1)\n",
    "    print(f\"   Selected dataset size: {len(df):,} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ 4. Feature and Target Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Group columns by pattern\n",
    "patterns = {\n",
    "    'Statistical': ['mean', 'max', 'min', 'std', 'avg'],\n",
    "    'Temperature': ['temp', 'temperature'],\n",
    "    'Weather': ['humid', 'pressure', 'wind', 'rain'],\n",
    "    'Time': ['date', 'year', 'month', 'day']\n",
    "}\n",
    "\n",
    "grouped_cols = {}\n",
    "for group, keywords in patterns.items():\n",
    "    cols = [c for c in numeric_cols if any(k in c.lower() for k in keywords)]\n",
    "    if cols:\n",
    "        grouped_cols[group] = cols\n",
    "\n",
    "print(\"\\nüéØ Feature Groups:\")\n",
    "for i, (group, cols) in enumerate(grouped_cols.items(), 1):\n",
    "    print(f\"  {i}. {group} ({len(cols)} columns)\")\n",
    "\n",
    "selection = input(\"\\nüëâ Select groups (e.g., 1,3) or 'all': \").strip()\n",
    "if selection.lower() == 'all':\n",
    "    selected_features = numeric_cols\n",
    "else:\n",
    "    selected_features = []\n",
    "    for idx in selection.split(','):\n",
    "        group_name = list(grouped_cols.keys())[int(idx.strip())-1]\n",
    "        selected_features.extend(grouped_cols[group_name])\n",
    "\n",
    "print(f\"‚úÖ Selected {len(selected_features)} features\")\n",
    "\n",
    "# Keyword filtering (optional)\n",
    "keyword_filter = input(\"\\nüëâ Filter by keywords (optional, press Enter to skip): \").strip()\n",
    "if keyword_filter:\n",
    "    keywords = [k.strip().lower() for k in keyword_filter.split(',')]\n",
    "    filtered_columns = [col for col in selected_features \n",
    "                       if all(kw in col.lower() for kw in keywords)]\n",
    "    if filtered_columns:\n",
    "        selected_features = filtered_columns\n",
    "        print(f\"‚úÖ Filtered to {len(selected_features)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "# Target variable creation\n",
    "print(\"\\nüéØ Target Variable Selection:\")\n",
    "print(\"1. Use existing column\")\n",
    "print(\"2. Create binary target (threshold)\")\n",
    "print(\"3. Create multi-class target (binning)\")\n",
    "\n",
    "target_option = input(\"\\nüëâ Your choice (1-3): \").strip()\n",
    "\n",
    "# Show available columns\n",
    "print(\"\\nAvailable columns:\")\n",
    "for i, col in enumerate(selected_features[:20], 1):\n",
    "    stats = df[col].describe()[['mean', '50%', 'std']]\n",
    "    print(f\"  {i}. {col} (mean={stats['mean']:.2f}, median={stats['50%']:.2f})\")\n",
    "\n",
    "col_idx = int(input(\"\\nüëâ Select column number: \")) - 1\n",
    "target_col = selected_features[col_idx]\n",
    "selected_features.remove(target_col)\n",
    "\n",
    "# Create target based on option\n",
    "if target_option == '1':\n",
    "    df['target'] = df[target_col]\n",
    "    # Detect problem type\n",
    "    n_unique = df['target'].nunique()\n",
    "    if n_unique <= 20 or df['target'].dtype == 'object':\n",
    "        problem_type = 'classification'\n",
    "    else:\n",
    "        problem_type = 'regression'\n",
    "elif target_option == '2':\n",
    "    threshold = df[target_col].median()\n",
    "    df['target'] = (df[target_col] > threshold).astype(int)\n",
    "    problem_type = 'classification'\n",
    "else:\n",
    "    n_classes = int(input(\"\\nüëâ Number of classes (3-10): \"))\n",
    "    df['target'] = pd.qcut(df[target_col], q=n_classes, labels=range(n_classes), duplicates='drop')\n",
    "    problem_type = 'classification'\n",
    "\n",
    "print(f\"\\n‚úÖ Problem type detected: {problem_type.upper()}\")\n",
    "print(f\"   Target variable: {target_col}\")\n",
    "print(f\"   Unique values: {df['target'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì 5. Manual Gradient Descent Implementation\n",
    "\n",
    "This section demonstrates gradient descent optimization from scratch, showing how ML algorithms learn parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we should run gradient descent demo\n",
    "# Initialize results dictionaries if they don't exist\n",
    "if 'results' not in locals():\n",
    "    results = {}\n",
    "if 'best_models' not in locals():\n",
    "    best_models = {}\n",
    "\n",
    "if problem_type == 'regression' and len(selected_features) >= 1:\n",
    "    print(\"\\nüéì GRADIENT DESCENT\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"We'll use a simple linear regression with visualization.\\n\")\n",
    "    print(\"üìù Note: This implementation uses only ONE feature for educational purposes,\")\n",
    "    print(\"   while the sklearn models will use ALL selected features.\\n\")\n",
    "    \n",
    "    # Select feature for demonstration\n",
    "    print(\"Select a feature for gradient descent visualization:\")\n",
    "    for i, feat in enumerate(selected_features[:20], 1):\n",
    "        corr = df[feat].corr(df['target'])\n",
    "        print(f\"  {i}. {feat} (correlation with target: {corr:.3f})\")\n",
    "    \n",
    "    feat_idx = int(input(\"\\nüëâ Select feature number (or 0 to skip): \"))\n",
    "    \n",
    "    if feat_idx > 0:\n",
    "        selected_feature = selected_features[feat_idx - 1]\n",
    "        run_gradient_descent = True\n",
    "    else:\n",
    "        run_gradient_descent = False\n",
    "else:\n",
    "    run_gradient_descent = False\n",
    "    print(\"\\nüí° Gradient descent demo is available for regression problems.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'run_gradient_descent' in locals() and run_gradient_descent:\n",
    "    # Prepare data for gradient descent\n",
    "    X_gd = df[selected_feature].values.reshape(-1, 1)\n",
    "    y_gd = df['target'].values\n",
    "    \n",
    "    # Standardize the data for better convergence\n",
    "    X_mean, X_std = X_gd.mean(), X_gd.std()\n",
    "    y_mean, y_std = y_gd.mean(), y_gd.std()\n",
    "    X_norm = (X_gd - X_mean) / X_std\n",
    "    y_norm = (y_gd - y_mean) / y_std\n",
    "    \n",
    "    # Add bias term (intercept)\n",
    "    X_norm = np.c_[np.ones(X_norm.shape[0]), X_norm]\n",
    "    \n",
    "    print(f\"\\nüìä Using feature: {selected_feature}\")\n",
    "    print(f\"   Data points: {len(X_norm)}\")\n",
    "    print(f\"   Feature range: [{X_gd.min():.2f}, {X_gd.max():.2f}]\")\n",
    "    print(f\"   Target range: [{y_gd.min():.2f}, {y_gd.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'run_gradient_descent' in locals() and run_gradient_descent:\n",
    "    \"\"\"\n",
    "    GRADIENT DESCENT IMPLEMENTATION\n",
    "    \n",
    "    This is a manual implementation of gradient descent for linear regression.\n",
    "    The algorithm iteratively updates parameters (theta) to minimize the cost function.\n",
    "    \n",
    "    Key concepts:\n",
    "    - Cost Function: J(Œ∏) = (1/2m) * Œ£(h(x) - y)¬≤ where h(x) = Œ∏‚ÇÄ + Œ∏‚ÇÅ*x\n",
    "    - Gradient: ‚àÇJ/‚àÇŒ∏‚±º = (1/m) * Œ£(h(x) - y) * x‚±º\n",
    "    - Update Rule: Œ∏‚±º := Œ∏‚±º - Œ± * ‚àÇJ/‚àÇŒ∏‚±º\n",
    "    \"\"\"\n",
    "    \n",
    "    def compute_cost(X, y, theta):\n",
    "        \"\"\"\n",
    "        Compute the cost (Mean Squared Error) for given parameters.\n",
    "        \n",
    "        Args:\n",
    "            X: Feature matrix with bias term (m x 2)\n",
    "            y: Target values (m x 1)\n",
    "            theta: Parameters [Œ∏‚ÇÄ, Œ∏‚ÇÅ] (2 x 1)\n",
    "        \n",
    "        Returns:\n",
    "            cost: Scalar cost value\n",
    "        \"\"\"\n",
    "        m = len(y)\n",
    "        predictions = X.dot(theta)\n",
    "        errors = predictions - y\n",
    "        cost = (1 / (2 * m)) * np.sum(errors ** 2)\n",
    "        return cost\n",
    "    \n",
    "    def gradient_descent(X, y, theta_init, alpha, iterations):\n",
    "        \"\"\"\n",
    "        Perform gradient descent to optimize parameters.\n",
    "        \n",
    "        Args:\n",
    "            X: Feature matrix with bias term\n",
    "            y: Target values\n",
    "            theta_init: Initial parameter values\n",
    "            alpha: Learning rate (step size)\n",
    "            iterations: Number of iterations\n",
    "        \n",
    "        Returns:\n",
    "            theta: Final parameters\n",
    "            cost_history: Cost at each iteration\n",
    "            theta_history: Parameters at each iteration\n",
    "        \"\"\"\n",
    "        m = len(y)\n",
    "        theta = theta_init.copy()\n",
    "        cost_history = []\n",
    "        theta_history = []\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            # Compute predictions\n",
    "            predictions = X.dot(theta)\n",
    "            \n",
    "            # Compute errors\n",
    "            errors = predictions - y\n",
    "            \n",
    "            # Compute gradients\n",
    "            # ‚àÇJ/‚àÇŒ∏‚ÇÄ = (1/m) * Œ£(errors)\n",
    "            # ‚àÇJ/‚àÇŒ∏‚ÇÅ = (1/m) * Œ£(errors * x)\n",
    "            gradients = (1 / m) * X.T.dot(errors)\n",
    "            \n",
    "            # Update parameters\n",
    "            theta = theta - alpha * gradients\n",
    "            \n",
    "            # Store history\n",
    "            cost = compute_cost(X, y, theta)\n",
    "            cost_history.append(cost)\n",
    "            theta_history.append(theta.copy())\n",
    "            \n",
    "            # Print progress every 100 iterations\n",
    "            if i % 100 == 0:\n",
    "                print(f\"   Iteration {i}: Cost = {cost:.6f}, Œ∏‚ÇÄ = {theta[0]:.4f}, Œ∏‚ÇÅ = {theta[1]:.4f}\")\n",
    "        \n",
    "        return theta, cost_history, np.array(theta_history)\n",
    "    \n",
    "    # Interactive parameter selection\n",
    "    print(\"\\n‚öôÔ∏è GRADIENT DESCENT PARAMETERS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Initial theta values\n",
    "    print(\"\\nInitial parameter values (Œ∏‚ÇÄ, Œ∏‚ÇÅ):\")\n",
    "    print(\"  1. Random initialization\")\n",
    "    print(\"  2. Zero initialization\")\n",
    "    print(\"  3. Custom values\")\n",
    "    \n",
    "    init_choice = input(\"\\nüëâ Your choice (1-3): \").strip()\n",
    "    \n",
    "    if init_choice == '1':\n",
    "        theta_init = np.random.randn(2) * 0.5\n",
    "    elif init_choice == '2':\n",
    "        theta_init = np.zeros(2)\n",
    "    else:\n",
    "        theta0 = float(input(\"  Enter Œ∏‚ÇÄ (intercept): \"))\n",
    "        theta1 = float(input(\"  Enter Œ∏‚ÇÅ (slope): \"))\n",
    "        theta_init = np.array([theta0, theta1])\n",
    "    \n",
    "    print(f\"\\n  Initial values: Œ∏‚ÇÄ = {theta_init[0]:.4f}, Œ∏‚ÇÅ = {theta_init[1]:.4f}\")\n",
    "    \n",
    "    # Learning rate\n",
    "    print(\"\\nLearning rate (Œ±) suggestions:\")\n",
    "    print(\"  ‚Ä¢ 0.001: Very slow but stable\")\n",
    "    print(\"  ‚Ä¢ 0.01:  Moderate speed (recommended)\")\n",
    "    print(\"  ‚Ä¢ 0.1:   Fast but may overshoot\")\n",
    "    print(\"  ‚Ä¢ 1.0:   Very fast, risk of divergence\")\n",
    "    \n",
    "    alpha = float(input(\"\\nüëâ Enter learning rate: \"))\n",
    "    \n",
    "    # Number of iterations\n",
    "    iterations = int(input(\"üëâ Number of iterations (e.g., 1000): \"))\n",
    "    \n",
    "    print(\"\\nüöÄ Running gradient descent...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Run gradient descent\n",
    "    theta_final, cost_history, theta_history = gradient_descent(\n",
    "        X_norm, y_norm, theta_init, alpha, iterations\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Gradient descent complete!\")\n",
    "    print(f\"   Final parameters: Œ∏‚ÇÄ = {theta_final[0]:.4f}, Œ∏‚ÇÅ = {theta_final[1]:.4f}\")\n",
    "    print(f\"   Final cost: {cost_history[-1]:.6f}\")\n",
    "    print(f\"   Cost reduction: {cost_history[0]:.6f} ‚Üí {cost_history[-1]:.6f} ({(1 - cost_history[-1]/cost_history[0])*100:.1f}% improvement)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add these cells to replace or enhance the gradient descent section (Section 5)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## üéì 5.1 Enhanced Gradient Descent for Weather Data\n",
    "# \n",
    "# This section provides specialized gradient descent analysis for weather/temperature data.\n",
    "\n",
    "# %%\n",
    "# Enhanced gradient descent setup for weather data\n",
    "if problem_type == 'regression' and run_weather_analysis and 'weather_station_selected' in locals() and weather_station_selected:\n",
    "    print(\"\\nüéì GRADIENT DESCENT FOR WEATHER DATA\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Using weather station: {weather_station_name}\")\n",
    "    \n",
    "    # Check if we have year-specific data\n",
    "    if 'df_year_temp' in locals() and df_year_temp is not None:\n",
    "        print(f\"Using data from year: {selected_year}\")\n",
    "        df_gd = df_year_temp\n",
    "    else:\n",
    "        df_gd = df\n",
    "    \n",
    "    # Create day index (scaled)\n",
    "    n_days = len(df_gd)\n",
    "    is_leap_year = n_days == 366\n",
    "    \n",
    "    # Create scaled index (0.01 to 3.65/3.66)\n",
    "    max_val = 3.66 if is_leap_year else 3.65\n",
    "    day_index = np.linspace(0.01, max_val, n_days)\n",
    "    \n",
    "    print(f\"\\nüìä Data preparation:\")\n",
    "    print(f\"   Days in dataset: {n_days} {'(leap year)' if is_leap_year else ''}\")\n",
    "    print(f\"   Index range: 0.01 to {max_val}\")\n",
    "    \n",
    "    # Prepare data\n",
    "    X_weather = day_index.reshape(-1, 1)\n",
    "    y_weather = df_gd[weather_station_selected].values\n",
    "    \n",
    "    # Remove any NaN values\n",
    "    mask = ~np.isnan(y_weather)\n",
    "    X_weather = X_weather[mask]\n",
    "    y_weather = y_weather[mask]\n",
    "    \n",
    "    print(f\"   Valid data points: {len(X_weather)}\")\n",
    "    print(f\"   Temperature range: [{y_weather.min():.2f}, {y_weather.max():.2f}]\")\n",
    "    \n",
    "    # Visualize the data\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.scatter(X_weather, y_weather, alpha=0.6, s=20)\n",
    "    plt.xlabel('Day Index (scaled)')\n",
    "    plt.ylabel('Temperature')\n",
    "    plt.title(f'Temperature Pattern - {weather_station_name} {selected_year if \"selected_year\" in locals() else \"\"}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    run_weather_gradient_descent = True\n",
    "else:\n",
    "    run_weather_gradient_descent = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## üîÑ 5.2 Multiple Gradient Descent Experiments\n",
    "\n",
    "# %%\n",
    "if 'run_weather_gradient_descent' in locals() and run_weather_gradient_descent:\n",
    "    print(\"\\nüîÑ MULTIPLE GRADIENT DESCENT EXPERIMENTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"We'll run gradient descent multiple times with different parameters to find the best configuration.\\n\")\n",
    "    \n",
    "    # Standardize the data\n",
    "    X_mean_w, X_std_w = X_weather.mean(), X_weather.std()\n",
    "    y_mean_w, y_std_w = y_weather.mean(), y_weather.std()\n",
    "    X_norm_w = (X_weather - X_mean_w) / X_std_w\n",
    "    y_norm_w = (y_weather - y_mean_w) / y_std_w\n",
    "    \n",
    "    # Add bias term\n",
    "    X_norm_w = np.c_[np.ones(X_norm_w.shape[0]), X_norm_w]\n",
    "    \n",
    "    # Define multiple experiment configurations\n",
    "    experiments = [\n",
    "        {'name': 'Conservative', 'theta_init': np.zeros(2), 'alpha': 0.01, 'iterations': 1000},\n",
    "        {'name': 'Random Start', 'theta_init': np.random.randn(2) * 0.5, 'alpha': 0.01, 'iterations': 1000},\n",
    "        {'name': 'Fast Learning', 'theta_init': np.zeros(2), 'alpha': 0.1, 'iterations': 500},\n",
    "        {'name': 'Very Fast', 'theta_init': np.zeros(2), 'alpha': 0.5, 'iterations': 200},\n",
    "        {'name': 'Custom', 'theta_init': None, 'alpha': None, 'iterations': None}\n",
    "    ]\n",
    "    \n",
    "    print(\"Experiment configurations:\")\n",
    "    for i, exp in enumerate(experiments, 1):\n",
    "        if exp['name'] != 'Custom':\n",
    "            print(f\"  {i}. {exp['name']}: Œ±={exp['alpha']}, iter={exp['iterations']}, Œ∏_init={exp['theta_init']}\")\n",
    "        else:\n",
    "            print(f\"  {i}. {exp['name']}: You define the parameters\")\n",
    "    \n",
    "    exp_choice = int(input(\"\\nüëâ Select experiment (1-5): \")) - 1\n",
    "    selected_exp = experiments[exp_choice].copy()\n",
    "    \n",
    "    # Handle custom experiment\n",
    "    if selected_exp['name'] == 'Custom':\n",
    "        print(\"\\nCustom experiment setup:\")\n",
    "        theta0 = float(input(\"  Œ∏‚ÇÄ (intercept, try -1 to 1): \"))\n",
    "        theta1 = float(input(\"  Œ∏‚ÇÅ (slope, try -1 to 1): \"))\n",
    "        selected_exp['theta_init'] = np.array([theta0, theta1])\n",
    "        selected_exp['alpha'] = float(input(\"  Learning rate Œ± (try 0.001 to 0.5): \"))\n",
    "        selected_exp['iterations'] = int(input(\"  Iterations (try 100 to 2000): \"))\n",
    "    \n",
    "    print(f\"\\nüöÄ Running experiment: {selected_exp['name']}\")\n",
    "    \n",
    "    # Run gradient descent\n",
    "    theta_weather, cost_history_weather, theta_history_weather = gradient_descent(\n",
    "        X_norm_w, y_norm_w, \n",
    "        selected_exp['theta_init'], \n",
    "        selected_exp['alpha'], \n",
    "        selected_exp['iterations']\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    weather_gd_results = {\n",
    "        'experiment': selected_exp,\n",
    "        'theta_final': theta_weather,\n",
    "        'cost_history': cost_history_weather,\n",
    "        'theta_history': theta_history_weather,\n",
    "        'final_cost': cost_history_weather[-1]\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n‚úÖ Experiment complete!\")\n",
    "    print(f\"   Final cost: {cost_history_weather[-1]:.6f}\")\n",
    "    print(f\"   Convergence: {'Yes' if abs(cost_history_weather[-1] - cost_history_weather[-2]) < 1e-6 else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## üìä 5.3 Comparative Visualization for Weather Gradient Descent\n",
    "\n",
    "# %%\n",
    "if 'weather_gd_results' in locals():\n",
    "    # Create comprehensive visualization\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=3,\n",
    "        subplot_titles=(\n",
    "            'Original Data with Fit', \n",
    "            'Cost Evolution', \n",
    "            'Parameter Evolution',\n",
    "            '2D Loss Contours', \n",
    "            'Learning Rate Analysis', \n",
    "            'Residuals Pattern'\n",
    "        ),\n",
    "        specs=[\n",
    "            [{'type': 'scatter'}, {'type': 'scatter'}, {'type': 'scatter'}],\n",
    "            [{'type': 'contour'}, {'type': 'scatter'}, {'type': 'scatter'}]\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # 1. Original data with fitted line\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=X_weather.flatten(), \n",
    "            y=y_weather,\n",
    "            mode='markers',\n",
    "            marker=dict(size=4, opacity=0.5),\n",
    "            name='Data',\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Add fitted line\n",
    "    x_line_w = np.linspace(X_weather.min(), X_weather.max(), 100)\n",
    "    x_line_norm_w = (x_line_w - X_mean_w) / X_std_w\n",
    "    X_line_norm_w = np.c_[np.ones(len(x_line_w)), x_line_norm_w]\n",
    "    y_line_norm_w = X_line_norm_w.dot(weather_gd_results['theta_final'])\n",
    "    y_line_w = y_line_norm_w * y_std_w + y_mean_w\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x_line_w,\n",
    "            y=y_line_w,\n",
    "            mode='lines',\n",
    "            line=dict(color='red', width=3),\n",
    "            name='Fit',\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. Cost evolution\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=list(range(len(weather_gd_results['cost_history']))),\n",
    "            y=weather_gd_results['cost_history'],\n",
    "            mode='lines',\n",
    "            name='Cost',\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 3. Parameter evolution\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=list(range(len(weather_gd_results['theta_history']))),\n",
    "            y=weather_gd_results['theta_history'][:, 0],\n",
    "            mode='lines',\n",
    "            name='Œ∏‚ÇÄ',\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1, col=3\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=list(range(len(weather_gd_results['theta_history']))),\n",
    "            y=weather_gd_results['theta_history'][:, 1],\n",
    "            mode='lines',\n",
    "            name='Œ∏‚ÇÅ',\n",
    "            showlegend=False,\n",
    "            yaxis='y2'\n",
    "        ),\n",
    "        row=1, col=3\n",
    "    )\n",
    "    \n",
    "    # 4. 2D Contour plot\n",
    "    # Create grid for contour\n",
    "    theta0_range_w = np.linspace(\n",
    "        weather_gd_results['theta_final'][0] - 2, \n",
    "        weather_gd_results['theta_final'][0] + 2, \n",
    "        40\n",
    "    )\n",
    "    theta1_range_w = np.linspace(\n",
    "        weather_gd_results['theta_final'][1] - 2, \n",
    "        weather_gd_results['theta_final'][1] + 2, \n",
    "        40\n",
    "    )\n",
    "    \n",
    "    theta0_grid_w, theta1_grid_w = np.meshgrid(theta0_range_w, theta1_range_w)\n",
    "    cost_grid_w = np.zeros_like(theta0_grid_w)\n",
    "    \n",
    "    for i in range(len(theta0_range_w)):\n",
    "        for j in range(len(theta1_range_w)):\n",
    "            theta_temp = np.array([theta0_grid_w[i, j], theta1_grid_w[i, j]])\n",
    "            cost_grid_w[i, j] = compute_cost(X_norm_w, y_norm_w, theta_temp)\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Contour(\n",
    "            x=theta0_range_w,\n",
    "            y=theta1_range_w,\n",
    "            z=cost_grid_w,\n",
    "            colorscale='Viridis',\n",
    "            showscale=False,\n",
    "            name='Loss'\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Add path on contour\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=weather_gd_results['theta_history'][:, 0],\n",
    "            y=weather_gd_results['theta_history'][:, 1],\n",
    "            mode='lines+markers',\n",
    "            line=dict(color='red', width=2),\n",
    "            marker=dict(size=4),\n",
    "            name='Path',\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 5. Learning rate analysis\n",
    "    step_sizes = []\n",
    "    for i in range(1, len(weather_gd_results['cost_history'])):\n",
    "        step = abs(weather_gd_results['cost_history'][i] - weather_gd_results['cost_history'][i-1])\n",
    "        step_sizes.append(step)\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=list(range(len(step_sizes))),\n",
    "            y=step_sizes,\n",
    "            mode='lines',\n",
    "            name='Step Size',\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    # 6. Residuals\n",
    "    y_pred_w = X_norm_w.dot(weather_gd_results['theta_final'])\n",
    "    y_pred_w = y_pred_w * y_std_w + y_mean_w\n",
    "    residuals = y_weather - y_pred_w\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=X_weather.flatten(),\n",
    "            y=residuals,\n",
    "            mode='markers',\n",
    "            marker=dict(size=4, opacity=0.5),\n",
    "            name='Residuals',\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=2, col=3\n",
    "    )\n",
    "    \n",
    "    # Add zero line\n",
    "    fig.add_hline(y=0, line_dash=\"dash\", line_color=\"red\", row=2, col=3)\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f'Weather Gradient Descent Analysis - {weather_station_name}',\n",
    "        height=800,\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    # Update axes labels\n",
    "    fig.update_xaxes(title_text='Day Index', row=1, col=1)\n",
    "    fig.update_yaxes(title_text='Temperature', row=1, col=1)\n",
    "    fig.update_xaxes(title_text='Iteration', row=1, col=2)\n",
    "    fig.update_yaxes(title_text='Cost', row=1, col=2)\n",
    "    fig.update_xaxes(title_text='Iteration', row=1, col=3)\n",
    "    fig.update_yaxes(title_text='Œ∏‚ÇÄ', row=1, col=3)\n",
    "    fig.update_xaxes(title_text='Œ∏‚ÇÄ', row=2, col=1)\n",
    "    fig.update_yaxes(title_text='Œ∏‚ÇÅ', row=2, col=1)\n",
    "    fig.update_xaxes(title_text='Iteration', row=2, col=2)\n",
    "    fig.update_yaxes(title_text='Step Size', row=2, col=2)\n",
    "    fig.update_xaxes(title_text='Day Index', row=2, col=3)\n",
    "    fig.update_yaxes(title_text='Residual', row=2, col=3)\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\nüìä WEATHER GRADIENT DESCENT SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nStation: {weather_station_name}\")\n",
    "    if 'selected_year' in locals():\n",
    "        print(f\"Year: {selected_year}\")\n",
    "    print(f\"\\nExperiment: {weather_gd_results['experiment']['name']}\")\n",
    "    print(f\"  Œ± = {weather_gd_results['experiment']['alpha']}\")\n",
    "    print(f\"  Iterations = {weather_gd_results['experiment']['iterations']}\")\n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  Final Œ∏‚ÇÄ = {weather_gd_results['theta_final'][0]:.4f}\")\n",
    "    print(f\"  Final Œ∏‚ÇÅ = {weather_gd_results['theta_final'][1]:.4f}\")\n",
    "    print(f\"  Final cost = {weather_gd_results['final_cost']:.6f}\")\n",
    "    print(f\"  R¬≤ score = {r2_score(y_weather, y_pred_w):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add these cells after the gradient descent sections\n",
    "\n",
    "# %% [markdown]\n",
    "# ## üîç 5.4 Convergence Analysis Questions\n",
    "# \n",
    "# Let's explore how gradient descent performs across different conditions.\n",
    "\n",
    "# %%\n",
    "if 'weather_gd_results' in locals():\n",
    "    print(\"\\nüîç CONVERGENCE ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Analyze convergence\n",
    "    cost_history = weather_gd_results['cost_history']\n",
    "    \n",
    "    # Find convergence point (where change becomes very small)\n",
    "    convergence_threshold = 1e-6\n",
    "    converged_at = None\n",
    "    for i in range(1, len(cost_history)):\n",
    "        if abs(cost_history[i] - cost_history[i-1]) < convergence_threshold:\n",
    "            converged_at = i\n",
    "            break\n",
    "    \n",
    "    print(f\"\\n1. Convergence Analysis:\")\n",
    "    print(f\"   ‚Ä¢ Converged: {'Yes' if converged_at else 'No'}\")\n",
    "    if converged_at:\n",
    "        print(f\"   ‚Ä¢ Converged at iteration: {converged_at}\")\n",
    "        print(f\"   ‚Ä¢ Convergence efficiency: {converged_at / weather_gd_results['experiment']['iterations'] * 100:.1f}% of total iterations\")\n",
    "    else:\n",
    "        print(f\"   ‚Ä¢ Still improving after {weather_gd_results['experiment']['iterations']} iterations\")\n",
    "        print(f\"   ‚Ä¢ Consider increasing iterations or adjusting learning rate\")\n",
    "    \n",
    "    # Analyze learning rate appropriateness\n",
    "    print(f\"\\n2. Learning Rate Analysis:\")\n",
    "    early_reduction = (cost_history[0] - cost_history[min(10, len(cost_history)-1)]) / cost_history[0]\n",
    "    final_reduction = (cost_history[0] - cost_history[-1]) / cost_history[0]\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Cost reduction in first 10 iterations: {early_reduction*100:.1f}%\")\n",
    "    print(f\"   ‚Ä¢ Total cost reduction: {final_reduction*100:.1f}%\")\n",
    "    \n",
    "    if early_reduction > 0.9:\n",
    "        print(f\"   ‚Ä¢ ‚ö° Very fast initial learning - learning rate might be too high\")\n",
    "    elif early_reduction < 0.1:\n",
    "        print(f\"   ‚Ä¢ üêå Very slow initial learning - consider increasing learning rate\")\n",
    "    else:\n",
    "        print(f\"   ‚Ä¢ ‚úÖ Learning rate appears well-balanced\")\n",
    "    \n",
    "    # Temperature pattern insights\n",
    "    print(f\"\\n3. Temperature Pattern Insights:\")\n",
    "    theta0_actual = weather_gd_results['theta_final'][0] * y_std_w - weather_gd_results['theta_final'][1] * y_std_w * X_mean_w / X_std_w + y_mean_w\n",
    "    theta1_actual = weather_gd_results['theta_final'][1] * y_std_w / X_std_w\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Baseline temperature (Œ∏‚ÇÄ): {theta0_actual:.2f}\")\n",
    "    print(f\"   ‚Ä¢ Temperature change rate (Œ∏‚ÇÅ): {theta1_actual:.4f} per day index\")\n",
    "    print(f\"   ‚Ä¢ Estimated yearly temperature change: {theta1_actual * 3.65:.2f}\")\n",
    "    \n",
    "    if theta1_actual > 0:\n",
    "        print(f\"   ‚Ä¢ Pattern: Temperature increases throughout the year\")\n",
    "    else:\n",
    "        print(f\"   ‚Ä¢ Pattern: Temperature decreases throughout the year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## üèÜ 5.5 Cross-Station Gradient Descent Comparison\n",
    "\n",
    "# %%\n",
    "if run_weather_analysis and 'mean_temp_cols' in locals() and problem_type == 'regression':\n",
    "    compare_stations = input(\"\\nüëâ Compare gradient descent across multiple stations? (y/n): \").strip().lower() == 'y'\n",
    "    \n",
    "    if compare_stations:\n",
    "        print(\"\\nüèÜ CROSS-STATION COMPARISON\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Select stations to compare\n",
    "        n_stations = min(5, len(mean_temp_cols))\n",
    "        print(f\"\\nSelecting top {n_stations} stations for comparison...\")\n",
    "        \n",
    "        # Use the same experimental setup for all stations\n",
    "        if 'weather_gd_results' in locals():\n",
    "            exp_setup = weather_gd_results['experiment']\n",
    "        else:\n",
    "            exp_setup = {'theta_init': np.zeros(2), 'alpha': 0.01, 'iterations': 1000}\n",
    "        \n",
    "        comparison_results = {}\n",
    "        \n",
    "        for i, station_col in enumerate(mean_temp_cols[:n_stations]):\n",
    "            station_name = station_col.replace('_temp_mean', '')\n",
    "            print(f\"\\nProcessing {i+1}/{n_stations}: {station_name}...\", end=' ')\n",
    "            \n",
    "            # Prepare data for this station\n",
    "            if 'df_year_temp' in locals() and df_year_temp is not None:\n",
    "                y_station = df_year_temp[station_col].values\n",
    "            else:\n",
    "                y_station = df[station_col].values\n",
    "            \n",
    "            # Remove NaN values\n",
    "            mask = ~np.isnan(y_station)\n",
    "            X_station = X_weather[mask] if 'X_weather' in locals() else np.linspace(0, 1, len(y_station)).reshape(-1, 1)\n",
    "            y_station = y_station[mask]\n",
    "            \n",
    "            # Standardize\n",
    "            X_mean_s, X_std_s = X_station.mean(), X_station.std()\n",
    "            y_mean_s, y_std_s = y_station.mean(), y_station.std()\n",
    "            X_norm_s = (X_station - X_mean_s) / X_std_s\n",
    "            y_norm_s = (y_station - y_mean_s) / y_std_s\n",
    "            X_norm_s = np.c_[np.ones(X_norm_s.shape[0]), X_norm_s]\n",
    "            \n",
    "            # Run gradient descent\n",
    "            theta_s, cost_history_s, _ = gradient_descent(\n",
    "                X_norm_s, y_norm_s,\n",
    "                exp_setup['theta_init'].copy() if 'theta_init' in exp_setup else np.zeros(2),\n",
    "                exp_setup['alpha'],\n",
    "                exp_setup['iterations']\n",
    "            )\n",
    "            \n",
    "            # Calculate actual parameters\n",
    "            theta0_actual = theta_s[0] * y_std_s - theta_s[1] * y_std_s * X_mean_s / X_std_s + y_mean_s\n",
    "            theta1_actual = theta_s[1] * y_std_s / X_std_s\n",
    "            \n",
    "            # Calculate R¬≤\n",
    "            y_pred_s = X_norm_s.dot(theta_s)\n",
    "            y_pred_s = y_pred_s * y_std_s + y_mean_s\n",
    "            r2 = r2_score(y_station, y_pred_s)\n",
    "            \n",
    "            comparison_results[station_name] = {\n",
    "                'theta0': theta0_actual,\n",
    "                'theta1': theta1_actual,\n",
    "                'final_cost': cost_history_s[-1],\n",
    "                'r2': r2,\n",
    "                'converged': abs(cost_history_s[-1] - cost_history_s[-2]) < 1e-6,\n",
    "                'cost_history': cost_history_s\n",
    "            }\n",
    "            \n",
    "            print(f\"R¬≤ = {r2:.4f}\")\n",
    "        \n",
    "        # Create comparison visualization\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=('R¬≤ Scores', 'Temperature Slopes', 'Convergence Rates', 'Cost Evolution')\n",
    "        )\n",
    "        \n",
    "        # 1. R¬≤ scores\n",
    "        stations = list(comparison_results.keys())\n",
    "        r2_scores = [comparison_results[s]['r2'] for s in stations]\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(x=stations, y=r2_scores, name='R¬≤ Score'),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # 2. Temperature slopes\n",
    "        slopes = [comparison_results[s]['theta1'] for s in stations]\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(x=stations, y=slopes, name='Slope (Œ∏‚ÇÅ)'),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        # 3. Convergence status\n",
    "        converged = [1 if comparison_results[s]['converged'] else 0 for s in stations]\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(x=stations, y=converged, name='Converged'),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # 4. Cost evolution for all stations\n",
    "        for station in stations:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=list(range(len(comparison_results[station]['cost_history']))),\n",
    "                    y=comparison_results[station]['cost_history'],\n",
    "                    mode='lines',\n",
    "                    name=station\n",
    "                ),\n",
    "                row=2, col=2\n",
    "            )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title='Gradient Descent Performance Across Weather Stations',\n",
    "            height=800,\n",
    "            showlegend=True\n",
    "        )\n",
    "        \n",
    "        fig.update_yaxes(title_text='R¬≤ Score', row=1, col=1)\n",
    "        fig.update_yaxes(title_text='Slope', row=1, col=2)\n",
    "        fig.update_yaxes(title_text='Converged (1=Yes)', row=2, col=1)\n",
    "        fig.update_yaxes(title_text='Cost', row=2, col=2)\n",
    "        fig.update_xaxes(title_text='Iteration', row=2, col=2)\n",
    "        \n",
    "        fig.show()\n",
    "        \n",
    "        # Summary table\n",
    "        print(\"\\nüìä COMPARISON SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        comparison_df = pd.DataFrame(comparison_results).T\n",
    "        comparison_df = comparison_df.round(4)\n",
    "        print(comparison_df[['theta0', 'theta1', 'r2', 'converged', 'final_cost']])\n",
    "        \n",
    "        # Insights\n",
    "        print(\"\\nüí° Key Insights:\")\n",
    "        best_r2_station = max(comparison_results, key=lambda x: comparison_results[x]['r2'])\n",
    "        worst_r2_station = min(comparison_results, key=lambda x: comparison_results[x]['r2'])\n",
    "        \n",
    "        print(f\"   ‚Ä¢ Best fit: {best_r2_station} (R¬≤ = {comparison_results[best_r2_station]['r2']:.4f})\")\n",
    "        print(f\"   ‚Ä¢ Worst fit: {worst_r2_station} (R¬≤ = {comparison_results[worst_r2_station]['r2']:.4f})\")\n",
    "        \n",
    "        converged_count = sum(converged)\n",
    "        print(f\"   ‚Ä¢ Convergence rate: {converged_count}/{len(stations)} stations converged\")\n",
    "        \n",
    "        if converged_count < len(stations):\n",
    "            print(f\"   ‚Ä¢ Consider increasing iterations for non-converged stations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## üìÖ 5.6 Cross-Year Analysis (if multiple years available)\n",
    "\n",
    "# %%\n",
    "if run_weather_analysis and 'DATE' in df.columns and 'weather_station_selected' in locals():\n",
    "    # Check if we have multiple years\n",
    "    df['year'] = pd.to_datetime(df['DATE'], format='%Y%m%d').dt.year\n",
    "    unique_years = sorted(df['year'].unique())\n",
    "    \n",
    "    if len(unique_years) > 1:\n",
    "        compare_years = input(\"\\nüëâ Compare gradient descent across different years? (y/n): \").strip().lower() == 'y'\n",
    "        \n",
    "        if compare_years:\n",
    "            print(\"\\nüìÖ CROSS-YEAR COMPARISON\")\n",
    "            print(\"=\"*60)\n",
    "            print(f\"Station: {weather_station_name}\")\n",
    "            \n",
    "            # Select years to compare\n",
    "            print(f\"\\nAvailable years: {unique_years}\")\n",
    "            n_years = min(5, len(unique_years))\n",
    "            print(f\"Comparing last {n_years} years...\")\n",
    "            \n",
    "            selected_years = unique_years[-n_years:]\n",
    "            year_results = {}\n",
    "            \n",
    "            for year in selected_years:\n",
    "                print(f\"\\nProcessing year {year}...\", end=' ')\n",
    "                \n",
    "                # Get data for this year\n",
    "                df_year = df[df['year'] == year]\n",
    "                n_days = len(df_year)\n",
    "                \n",
    "                # Create scaled index\n",
    "                max_val = 3.66 if n_days == 366 else 3.65\n",
    "                day_index_year = np.linspace(0.01, max_val, n_days)\n",
    "                \n",
    "                X_year = day_index_year.reshape(-1, 1)\n",
    "                y_year = df_year[weather_station_selected].values\n",
    "                \n",
    "                # Remove NaN\n",
    "                mask = ~np.isnan(y_year)\n",
    "                X_year = X_year[mask]\n",
    "                y_year = y_year[mask]\n",
    "                \n",
    "                # Skip years with not enough data\n",
    "                if len(y_year) < 2:\n",
    "                    print(\"Not enough valid data. Skipping year.\")\n",
    "                    continue\n",
    "                \n",
    "                # Standardize\n",
    "                X_mean_y, X_std_y = X_year.mean(), X_year.std()\n",
    "                y_mean_y, y_std_y = y_year.mean(), y_year.std()\n",
    "                X_norm_y = (X_year - X_mean_y) / X_std_y\n",
    "                y_norm_y = (y_year - y_mean_y) / y_std_y\n",
    "                X_norm_y = np.c_[np.ones(X_norm_y.shape[0]), X_norm_y]\n",
    "                \n",
    "                # Run gradient descent\n",
    "                theta_y, cost_history_y, _ = gradient_descent(\n",
    "                    X_norm_y, y_norm_y,\n",
    "                    np.zeros(2),\n",
    "                    0.01,\n",
    "                    1000\n",
    "                )\n",
    "                \n",
    "                # Calculate metrics\n",
    "                y_pred_y = X_norm_y.dot(theta_y)\n",
    "                y_pred_y = y_pred_y * y_std_y + y_mean_y\n",
    "            \n",
    "                # Extra nan check for safety\n",
    "                nan_mask = (~np.isnan(y_year)) & (~np.isnan(y_pred_y))\n",
    "                if nan_mask.sum() < 2:\n",
    "                    print(\"Not enough valid predictions. Skipping year.\")\n",
    "                    continue\n",
    "                y_year_valid = y_year[nan_mask]\n",
    "                y_pred_y_valid = y_pred_y[nan_mask]\n",
    "            \n",
    "                r2_y = r2_score(y_year_valid, y_pred_y_valid)\n",
    "                \n",
    "                year_results[year] = {\n",
    "                    'mean_temp': y_year_valid.mean(),\n",
    "                    'std_temp': y_year_valid.std(),\n",
    "                    'r2': r2_y,\n",
    "                    'final_cost': cost_history_y[-1],\n",
    "                    'n_days': len(X_year)\n",
    "                }\n",
    "                \n",
    "                print(f\"R¬≤ = {r2_y:.4f}\")\n",
    "            \n",
    "            # Create year comparison visualization\n",
    "            fig = make_subplots(\n",
    "                rows=2, cols=2,\n",
    "                subplot_titles=('Mean Temperature', 'R¬≤ Score', 'Temperature Std Dev', 'Sample Size')\n",
    "            )\n",
    "            \n",
    "            years = list(year_results.keys())\n",
    "            \n",
    "            # Mean temperature\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=years, y=[year_results[y]['mean_temp'] for y in years], \n",
    "                          mode='lines+markers', name='Mean Temp'),\n",
    "                row=1, col=1\n",
    "            )\n",
    "            \n",
    "            # R¬≤ scores\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=years, y=[year_results[y]['r2'] for y in years], \n",
    "                          mode='lines+markers', name='R¬≤ Score'),\n",
    "                row=1, col=2\n",
    "            )\n",
    "            \n",
    "            # Standard deviation\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=years, y=[year_results[y]['std_temp'] for y in years], \n",
    "                          mode='lines+markers', name='Std Dev'),\n",
    "                row=2, col=1\n",
    "            )\n",
    "            \n",
    "            # Sample size\n",
    "            fig.add_trace(\n",
    "                go.Bar(x=years, y=[year_results[y]['n_days'] for y in years], \n",
    "                       name='Days'),\n",
    "                row=2, col=2\n",
    "            )\n",
    "            \n",
    "            fig.update_layout(\n",
    "                title=f'Year-over-Year Analysis - {weather_station_name}',\n",
    "                height=800,\n",
    "                showlegend=True\n",
    "            )\n",
    "            \n",
    "            fig.show()\n",
    "            \n",
    "            # Summary\n",
    "            print(\"\\nüìä YEAR COMPARISON SUMMARY\")\n",
    "            print(\"=\"*60)\n",
    "            year_df = pd.DataFrame(year_results).T\n",
    "            print(year_df.round(4))\n",
    "            \n",
    "            # Climate trends\n",
    "            print(\"\\nüå°Ô∏è Climate Trends:\")\n",
    "            temp_trend = np.polyfit(years, [year_results[y]['mean_temp'] for y in years], 1)\n",
    "            print(f\"   ‚Ä¢ Average temperature trend: {temp_trend[0]:.4f} per year\")\n",
    "            if temp_trend[0] > 0:\n",
    "                print(f\"   ‚Ä¢ Climate warming detected: +{temp_trend[0]*10:.2f} over 10 years\")\n",
    "            else:\n",
    "                print(f\"   ‚Ä¢ Climate cooling detected: {temp_trend[0]*10:.2f} over 10 years\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ 6. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = df[selected_features]\n",
    "y = df['target']\n",
    "\n",
    "# Feature selection if too many features\n",
    "if X.shape[1] > 50:\n",
    "    k = min(30, X.shape[1] // 2)\n",
    "    if problem_type == 'classification':\n",
    "        selector = SelectKBest(f_classif, k=k)\n",
    "    else:\n",
    "        selector = SelectKBest(f_regression, k=k)\n",
    "    \n",
    "    X_selected = selector.fit_transform(X, y)\n",
    "    selected_features = X.columns[selector.get_support()].tolist()\n",
    "    X = pd.DataFrame(X_selected, columns=selected_features)\n",
    "    print(f\"\\n‚úÖ Reduced features from {df[selected_features].shape[1]} to {k}\")\n",
    "\n",
    "# Train-test split\n",
    "if problem_type == 'classification':\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "print(f\"\\n‚úÇÔ∏è Data split: {X_train.shape[0]:,} train, {X_test.shape[0]:,} test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'run_gradient_descent' in locals() and run_gradient_descent and 'theta_final' in locals():\n",
    "    # Create 3D visualization of the loss function\n",
    "    print(\"\\nüìä Creating 3D Loss Function Visualization...\")\n",
    "    \n",
    "    # Create grid of theta values for 3D plot\n",
    "    theta0_range = np.linspace(theta_final[0] - 2, theta_final[0] + 2, 50)\n",
    "    theta1_range = np.linspace(theta_final[1] - 2, theta_final[1] + 2, 50)\n",
    "    theta0_grid, theta1_grid = np.meshgrid(theta0_range, theta1_range)\n",
    "    \n",
    "    # Compute cost for each combination\n",
    "    cost_grid = np.zeros_like(theta0_grid)\n",
    "    for i in range(len(theta0_range)):\n",
    "        for j in range(len(theta1_range)):\n",
    "            theta_temp = np.array([theta0_grid[i, j], theta1_grid[i, j]])\n",
    "            cost_grid[i, j] = compute_cost(X_norm, y_norm, theta_temp)\n",
    "    \n",
    "    # Create 3D surface plot\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        specs=[[{'type': 'surface', 'rowspan': 2}, {'type': 'scatter'}],\n",
    "               [None, {'type': 'scatter'}]],\n",
    "        subplot_titles=('3D Loss Function', 'Loss vs Iterations', 'Parameter Evolution')\n",
    "    )\n",
    "    \n",
    "    # 3D Surface\n",
    "    fig.add_trace(\n",
    "        go.Surface(\n",
    "            x=theta0_range,\n",
    "            y=theta1_range,\n",
    "            z=cost_grid,\n",
    "            colorscale='Viridis',\n",
    "            opacity=0.7,\n",
    "            name='Loss Surface'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Add gradient descent path on 3D surface\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=theta_history[:, 0],\n",
    "            y=theta_history[:, 1],\n",
    "            z=cost_history,\n",
    "            mode='lines+markers',\n",
    "            line=dict(color='red', width=4),\n",
    "            marker=dict(size=4),\n",
    "            name='GD Path'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Loss vs Iterations\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=list(range(len(cost_history))),\n",
    "            y=cost_history,\n",
    "            mode='lines',\n",
    "            name='Cost',\n",
    "            line=dict(color='blue')\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Parameter evolution\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=list(range(len(theta_history))),\n",
    "            y=theta_history[:, 0],\n",
    "            mode='lines',\n",
    "            name='Œ∏‚ÇÄ',\n",
    "            line=dict(color='green')\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=list(range(len(theta_history))),\n",
    "            y=theta_history[:, 1],\n",
    "            mode='lines',\n",
    "            name='Œ∏‚ÇÅ',\n",
    "            line=dict(color='orange')\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title='Gradient Descent Visualization',\n",
    "        height=800,\n",
    "        showlegend=True,\n",
    "        legend=dict(\n",
    "            x=0,               # X position, 0 (left) to 1 (right)\n",
    "            y=0,               # Y position, 0 (bottom) to 1 (top)\n",
    "            xanchor='left',    # or 'right', 'center'\n",
    "            yanchor='bottom',  # or 'left', 'middle'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Update axes\n",
    "    fig.update_xaxes(title_text='Œ∏‚ÇÄ', row=1, col=1)\n",
    "    fig.update_yaxes(title_text='Œ∏‚ÇÅ', row=1, col=1)\n",
    "    fig.update_xaxes(title_text='Iteration', row=1, col=2)\n",
    "    fig.update_yaxes(title_text='Cost', row=1, col=2)\n",
    "    fig.update_xaxes(title_text='Iteration', row=2, col=2)\n",
    "    fig.update_yaxes(title_text='Parameter Value', row=2, col=2)\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Create contour plot with gradient descent path\n",
    "    fig2 = go.Figure()\n",
    "    \n",
    "    # Add contour\n",
    "    fig2.add_trace(\n",
    "        go.Contour(\n",
    "            x=theta0_range,\n",
    "            y=theta1_range,\n",
    "            z=cost_grid,\n",
    "            colorscale='Viridis',\n",
    "            showscale=True,\n",
    "            name='Loss Contours'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Add gradient descent path\n",
    "    fig2.add_trace(\n",
    "        go.Scatter(\n",
    "            x=theta_history[:, 0],\n",
    "            y=theta_history[:, 1],\n",
    "            mode='lines+markers',\n",
    "            line=dict(color='red', width=3),\n",
    "            marker=dict(size=8, color='red'),\n",
    "            name='GD Path'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Mark start and end points\n",
    "    fig2.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[theta_init[0]],\n",
    "            y=[theta_init[1]],\n",
    "            mode='markers',\n",
    "            marker=dict(size=15, color='green', symbol='star'),\n",
    "            name='Start'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig2.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[theta_final[0]],\n",
    "            y=[theta_final[1]],\n",
    "            mode='markers',\n",
    "            marker=dict(size=15, color='blue', symbol='star'),\n",
    "            name='End'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig2.update_layout(\n",
    "        title='Gradient Descent Path on Loss Contours',\n",
    "        xaxis_title='Œ∏‚ÇÄ',\n",
    "        yaxis_title='Œ∏‚ÇÅ',\n",
    "        height=600,\n",
    "        width=800,\n",
    "        showlegend=True,\n",
    "        legend=dict(\n",
    "            x=0,               # X position, 0 (left) to 1 (right)\n",
    "            y=0,               # Y position, 0 (bottom) to 1 (top)\n",
    "            xanchor='left',    # or 'right', 'center'\n",
    "            yanchor='bottom',  # or 'left', 'middle'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig2.show()\n",
    "    \n",
    "    # Visualize the fitted line\n",
    "    fig3 = go.Figure()\n",
    "    \n",
    "    # Scatter plot of original data\n",
    "    sample_size = min(500, len(X_gd))  # Sample for better visualization\n",
    "    sample_idx = np.random.choice(len(X_gd), sample_size, replace=False)\n",
    "    \n",
    "    fig3.add_trace(\n",
    "        go.Scatter(\n",
    "            x=X_gd[sample_idx].flatten(),\n",
    "            y=y_gd[sample_idx],\n",
    "            mode='markers',\n",
    "            marker=dict(size=5, opacity=0.5),\n",
    "            name='Data Points'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Fitted line (need to transform back from normalized space)\n",
    "    x_line = np.linspace(X_gd.min(), X_gd.max(), 100)\n",
    "    x_line_norm = (x_line - X_mean) / X_std\n",
    "    X_line_norm = np.c_[np.ones(len(x_line)), x_line_norm]\n",
    "    y_line_norm = X_line_norm.dot(theta_final)\n",
    "    y_line = y_line_norm * y_std + y_mean\n",
    "    \n",
    "    fig3.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x_line,\n",
    "            y=y_line,\n",
    "            mode='lines',\n",
    "            line=dict(color='red', width=3),\n",
    "            name='Fitted Line'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig3.update_layout(\n",
    "        title=f'Gradient Descent Result: {selected_feature} vs {target_col}',\n",
    "        xaxis_title=selected_feature,\n",
    "        yaxis_title=target_col,\n",
    "        height=500,\n",
    "        showlegend=True,\n",
    "        legend=dict(\n",
    "            x=0,               # X position, 0 (left) to 1 (right)\n",
    "            y=0,               # Y position, 0 (bottom) to 1 (top)\n",
    "            xanchor='left',    # or 'right', 'center'\n",
    "            yanchor='bottom',  # or 'left', 'middle'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig3.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\nüìä GRADIENT DESCENT SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nAlgorithm Performance:\")\n",
    "    print(f\"  ‚Ä¢ Initial cost: {cost_history[0]:.6f}\")\n",
    "    print(f\"  ‚Ä¢ Final cost: {cost_history[-1]:.6f}\")\n",
    "    print(f\"  ‚Ä¢ Cost reduction: {(1 - cost_history[-1]/cost_history[0])*100:.2f}%\")\n",
    "    print(f\"  ‚Ä¢ Convergence: {'Yes' if abs(cost_history[-1] - cost_history[-2]) < 1e-6 else 'No'}\")\n",
    "    \n",
    "    print(f\"\\nLearning Insights:\")\n",
    "    if alpha >= 1.0 and cost_history[-1] > cost_history[0]:\n",
    "        print(f\"  ‚ö†Ô∏è Learning rate too high! The cost increased.\")\n",
    "    elif alpha <= 0.001 and iterations < 1000:\n",
    "        print(f\"  ‚ö†Ô∏è Learning rate too low! May need more iterations.\")\n",
    "    else:\n",
    "        print(f\"  ‚úÖ Learning rate appears appropriate.\")\n",
    "    \n",
    "    # Compare with sklearn\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_gd, y_gd)\n",
    "    \n",
    "    print(f\"\\nComparison with sklearn LinearRegression:\")\n",
    "    print(f\"  ‚Ä¢ Our Œ∏‚ÇÄ: {theta_final[0] * y_std - theta_final[1] * y_std * X_mean / X_std + y_mean:.4f}\")\n",
    "    print(f\"  ‚Ä¢ sklearn intercept: {lr.intercept_:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Our Œ∏‚ÇÅ: {theta_final[1] * y_std / X_std:.4f}\")\n",
    "    print(f\"  ‚Ä¢ sklearn coefficient: {lr.coef_[0]:.4f}\")\n",
    "    \n",
    "    # Store gradient descent results for comparison\n",
    "    # Transform predictions back to original scale\n",
    "    X_test_gd = X_test[selected_feature].values.reshape(-1, 1)\n",
    "    X_test_norm = (X_test_gd - X_mean) / X_std\n",
    "    X_test_norm = np.c_[np.ones(X_test_norm.shape[0]), X_test_norm]\n",
    "    y_pred_norm = X_test_norm.dot(theta_final)\n",
    "    y_pred_gd = y_pred_norm * y_std + y_mean\n",
    "    \n",
    "    # Calculate metrics for gradient descent\n",
    "    gd_metrics = {\n",
    "        'r2': r2_score(y_test, y_pred_gd),\n",
    "        'mse': mean_squared_error(y_test, y_pred_gd),\n",
    "        'rmse': np.sqrt(mean_squared_error(y_test, y_pred_gd)),\n",
    "        'mae': mean_absolute_error(y_test, y_pred_gd),\n",
    "        'cv_score': 0.0  # No CV for manual implementation\n",
    "    }\n",
    "    \n",
    "    # Add to results\n",
    "    results['Gradient Descent (Manual)'] = {\n",
    "        'metrics': gd_metrics,\n",
    "        'best_params': {\n",
    "            'learning_rate': alpha,\n",
    "            'iterations': iterations,\n",
    "            'feature': selected_feature\n",
    "        },\n",
    "        'predictions': y_pred_gd\n",
    "    }\n",
    "    \n",
    "    best_models['Gradient Descent (Manual)'] = {\n",
    "        'theta': theta_final,\n",
    "        'feature': selected_feature,\n",
    "        'normalization': {'X_mean': X_mean, 'X_std': X_std, 'y_mean': y_mean, 'y_std': y_std}\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n‚úÖ Gradient Descent results added to model comparison!\")\n",
    "    print(f\"   Test R¬≤: {gd_metrics['r2']:.4f}\")\n",
    "    print(f\"   Test RMSE: {gd_metrics['rmse']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ 7. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models based on problem type\n",
    "if problem_type == 'classification':\n",
    "    # Check class balance\n",
    "    class_props = y_train.value_counts(normalize=True)\n",
    "    is_balanced = class_props.min() >= 0.2\n",
    "    class_weight = None if is_balanced else 'balanced'\n",
    "    \n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000, class_weight=class_weight),\n",
    "        'Decision Tree': DecisionTreeClassifier(max_depth=10, class_weight=class_weight),\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=10, class_weight=class_weight, n_jobs=-1),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, max_depth=5),\n",
    "        'SVM': SVC(kernel='rbf', probability=True, class_weight=class_weight),\n",
    "        'SGD Classifier': SGDClassifier(max_iter=1000, tol=1e-3, class_weight=class_weight, random_state=42)\n",
    "    }\n",
    "    \n",
    "    # Simplified parameter grids\n",
    "    param_grids = {\n",
    "        'Logistic Regression': {'C': [0.1, 1, 10]},\n",
    "        'Decision Tree': {'max_depth': [5, 10, 15], 'min_samples_split': [2, 10]},\n",
    "        'Random Forest': {'n_estimators': [50, 100], 'max_depth': [10, 20]},\n",
    "        'Gradient Boosting': {'n_estimators': [50, 100], 'learning_rate': [0.1, 0.2]},\n",
    "        'SVM': {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto']},\n",
    "        'SGD Classifier': {'alpha': [0.0001, 0.001, 0.01], 'penalty': ['l2', 'l1', 'elasticnet']}\n",
    "    }\n",
    "    \n",
    "    cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scoring = 'balanced_accuracy' if not is_balanced else 'accuracy'\n",
    "    \n",
    "else:  # regression\n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Ridge': Ridge(),\n",
    "        'Lasso': Lasso(max_iter=2000),\n",
    "        'Decision Tree': DecisionTreeRegressor(max_depth=10),\n",
    "        'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=10, n_jobs=-1),\n",
    "        'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, max_depth=5),\n",
    "        'SVR': SVR(kernel='rbf'),\n",
    "        'SGD Regressor': SGDRegressor(max_iter=1000, tol=1e-3, random_state=42)\n",
    "    }\n",
    "    \n",
    "    param_grids = {\n",
    "        'Linear Regression': {},\n",
    "        'Ridge': {'alpha': [0.1, 1, 10]},\n",
    "        'Lasso': {'alpha': [0.01, 0.1, 1]},\n",
    "        'Decision Tree': {'max_depth': [5, 10, 15], 'min_samples_split': [2, 10]},\n",
    "        'Random Forest': {'n_estimators': [50, 100], 'max_depth': [10, 20]},\n",
    "        'Gradient Boosting': {'n_estimators': [50, 100], 'learning_rate': [0.1, 0.2]},\n",
    "        'SVR': {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto']},\n",
    "        'SGD Regressor': {'alpha': [0.0001, 0.001, 0.01], 'penalty': ['l2', 'l1', 'elasticnet']}\n",
    "    }\n",
    "    \n",
    "    cv_strategy = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scoring = 'r2'\n",
    "\n",
    "print(f\"\\nü§ñ Training {len(models)} models for {problem_type}...\")\n",
    "print(f\"   Scoring metric: {scoring}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models\n",
    "# Initialize results - check if gradient descent was already run\n",
    "if 'results' not in locals():\n",
    "    results = {}\n",
    "if 'best_models' not in locals():\n",
    "    best_models = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n Training {name}...\", end=' ')\n",
    "    \n",
    "    # Grid search\n",
    "    grid = GridSearchCV(model, param_grids[name], cv=cv_strategy, scoring=scoring, n_jobs=-1)\n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    # Store best model\n",
    "    best_models[name] = grid.best_estimator_\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = grid.best_estimator_.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    if problem_type == 'classification':\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'balanced_accuracy': balanced_accuracy_score(y_test, y_pred),\n",
    "            'f1_score': f1_score(y_test, y_pred, average='weighted'),\n",
    "            'cv_score': grid.best_score_\n",
    "        }\n",
    "    else:\n",
    "        metrics = {\n",
    "            'r2': r2_score(y_test, y_pred),\n",
    "            'mse': mean_squared_error(y_test, y_pred),\n",
    "            'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "            'mae': mean_absolute_error(y_test, y_pred),\n",
    "            'cv_score': grid.best_score_\n",
    "        }\n",
    "    \n",
    "    results[name] = {\n",
    "        'metrics': metrics,\n",
    "        'best_params': grid.best_params_,\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úì CV Score: {grid.best_score_:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ All models trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 8. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_data = []\n",
    "for name, result in results.items():\n",
    "    row = {'Model': name}\n",
    "    row.update(result['metrics'])\n",
    "    comparison_data.append(row)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Handle different metrics for gradient descent\n",
    "if 'Gradient Descent (Manual)' in comparison_df['Model'].values:\n",
    "    # For gradient descent, CV score is not available, so fill with NaN\n",
    "    comparison_df.loc[comparison_df['Model'] == 'Gradient Descent (Manual)', 'cv_score'] = np.nan\n",
    "\n",
    "# Sort by primary metric\n",
    "if problem_type == 'classification':\n",
    "    comparison_df = comparison_df.sort_values('balanced_accuracy', ascending=False)\n",
    "    primary_metric = 'balanced_accuracy'\n",
    "else:\n",
    "    comparison_df = comparison_df.sort_values('r2', ascending=False)\n",
    "    primary_metric = 'r2'\n",
    "\n",
    "print(\"\\nüìä Model Performance Comparison:\")\n",
    "print(comparison_df.to_string(index=False, float_format='%.4f'))\n",
    "\n",
    "# Get best model (excluding gradient descent for best model selection if it's just a demo)\n",
    "models_for_best = comparison_df[comparison_df['Model'] != 'Gradient Descent (Manual)']\n",
    "if len(models_for_best) > 0:\n",
    "    best_model_name = models_for_best.iloc[0]['Model']\n",
    "else:\n",
    "    best_model_name = comparison_df.iloc[0]['Model']\n",
    "    \n",
    "print(f\"\\nüèÜ Best Model: {best_model_name}\")\n",
    "\n",
    "# Note about gradient descent if it was run\n",
    "if 'Gradient Descent (Manual)' in comparison_df['Model'].values:\n",
    "    gd_rank = comparison_df[comparison_df['Model'] == 'Gradient Descent (Manual)'].index[0] + 1\n",
    "    print(f\"\\nüìù Note: Gradient Descent (Manual) ranked #{gd_rank} - uses only one feature ({results['Gradient Descent (Manual)']['best_params']['feature']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà 9. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison visualization\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=('Model Performance', 'Best Model Analysis'))\n",
    "\n",
    "# Performance comparison\n",
    "fig.add_trace(\n",
    "    go.Bar(x=comparison_df['Model'], y=comparison_df[primary_metric], name=primary_metric),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Best model analysis\n",
    "best_model = best_models[best_model_name]\n",
    "best_pred = results[best_model_name]['predictions']\n",
    "\n",
    "if problem_type == 'classification':\n",
    "    # Confusion matrix for classification\n",
    "    cm = confusion_matrix(y_test, best_pred)\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(z=cm, text=cm, texttemplate='%{text}', colorscale='Blues'),\n",
    "        row=1, col=2\n",
    "    )\n",
    "else:\n",
    "    # Actual vs Predicted for regression\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=y_test, y=best_pred, mode='markers', name='Predictions'),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    # Add diagonal line\n",
    "    min_val = min(y_test.min(), best_pred.min())\n",
    "    max_val = max(y_test.max(), best_pred.max())\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=[min_val, max_val], y=[min_val, max_val], \n",
    "                  mode='lines', name='Perfect Prediction', line=dict(dash='dash')),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "fig.update_layout(height=500, showlegend=True, title_text=f\"Model Analysis - {problem_type.title()}\")\n",
    "fig.show()\n",
    "\n",
    "# Feature importance (if available)\n",
    "if best_model_name == 'Gradient Descent (Manual)':\n",
    "    # For gradient descent, show the single feature coefficient\n",
    "    print(f\"\\nüìä Gradient Descent Coefficient:\")\n",
    "    print(f\"   Feature: {best_models[best_model_name]['feature']}\")\n",
    "    print(f\"   Œ∏‚ÇÅ (slope): {best_models[best_model_name]['theta'][1]:.4f}\")\n",
    "    print(f\"   Œ∏‚ÇÄ (intercept): {best_models[best_model_name]['theta'][0]:.4f}\")\n",
    "elif hasattr(best_model, 'feature_importances_'):\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False).head(15)\n",
    "    \n",
    "    fig = px.bar(importance_df, y='feature', x='importance', orientation='h',\n",
    "                title=f'Top 15 Feature Importances - {best_model_name}')\n",
    "    fig.update_layout(height=500)\n",
    "    fig.show()\n",
    "elif hasattr(best_model, 'coef_'):\n",
    "    # For linear models\n",
    "    if problem_type == 'classification' and len(np.unique(y_train)) == 2:\n",
    "        coef = best_model.coef_[0]\n",
    "    else:\n",
    "        coef = best_model.coef_\n",
    "    \n",
    "    coef_df = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'coefficient': np.abs(coef)\n",
    "    }).sort_values('coefficient', ascending=False).head(15)\n",
    "    \n",
    "    fig = px.bar(coef_df, y='feature', x='coefficient', orientation='h',\n",
    "                title=f'Top 15 Feature Coefficients - {best_model_name}')\n",
    "    fig.update_layout(height=500)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ 10. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = Path('model_results') / datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save results summary\n",
    "summary = {\n",
    "    'analysis_date': datetime.now().strftime('%Y-%m-%d %H:%M'),\n",
    "    'problem_type': problem_type,\n",
    "    'dataset_info': {\n",
    "        'total_samples': len(df),\n",
    "        'features_used': len(selected_features),\n",
    "        'target_variable': target_col\n",
    "    },\n",
    "    'model_comparison': comparison_df.to_dict('records'),\n",
    "    'best_model': {\n",
    "        'name': best_model_name,\n",
    "        'parameters': results[best_model_name]['best_params'],\n",
    "        'metrics': results[best_model_name]['metrics']\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(output_dir / 'results_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=4)\n",
    "\n",
    "# Save models\n",
    "for name, model in best_models.items():\n",
    "    joblib.dump(model, output_dir / f\"{name.lower().replace(' ', '_')}.pkl\")\n",
    "\n",
    "# Save predictions\n",
    "predictions_df = pd.DataFrame({\n",
    "    'actual': y_test,\n",
    "    'predicted': results[best_model_name]['predictions']\n",
    "})\n",
    "predictions_df.to_csv(output_dir / 'predictions.csv', index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Results saved to: {output_dir}\")\n",
    "print(f\"\\nüéâ Analysis complete! Best model: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 11. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä FINAL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüéØ Problem Type: {problem_type.upper()}\")\n",
    "print(f\"   Target Variable: {target_col}\")\n",
    "print(f\"   Features Used: {len(selected_features)}\")\n",
    "print(f\"   Training Samples: {X_train.shape[0]:,}\")\n",
    "print(f\"   Test Samples: {X_test.shape[0]:,}\")\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name}\")\n",
    "for metric, value in results[best_model_name]['metrics'].items():\n",
    "    print(f\"   {metric}: {value:.4f}\")\n",
    "\n",
    "print(f\"\\nüìÅ All results saved to: {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
