{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Universal ML Model Training - Multi-Purpose Machine Learning Pipeline\n",
    "\n",
    "This notebook provides a comprehensive machine learning pipeline that automatically adapts to your dataset and problem type. It supports:\n",
    "\n",
    "## üéØ Problem Types:\n",
    "1. **Binary Classification** - Two-class prediction (e.g., pleasant/unpleasant weather)\n",
    "2. **Multi-class Classification** - Multiple class prediction (e.g., weather categories)\n",
    "3. **Regression** - Continuous value prediction (e.g., temperature, humidity)\n",
    "\n",
    "## üåü Key Features:\n",
    "- **Automatic problem type detection** based on target variable\n",
    "- **Multi-station support** for weather or sensor data\n",
    "- **Adaptive model selection** - Different algorithms for classification vs regression\n",
    "- **Educational gradient descent** implementation for single-feature regression\n",
    "- **Comprehensive evaluation** with appropriate metrics for each problem type\n",
    "- **Interactive feature selection** with multiple strategies\n",
    "- **Station-wise analysis** for multi-location datasets\n",
    "- **Overfitting detection** and model recommendations\n",
    "\n",
    "## üìÅ Expected Structure\n",
    "```\n",
    "Your Project/\n",
    "‚îú‚îÄ‚îÄ 02_data/Processed_data/    ‚Üê Pre-scaled data\n",
    "‚îú‚îÄ‚îÄ 03_notebooks/              ‚Üê Run from here\n",
    "‚îî‚îÄ‚îÄ 05_results/                ‚Üê Output files\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration\n",
    "\n",
    "Define all configuration parameters used throughout the analysis. These settings control data quality thresholds, model parameters, and analysis behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded successfully!\n",
      "\n",
      "üìã Key Settings:\n",
      "  ‚Ä¢ target_column: pleasant_weather\n",
      "  ‚Ä¢ station_column: station_id\n",
      "  ‚Ä¢ missing_threshold: 0.04\n",
      "  ‚Ä¢ critical_features: ['temp', 'humidity', 'pressure', 'wind']\n",
      "  ‚Ä¢ overfitting_threshold: 0.05\n",
      "  ‚Ä¢ high_accuracy_threshold: 0.95\n",
      "  ‚Ä¢ exclude_patterns: ['date', 'time', 'year', 'month', 'day', 'hour', 'minute', 'id']\n"
     ]
    }
   ],
   "source": [
    "# Configuration settings for the analysis\n",
    "CONFIG = {\n",
    "    'target_column': 'pleasant_weather',  # Column name pattern for pleasant weather labels\n",
    "    'station_column': 'station_id',  # Column name for station identifiers\n",
    "    'missing_threshold': 0.04,  # Maximum allowed missing data per station\n",
    "    'critical_features': ['temp', 'humidity', 'pressure', 'wind'],  # Features to check for missing data\n",
    "    'overfitting_threshold': 0.05,  # Maximum train-test score difference\n",
    "    'high_accuracy_threshold': 0.95,  # Threshold for high-accuracy station reporting\n",
    "    'exclude_patterns': ['date', 'time', 'year', 'month', 'day', 'hour', 'minute', 'id'],  # Temporal features to exclude\n",
    "    'weather_patterns': {  # Patterns to identify weather feature types\n",
    "        'temperature': ['temp'],\n",
    "        'humidity': ['humid', 'moisture'],\n",
    "        'pressure': ['pressure', 'press'],\n",
    "        'wind': ['wind', 'gust'],\n",
    "        'precipitation': ['rain', 'precip', 'snow'],\n",
    "        'radiation': ['radiation', 'solar'],\n",
    "        'visibility': ['visibility', 'vis'],\n",
    "        'clouds': ['cloud']\n",
    "    },\n",
    "    'neural_network_params': {\n",
    "        'hidden_layer_sizes': [(50,), (100,), (100,50), (100,75,50), (200,100,50)],\n",
    "        'max_iter': [500, 1000, 2000],\n",
    "        'tol': [1e-3, 1e-4, 1e-5],\n",
    "        'activation': ['relu', 'tanh'],\n",
    "        'solver': ['adam', 'lbfgs']\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Configuration loaded successfully!\")\n",
    "print(\"\\nüìã Key Settings:\")\n",
    "for key, value in CONFIG.items():\n",
    "    if key not in ['neural_network_params', 'weather_patterns']:\n",
    "        print(f\"  ‚Ä¢ {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö 1. Setup and Imports\n",
    "\n",
    "Import all required libraries for:\n",
    "- **Data Processing**: numpy, pandas, pathlib\n",
    "- **Machine Learning**: scikit-learn models and utilities\n",
    "- **Visualization**: matplotlib, seaborn, plotly\n",
    "- **Progress Tracking**: tqdm for training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.17.1.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n",
      "üìÖ Analysis date: 2025-06-09 05:20\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import joblib\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# Progress bar imports\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "\n",
    "# ML imports - Model Selection and Splitting\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, cross_val_score, GridSearchCV, \n",
    "    KFold, StratifiedKFold\n",
    ")\n",
    "\n",
    "# ML imports - Metrics\n",
    "from sklearn.metrics import (\n",
    "    # Classification metrics\n",
    "    accuracy_score, balanced_accuracy_score, precision_score, \n",
    "    recall_score, f1_score, roc_auc_score, confusion_matrix,\n",
    "    classification_report,\n",
    "    # Regression metrics\n",
    "    mean_squared_error, mean_absolute_error, r2_score,\n",
    "    explained_variance_score\n",
    ")\n",
    "\n",
    "# ML imports - Preprocessing and Feature Selection\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_selection import (\n",
    "    SelectKBest, f_classif, f_regression, \n",
    "    mutual_info_classif, mutual_info_regression\n",
    ")\n",
    "\n",
    "# ML imports - Models\n",
    "from sklearn.linear_model import (\n",
    "    LogisticRegression, LinearRegression, Ridge, Lasso, \n",
    "    ElasticNet, SGDClassifier, SGDRegressor\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, RandomForestRegressor, \n",
    "    GradientBoostingClassifier, GradientBoostingRegressor\n",
    ")\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.offline as pyo\n",
    "pyo.init_notebook_mode(connected=True)\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üìÖ Analysis date: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• 2. Load and Merge Data\n",
    "\n",
    "### Data Loading Process:\n",
    "1. **Interactive path setup** - Select project root directory\n",
    "2. **File selection** - Choose which CSV files to load\n",
    "3. **Dataset identification** - Automatically identify features vs. target datasets\n",
    "4. **Data merging** - Merge datasets on common keys (date/station columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìç Current directory: C:\\Users\\User\\Dropbox\\Personal\\CareerFoundry\\07 Machine Learning\\ML\\03_notebooks\n",
      "üìÅ Project root: C:\\Users\\User\\Dropbox\\Personal\\CareerFoundry\\07 Machine Learning\\ML\n",
      "\n",
      "============================================================\n",
      "üì• SELECT INPUT FOLDER\n",
      "============================================================\n",
      "\n",
      "üìã Available folders in project:\n",
      "   1: 01_roject_management\n",
      "   2: 02_data\n",
      "   3: 03_notebooks\n",
      "   4: 04_analysis\n",
      "   5: 05_results\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Choose input folder number (1-5):  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Selected: 02_data\n",
      "\n",
      "----------------------------------------\n",
      "üìÇ Subfolders in '02_data':\n",
      "   0: Use '02_data' (parent folder)\n",
      "   1: Merged_data\n",
      "   2: Original_data\n",
      "   3: Processed_data\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Choose subfolder (0-3) [Enter for 0]:  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Input path set to: C:\\Users\\User\\Dropbox\\Personal\\CareerFoundry\\07 Machine Learning\\ML\\02_data\\Processed_data\n",
      "\n",
      "\n",
      "============================================================\n",
      "üì§ SELECT OUTPUT FOLDER\n",
      "============================================================\n",
      "\n",
      "üìã Available folders in project:\n",
      "   1: 01_roject_management\n",
      "   2: 02_data\n",
      "   3: 03_notebooks\n",
      "   4: 04_analysis\n",
      "   5: 05_results\n",
      "\n",
      "   üí° Press Enter to use input folder: 02_data\\Processed_data\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Choose output folder number (1-5) [Enter for input folder]:  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Selected: 02_data\n",
      "\n",
      "----------------------------------------\n",
      "üìÇ Subfolders in '02_data':\n",
      "   0: Use '02_data' (parent folder)\n",
      "   1: Merged_data\n",
      "   2: Original_data\n",
      "   3: Processed_data\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Choose subfolder (0-3) [Enter for 0]:  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "============================================================\n",
      "‚úÖ PROJECT SETUP COMPLETE!\n",
      "============================================================\n",
      "\n",
      "   üì• Input path:  C:\\Users\\User\\Dropbox\\Personal\\CareerFoundry\\07 Machine Learning\\ML\\02_data\\Processed_data\n",
      "   üì§ Output path: C:\\Users\\User\\Dropbox\\Personal\\CareerFoundry\\07 Machine Learning\\ML\\02_data\\Processed_data\n",
      "\n",
      "============================================================\n",
      "\n",
      "üìã Available data files:\n",
      "   1: üìä Dataset-Answers-Weather_Prediction_Pleasant_Weather.csv (CSV)\n",
      "   2: üìä Dataset-Answers-Weather_Prediction_Pleasant_Weather_test.csv (CSV)\n",
      "   3: üìä Dataset-weather-prediction-dataset-processed_scaled_20250528_1500.csv (CSV)\n",
      "   4: üìä Dataset-weather-prediction-dataset-processed_scaled_20250528_1500_test.csv (CSV)\n",
      "\n",
      "üîç How would you like to select files?\n",
      "   1: Select specific files\n",
      "   2: Load all files\n",
      "   3: Load files by type (CSV, Excel, etc.)\n",
      "   4: Load files matching a pattern\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Choose selection mode (1-4):  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå Select files (separate numbers with commas, e.g., 1,3,5)\n",
      "   Or use ranges (e.g., 1-3,5,7-9)\n",
      "   Press Enter to select all files\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Enter file numbers:  1,3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Loading 2 files...\n",
      "\n",
      "[1/2] Loading: Dataset-Answers-Weather_Prediction_Pleasant_Weather.csv\n",
      "   ‚úÖ Loaded: 22950 rows √ó 16 columns\n",
      "\n",
      "[2/2] Loading: Dataset-weather-prediction-dataset-processed_scaled_20250528_1500.csv\n",
      "   ‚úÖ Loaded: 22950 rows √ó 171 columns\n",
      "\n",
      "============================================================\n",
      "üìä LOADING SUMMARY\n",
      "============================================================\n",
      "‚úÖ Successfully loaded: 2 files\n",
      "\n",
      "üìã Loaded datasets:\n",
      "   - Dataset-Answers-Weather_Prediction_Pleasant_Weather.csv: 22950 rows √ó 16 columns\n",
      "   - Dataset-weather-prediction-dataset-processed_scaled_20250528_1500.csv: 22950 rows √ó 171 columns\n",
      "\n",
      "üîç Identifying datasets...\n",
      "\n",
      "üìä Dataset-Answers-Weather_Prediction_Pleasant_Weather.csv:\n",
      "   Shape: (22950, 16)\n",
      "   Columns sample: ['DATE', 'BASEL_pleasant_weather', 'BELGRADE_pleasant_weather', 'BUDAPEST_pleasant_weather', 'DEBILT_pleasant_weather']...\n",
      "   ‚úÖ Identified as TARGET dataset\n",
      "\n",
      "üìä Dataset-weather-prediction-dataset-processed_scaled_20250528_1500.csv:\n",
      "   Shape: (22950, 171)\n",
      "   Columns sample: ['id', 'DATE', 'MONTH', 'BASEL_cloud_cover', 'BASEL_wind_speed']...\n",
      "   ‚úÖ Identified as FEATURES dataset\n",
      "\n",
      "‚úÖ Datasets identified:\n",
      "   Features: (22950, 171)\n",
      "   Targets: (22950, 16)\n"
     ]
    }
   ],
   "source": [
    "# Import the module\n",
    "sys.path.append('./src')  # Optional: adapt if running outside src\n",
    "from file_handler import setup_paths, load_multiple_datasets\n",
    "\n",
    "# 1. Interactive project path setup\n",
    "project_root, input_path, output_path = setup_paths()\n",
    "\n",
    "# 2. File selection and loading\n",
    "datasets = load_multiple_datasets(input_path)\n",
    "\n",
    "# 3. Identify features and target datasets\n",
    "print(\"\\nüîç Identifying datasets...\")\n",
    "features_df = None\n",
    "target_df = None\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    print(f\"\\nüìä {name}:\")\n",
    "    print(f\"   Shape: {df.shape}\")\n",
    "    print(f\"   Columns sample: {list(df.columns[:5])}...\")\n",
    "    \n",
    "    # Check if this is the target dataset\n",
    "    if 'answer' in name.lower() or 'target' in name.lower() or 'label' in name.lower():\n",
    "        target_df = df\n",
    "        print(\"   ‚úÖ Identified as TARGET dataset\")\n",
    "    elif 'processed' in name.lower() or 'scaled' in name.lower() or 'feature' in name.lower():\n",
    "        features_df = df\n",
    "        print(\"   ‚úÖ Identified as FEATURES dataset\")\n",
    "\n",
    "if features_df is None or target_df is None:\n",
    "    print(\"\\n‚ö†Ô∏è Could not automatically identify datasets. Please select manually:\")\n",
    "    dataset_names = list(datasets.keys())\n",
    "    \n",
    "    print(\"\\nAvailable datasets:\")\n",
    "    for i, name in enumerate(dataset_names):\n",
    "        print(f\"  {i+1}. {name}\")\n",
    "    \n",
    "    features_idx = int(input(\"\\nüëâ Select FEATURES dataset number: \")) - 1\n",
    "    target_idx = int(input(\"üëâ Select TARGET dataset number: \")) - 1\n",
    "    \n",
    "    features_df = datasets[dataset_names[features_idx]]\n",
    "    target_df = datasets[dataset_names[target_idx]]\n",
    "\n",
    "print(f\"\\n‚úÖ Datasets identified:\")\n",
    "print(f\"   Features: {features_df.shape}\")\n",
    "print(f\"   Targets: {target_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Features and Target Datasets\n",
    "\n",
    "This step merges the features and target datasets based on common columns (typically date and station identifiers). The merge operation ensures that we have aligned features and targets for each observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Merging datasets...\n",
      "\n",
      "Common columns: ['DATE']\n",
      "\n",
      "üîó Merging on: ['DATE']\n",
      "‚úÖ Merged dataset: 22,950 rows √ó 186 columns\n",
      "   Memory usage: 32.57 MB\n"
     ]
    }
   ],
   "source": [
    "# 4. Merge datasets\n",
    "print(\"\\nüîÑ Merging datasets...\")\n",
    "\n",
    "# Find common columns for merging\n",
    "common_cols = list(set(features_df.columns) & set(target_df.columns))\n",
    "print(f\"\\nCommon columns: {common_cols}\")\n",
    "\n",
    "# Identify merge keys (date and station identifiers)\n",
    "merge_keys = []\n",
    "date_cols = [col for col in common_cols if 'date' in col.lower()]\n",
    "station_cols = [col for col in common_cols if 'station' in col.lower() or 'id' in col.lower()]\n",
    "\n",
    "if date_cols:\n",
    "    merge_keys.extend(date_cols[:1])  # Use first date column\n",
    "if station_cols:\n",
    "    merge_keys.extend(station_cols[:1])  # Use first station column\n",
    "\n",
    "# If no automatic detection, ask user\n",
    "if not merge_keys:\n",
    "    print(\"\\n‚ö†Ô∏è Could not automatically detect merge keys.\")\n",
    "    print(\"Available columns in both datasets:\")\n",
    "    for i, col in enumerate(common_cols[:20]):\n",
    "        print(f\"  {i+1}. {col}\")\n",
    "    \n",
    "    key_indices = input(\"\\nüëâ Select merge key columns (comma-separated numbers): \")\n",
    "    merge_keys = [common_cols[int(i)-1] for i in key_indices.split(',')]\n",
    "\n",
    "print(f\"\\nüîó Merging on: {merge_keys}\")\n",
    "\n",
    "# Perform merge\n",
    "df = pd.merge(features_df, target_df[merge_keys + [col for col in target_df.columns if col not in merge_keys]], \n",
    "              on=merge_keys, how='inner')\n",
    "\n",
    "print(f\"‚úÖ Merged dataset: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "print(f\"   Memory usage: {df.memory_usage().sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Update station column name if needed\n",
    "if CONFIG['station_column'] not in df.columns:\n",
    "    station_candidates = [col for col in df.columns if 'station' in col.lower()]\n",
    "    if station_candidates:\n",
    "        CONFIG['station_column'] = station_candidates[0]\n",
    "        print(f\"\\nüìç Updated station column to: {CONFIG['station_column']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ 3. Data Quality Control\n",
    "\n",
    "### Quality Control Steps:\n",
    "1. **Identify critical features** - Find weather-related columns (temperature, humidity, pressure, wind)\n",
    "2. **Station-wise quality check** - Calculate missing data percentage per station\n",
    "3. **Remove low-quality stations** - Drop stations exceeding missing data threshold\n",
    "4. **Fill remaining missing values** - Use median imputation for numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Found 96 critical features:\n",
      "   ‚Ä¢ DEBILT_humidity\n",
      "   ‚Ä¢ MADRID_wind_speed\n",
      "   ‚Ä¢ SONNBLICK_temp_mean\n",
      "   ‚Ä¢ MADRID_humidity\n",
      "   ‚Ä¢ LJUBLJANA_temp_min\n",
      "   ‚Ä¢ TOURS_pressure\n",
      "   ‚Ä¢ HEATHROW_temp_min\n",
      "   ‚Ä¢ ROMA_pressure\n",
      "   ‚Ä¢ MUNCHENB_temp_mean\n",
      "   ‚Ä¢ BASEL_humidity\n",
      "   ... and 86 more\n",
      "\n",
      "‚ö†Ô∏è Station column 'station_id' not found. Proceeding with overall data quality check.\n",
      "\n",
      "‚úÖ Final dataset: 22,950 rows √ó 187 columns\n"
     ]
    }
   ],
   "source": [
    "# Identify critical features\n",
    "critical_features = []\n",
    "for pattern in CONFIG['critical_features']:\n",
    "    matching_cols = [col for col in df.columns if pattern.lower() in col.lower()]\n",
    "    critical_features.extend(matching_cols)\n",
    "\n",
    "critical_features = list(set(critical_features))  # Remove duplicates\n",
    "print(f\"\\nüîç Found {len(critical_features)} critical features:\")\n",
    "for feat in critical_features[:10]:\n",
    "    print(f\"   ‚Ä¢ {feat}\")\n",
    "if len(critical_features) > 10:\n",
    "    print(f\"   ... and {len(critical_features) - 10} more\")\n",
    "\n",
    "# Check for station column\n",
    "if CONFIG['station_column'] in df.columns:\n",
    "    # Calculate missing data per station\n",
    "    print(f\"\\nüìä Analyzing data quality by station...\")\n",
    "    stations_to_drop = []\n",
    "    station_quality = {}\n",
    "    \n",
    "    for station in df[CONFIG['station_column']].unique():\n",
    "        station_data = df[df[CONFIG['station_column']] == station]\n",
    "        \n",
    "        # Calculate missing percentage for critical features\n",
    "        if critical_features:\n",
    "            missing_pct = station_data[critical_features].isnull().sum().sum() / (len(station_data) * len(critical_features))\n",
    "        else:\n",
    "            missing_pct = station_data.isnull().sum().sum() / (len(station_data) * len(station_data.columns))\n",
    "        \n",
    "        station_quality[station] = {\n",
    "            'missing_pct': missing_pct,\n",
    "            'row_count': len(station_data)\n",
    "        }\n",
    "        \n",
    "        if missing_pct > CONFIG['missing_threshold']:\n",
    "            stations_to_drop.append(station)\n",
    "    \n",
    "    # Report and drop stations\n",
    "    if stations_to_drop:\n",
    "        print(f\"\\n‚ö†Ô∏è Dropping {len(stations_to_drop)} stations with >{CONFIG['missing_threshold']*100:.0f}% missing data:\")\n",
    "        for station in stations_to_drop[:5]:\n",
    "            print(f\"   ‚Ä¢ {station}: {station_quality[station]['missing_pct']*100:.1f}% missing\")\n",
    "        if len(stations_to_drop) > 5:\n",
    "            print(f\"   ... and {len(stations_to_drop) - 5} more\")\n",
    "        \n",
    "        # Drop stations\n",
    "        df_before = len(df)\n",
    "        df = df[~df[CONFIG['station_column']].isin(stations_to_drop)]\n",
    "        print(f\"\\n‚úÖ Removed {df_before - len(df):,} rows from {len(stations_to_drop)} stations\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ All stations have acceptable data quality!\")\n",
    "    \n",
    "    # Summary of remaining stations\n",
    "    remaining_stations = df[CONFIG['station_column']].nunique()\n",
    "    print(f\"\\nüìç Remaining stations: {remaining_stations}\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Station column '{CONFIG['station_column']}' not found. Proceeding with overall data quality check.\")\n",
    "    \n",
    "    # Overall missing data handling\n",
    "    missing_pct = df.isnull().sum() / len(df) * 100\n",
    "    cols_to_drop = missing_pct[missing_pct > CONFIG['missing_threshold']*100].index\n",
    "    if len(cols_to_drop) > 0:\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "        print(f\"   Dropped {len(cols_to_drop)} columns with >{CONFIG['missing_threshold']*100:.0f}% missing data\")\n",
    "\n",
    "# Fill remaining missing values\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
    "\n",
    "print(f\"\\n‚úÖ Final dataset: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ 4. Interactive Target Selection and Problem Type Detection\n",
    "\n",
    "### Multi-Purpose Target Selection\n",
    "\n",
    "This section automatically detects if the dataset contains multiple stations/targets and determines the problem type:\n",
    "- **Classification**: Binary (2 classes) or Multi-class (>2 classes)\n",
    "- **Regression**: Continuous values\n",
    "\n",
    "Options:\n",
    "- **Single Station/Feature Model**: Focus on one specific station or feature\n",
    "- **Multi-Station/Feature Model**: Use data from all stations or features\n",
    "- **Traditional Approach**: Manual target selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üåç DETECTING DATA STRUCTURE AND TARGET OPTIONS\n",
      "============================================================\n",
      "\n",
      "‚úÖ Detected multi-station/multi-source dataset with 18 stations:\n",
      "   1. DUSSELDORF (12 features)\n",
      "   2. LJUBLJANA (11 features)\n",
      "   3. DEBILT (11 features)\n",
      "   4. MADRID (11 features)\n",
      "   5. VALENTIA (11 features)\n",
      "   6. HEATHROW (11 features)\n",
      "   7. OSLO (12 features)\n",
      "   8. ROMA (6 features)\n",
      "   9. BELGRADE (10 features)\n",
      "  10. BUDAPEST (10 features)\n",
      "  11. MAASTRICHT (11 features)\n",
      "  12. GDANSK (7 features)\n",
      "  13. BASEL (12 features)\n",
      "  14. MUNCHENB (10 features)\n",
      "  15. SONNBLICK (11 features)\n",
      "  16. TOURS (8 features)\n",
      "  17. KASSEL (10 features)\n",
      "  18. STOCKHOLM (9 features)\n",
      "\n",
      "üìä Model Training Approach:\n",
      "  1. Single Station Model - Train separate model for one station\n",
      "  2. Multi-Station Model - Use data from all stations\n",
      "  3. Traditional Approach - Select target column manually\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "üëâ Select approach (1-3):  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ SELECT TARGET STATION\n",
      "----------------------------------------\n",
      "   1. DUSSELDORF\n",
      "   2. LJUBLJANA\n",
      "   3. DEBILT\n",
      "   4. MADRID\n",
      "   5. VALENTIA\n",
      "   6. HEATHROW\n",
      "   7. OSLO\n",
      "   8. ROMA\n",
      "   9. BELGRADE\n",
      "  10. BUDAPEST\n",
      "  11. MAASTRICHT\n",
      "  12. GDANSK\n",
      "  13. BASEL\n",
      "  14. MUNCHENB\n",
      "  15. SONNBLICK\n",
      "  16. TOURS\n",
      "  17. KASSEL\n",
      "  18. STOCKHOLM\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "üëâ Select station number:  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Available features for VALENTIA:\n",
      "----------------------------------------\n",
      "   1. VALENTIA_cloud_cover\n",
      "      Type: Categorical (9 unique values)\n",
      "   2. VALENTIA_humidity\n",
      "      Type: Continuous (range: [-6.27, 2.45])\n",
      "   3. VALENTIA_pressure\n",
      "      Type: Continuous (range: [-5.51, 2.99])\n",
      "   4. VALENTIA_global_radiation\n",
      "      Type: Continuous (range: [-1.31, 3.35])\n",
      "   5. VALENTIA_precipitation\n",
      "      Type: Continuous (range: [-0.49, 106.03])\n",
      "   6. VALENTIA_snow_depth\n",
      "      Type: Categorical (4 unique values)\n",
      "   7. VALENTIA_sunshine\n",
      "      Type: Continuous (range: [-1.04, 3.71])\n",
      "   8. VALENTIA_temp_mean\n",
      "      Type: Continuous (range: [-4.27, 3.87])\n",
      "   9. VALENTIA_temp_min\n",
      "      Type: Continuous (range: [-4.15, 3.17])\n",
      "  10. VALENTIA_temp_max\n",
      "      Type: Continuous (range: [-4.32, 4.28])\n",
      "  11. VALENTIA_pleasant_weather\n",
      "\n",
      "üéØ SELECT TARGET VARIABLE\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "üëâ Select target feature number (1-11):  11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Selected target: VALENTIA_pleasant_weather\n",
      "   Encoded categorical target to numeric\n",
      "   Problem type: Binary Classification\n",
      "   Class distribution: {0: np.int64(21776), 1: np.int64(1174)}\n"
     ]
    }
   ],
   "source": [
    "# MULTI-STATION/TARGET DATA DETECTION\n",
    "print(\"\\nüåç DETECTING DATA STRUCTURE AND TARGET OPTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize variables\n",
    "single_station_mode = False\n",
    "selected_station = None\n",
    "target_col = None\n",
    "problem_type = None\n",
    "\n",
    "# Look for potential target columns\n",
    "target_patterns = ['target', 'label', 'pleasant_weather', 'y', 'output', 'result']\n",
    "potential_targets = []\n",
    "\n",
    "for col in df.columns:\n",
    "    if any(pattern in col.lower() for pattern in target_patterns):\n",
    "        potential_targets.append(col)\n",
    "\n",
    "# Detect if this is multi-station data\n",
    "station_names = []\n",
    "if potential_targets:\n",
    "    # Check if targets have station prefixes\n",
    "    for target in potential_targets:\n",
    "        parts = target.split('_')\n",
    "        if len(parts) > 1:\n",
    "            potential_station = parts[0]\n",
    "            # Check if this prefix appears in other columns\n",
    "            station_cols = [col for col in df.columns if col.startswith(potential_station + '_')]\n",
    "            if len(station_cols) > 5:  # Likely a station prefix\n",
    "                station_names.append(potential_station)\n",
    "    \n",
    "    station_names = list(set(station_names))\n",
    "\n",
    "if station_names:\n",
    "    print(f\"\\n‚úÖ Detected multi-station/multi-source dataset with {len(station_names)} stations:\")\n",
    "    for i, station in enumerate(station_names, 1):\n",
    "        station_features = [col for col in df.columns if col.startswith(station + '_')]\n",
    "        print(f\"  {i:2d}. {station} ({len(station_features)} features)\")\n",
    "    \n",
    "    # Ask user to select approach\n",
    "    print(\"\\nüìä Model Training Approach:\")\n",
    "    print(\"  1. Single Station Model - Train separate model for one station\")\n",
    "    print(\"  2. Multi-Station Model - Use data from all stations\")\n",
    "    print(\"  3. Traditional Approach - Select target column manually\")\n",
    "    \n",
    "    approach = input(\"\\nüëâ Select approach (1-3): \").strip()\n",
    "    \n",
    "    if approach == '1':\n",
    "        # Single station approach\n",
    "        print(\"\\nüéØ SELECT TARGET STATION\")\n",
    "        print(\"-\"*40)\n",
    "        for i, station in enumerate(station_names, 1):\n",
    "            print(f\"  {i:2d}. {station}\")\n",
    "        \n",
    "        station_idx = int(input(\"\\nüëâ Select station number: \")) - 1\n",
    "        selected_station = station_names[station_idx]\n",
    "        single_station_mode = True\n",
    "        \n",
    "        # Show all features for the selected station\n",
    "        station_features = [col for col in df.columns if col.startswith(selected_station + '_')]\n",
    "        \n",
    "        print(f\"\\nüìä Available features for {selected_station}:\")\n",
    "        print(\"-\"*40)\n",
    "        \n",
    "        # Categorize and display features\n",
    "        feature_list = []\n",
    "        for i, feat in enumerate(station_features, 1):\n",
    "            feature_list.append(feat)\n",
    "            print(f\"  {i:2d}. {feat}\")\n",
    "            # Show basic stats\n",
    "            if df[feat].dtype in [np.number]:\n",
    "                unique_vals = df[feat].nunique()\n",
    "                if unique_vals == 2:\n",
    "                    print(f\"      Type: Binary ({dict(df[feat].value_counts())})\")\n",
    "                elif unique_vals <= 10:\n",
    "                    print(f\"      Type: Categorical ({unique_vals} unique values)\")\n",
    "                else:\n",
    "                    print(f\"      Type: Continuous (range: [{df[feat].min():.2f}, {df[feat].max():.2f}])\")\n",
    "        \n",
    "        # Ask user to select target\n",
    "        print(\"\\nüéØ SELECT TARGET VARIABLE\")\n",
    "        target_idx = int(input(f\"\\nüëâ Select target feature number (1-{len(feature_list)}): \"))\n",
    "        target_col = feature_list[target_idx - 1]\n",
    "        \n",
    "    else:\n",
    "        approach = '3'  # Fall back to traditional approach\n",
    "\n",
    "if approach == '3' or not station_names:\n",
    "    # Traditional approach - show all potential targets\n",
    "    print(\"\\nüéØ SELECT TARGET VARIABLE\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    # Show all columns with basic info\n",
    "    all_cols = list(df.columns)\n",
    "    print(\"\\nAvailable columns:\")\n",
    "    for i, col in enumerate(all_cols[:50], 1):\n",
    "        col_info = f\"{i:3d}. {col}\"\n",
    "        if df[col].dtype in [np.number]:\n",
    "            unique_vals = df[col].nunique()\n",
    "            if unique_vals == 2:\n",
    "                col_info += \" (Binary)\"\n",
    "            elif unique_vals <= 10:\n",
    "                col_info += f\" (Categorical: {unique_vals} classes)\"\n",
    "            else:\n",
    "                col_info += \" (Continuous)\"\n",
    "        print(col_info)\n",
    "    \n",
    "    if len(all_cols) > 50:\n",
    "        print(f\"\\n... and {len(all_cols) - 50} more columns\")\n",
    "    \n",
    "    target_idx = int(input(f\"\\nüëâ Select target column number (1-{len(all_cols)}): \"))\n",
    "    target_col = all_cols[target_idx - 1]\n",
    "\n",
    "# Determine problem type based on target\n",
    "print(f\"\\n‚úÖ Selected target: {target_col}\")\n",
    "\n",
    "if df[target_col].dtype not in [np.number]:\n",
    "    # Convert categorical to numeric if needed\n",
    "    le = LabelEncoder()\n",
    "    df[target_col] = le.fit_transform(df[target_col])\n",
    "    print(f\"   Encoded categorical target to numeric\")\n",
    "\n",
    "unique_values = df[target_col].nunique()\n",
    "if unique_values == 2:\n",
    "    problem_type = 'classification'\n",
    "    print(f\"   Problem type: Binary Classification\")\n",
    "    print(f\"   Class distribution: {dict(df[target_col].value_counts())}\")\n",
    "elif unique_values <= 10:\n",
    "    problem_type = 'classification'\n",
    "    print(f\"   Problem type: Multi-class Classification ({unique_values} classes)\")\n",
    "    print(f\"   Class distribution: {dict(df[target_col].value_counts().head())}\")\n",
    "else:\n",
    "    problem_type = 'regression'\n",
    "    print(f\"   Problem type: Regression\")\n",
    "    print(f\"   Target statistics:\")\n",
    "    print(f\"     ‚Ä¢ Mean: {df[target_col].mean():.2f}\")\n",
    "    print(f\"     ‚Ä¢ Std: {df[target_col].std():.2f}\")\n",
    "    print(f\"     ‚Ä¢ Range: [{df[target_col].min():.2f}, {df[target_col].max():.2f}]\")\n",
    "\n",
    "# Create target column alias\n",
    "df['target'] = df[target_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection Strategy\n",
    "\n",
    "Choose features based on the selected approach and problem type:\n",
    "- **Automatic selection**: Based on correlation with target\n",
    "- **Pattern-based**: Select features matching specific patterns\n",
    "- **Category-based**: Select by feature type (temperature, humidity, etc.)\n",
    "- **Manual selection**: Choose individual features from a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ FEATURE SELECTION\n",
      "============================================================\n",
      "\n",
      "üéØ Single Station Mode: VALENTIA\n",
      "\n",
      "üìä Found 10 available numeric features\n",
      "\n",
      "üìä Feature Selection Options:\n",
      "  1. Automatic selection (based on correlation/importance)\n",
      "  2. Pattern-based selection (e.g., 'temp', 'humid', 'mean')\n",
      "  3. Category-based selection (temperature, humidity, etc.)\n",
      "  4. Manual feature selection\n",
      "  5. Use all available features\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "üëâ Choose option (1-5):  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Available features:\n",
      "    1. VALENTIA_cloud_cover\n",
      "    2. VALENTIA_humidity\n",
      "    3. VALENTIA_pressure\n",
      "    4. VALENTIA_global_radiation\n",
      "    5. VALENTIA_precipitation\n",
      "    6. VALENTIA_snow_depth\n",
      "    7. VALENTIA_sunshine\n",
      "    8. VALENTIA_temp_mean\n",
      "    9. VALENTIA_temp_min\n",
      "   10. VALENTIA_temp_max\n",
      "\n",
      "Enter feature numbers (comma-separated) or 'all'\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "üëâ Selection:  8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Selected 1 features manually\n",
      "\n",
      "üìä FINAL FEATURE SELECTION:\n",
      "   Selected features: 1\n",
      "   Excluded temporal: 0\n",
      "\n",
      "Sample of selected features:\n",
      "   ‚Ä¢ VALENTIA_temp_mean\n"
     ]
    }
   ],
   "source": [
    "# FEATURE SELECTION\n",
    "print(\"\\nüéØ FEATURE SELECTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Identify columns to exclude\n",
    "exclude_cols = [target_col, 'target']\n",
    "if CONFIG['station_column'] in df.columns:\n",
    "    exclude_cols.append(CONFIG['station_column'])\n",
    "\n",
    "# Get available features based on mode\n",
    "if single_station_mode and selected_station:\n",
    "    print(f\"\\nüéØ Single Station Mode: {selected_station}\")\n",
    "    available_features = [col for col in df.columns \n",
    "                         if col.startswith(selected_station + '_') \n",
    "                         and col not in exclude_cols \n",
    "                         and df[col].dtype in [np.number]]\n",
    "else:\n",
    "    # All numeric features\n",
    "    available_features = [col for col in df.columns \n",
    "                         if col not in exclude_cols \n",
    "                         and df[col].dtype in [np.number]]\n",
    "\n",
    "print(f\"\\nüìä Found {len(available_features)} available numeric features\")\n",
    "\n",
    "# Feature selection options\n",
    "print(\"\\nüìä Feature Selection Options:\")\n",
    "print(\"  1. Automatic selection (based on correlation/importance)\")\n",
    "print(\"  2. Pattern-based selection (e.g., 'temp', 'humid', 'mean')\")\n",
    "print(\"  3. Category-based selection (temperature, humidity, etc.)\")\n",
    "print(\"  4. Manual feature selection\")\n",
    "print(\"  5. Use all available features\")\n",
    "\n",
    "selection_option = input(\"\\nüëâ Choose option (1-5): \").strip()\n",
    "\n",
    "if selection_option == '1':\n",
    "    # Automatic selection based on correlation\n",
    "    print(\"\\nüîÑ Calculating feature importance...\")\n",
    "    \n",
    "    # Calculate correlations or mutual information\n",
    "    if problem_type == 'regression':\n",
    "        correlations = pd.DataFrame({\n",
    "            'feature': available_features,\n",
    "            'correlation': [abs(df[feat].corr(df['target'])) for feat in available_features]\n",
    "        }).sort_values('correlation', ascending=False)\n",
    "        \n",
    "        # Select top features\n",
    "        threshold = 0.1\n",
    "        selected_features = correlations[correlations['correlation'] > threshold]['feature'].tolist()\n",
    "        \n",
    "        print(f\"\\n‚úÖ Selected {len(selected_features)} features with correlation > {threshold}\")\n",
    "        print(\"\\nTop 10 correlated features:\")\n",
    "        for _, row in correlations.head(10).iterrows():\n",
    "            print(f\"   ‚Ä¢ {row['feature']}: {row['correlation']:.3f}\")\n",
    "    else:\n",
    "        # Use mutual information for classification\n",
    "        from sklearn.feature_selection import mutual_info_classif\n",
    "        mi_scores = mutual_info_classif(df[available_features], df['target'])\n",
    "        mi_df = pd.DataFrame({\n",
    "            'feature': available_features,\n",
    "            'mi_score': mi_scores\n",
    "        }).sort_values('mi_score', ascending=False)\n",
    "        \n",
    "        # Select top features\n",
    "        n_features = min(30, len(available_features) // 2)\n",
    "        selected_features = mi_df.head(n_features)['feature'].tolist()\n",
    "        \n",
    "        print(f\"\\n‚úÖ Selected top {len(selected_features)} features by mutual information\")\n",
    "        print(\"\\nTop 10 features:\")\n",
    "        for _, row in mi_df.head(10).iterrows():\n",
    "            print(f\"   ‚Ä¢ {row['feature']}: {row['mi_score']:.3f}\")\n",
    "    \n",
    "    feature_cols = selected_features\n",
    "    \n",
    "elif selection_option == '2':\n",
    "    # Pattern-based selection\n",
    "    print(\"\\nEnter patterns to match (comma-separated)\")\n",
    "    print(\"Example: 'mean,temp,humid' for mean values, temperature, and humidity\")\n",
    "    patterns = input(\"\\nüëâ Patterns: \").strip().split(',')\n",
    "    \n",
    "    feature_cols = []\n",
    "    for pattern in patterns:\n",
    "        matching = [f for f in available_features if pattern.strip().lower() in f.lower()]\n",
    "        feature_cols.extend(matching)\n",
    "        print(f\"  ‚Ä¢ '{pattern.strip()}' matched {len(matching)} features\")\n",
    "    \n",
    "    feature_cols = list(set(feature_cols))  # Remove duplicates\n",
    "    print(f\"\\n‚úÖ Selected {len(feature_cols)} features by pattern\")\n",
    "    \n",
    "elif selection_option == '3':\n",
    "    # Category-based selection\n",
    "    categories = {\n",
    "        '1': ('Temperature', ['temp']),\n",
    "        '2': ('Humidity', ['humid', 'moisture']),\n",
    "        '3': ('Pressure', ['pressure', 'press']),\n",
    "        '4': ('Wind', ['wind', 'gust']),\n",
    "        '5': ('Precipitation', ['rain', 'precip', 'snow']),\n",
    "        '6': ('All Weather', ['temp', 'humid', 'pressure', 'wind', 'rain'])\n",
    "    }\n",
    "    \n",
    "    print(\"\\nüìä Feature Categories:\")\n",
    "    for key, (name, _) in categories.items():\n",
    "        print(f\"  {key}. {name}\")\n",
    "    \n",
    "    cat_selection = input(\"\\nüëâ Select categories (comma-separated numbers): \").strip().split(',')\n",
    "    \n",
    "    feature_cols = []\n",
    "    for cat in cat_selection:\n",
    "        if cat in categories:\n",
    "            name, patterns = categories[cat]\n",
    "            for pattern in patterns:\n",
    "                matching = [f for f in available_features if pattern in f.lower()]\n",
    "                feature_cols.extend(matching)\n",
    "            print(f\"  ‚Ä¢ {name}: added {len(matching)} features\")\n",
    "    \n",
    "    feature_cols = list(set(feature_cols))  # Remove duplicates\n",
    "    print(f\"\\n‚úÖ Selected {len(feature_cols)} features by category\")\n",
    "    \n",
    "elif selection_option == '4':\n",
    "    # Manual selection\n",
    "    print(\"\\nüìã Available features:\")\n",
    "    for i, feat in enumerate(available_features[:50], 1):\n",
    "        print(f\"  {i:3d}. {feat}\")\n",
    "    if len(available_features) > 50:\n",
    "        print(f\"  ... and {len(available_features) - 50} more\")\n",
    "    \n",
    "    print(\"\\nEnter feature numbers (comma-separated) or 'all'\")\n",
    "    selection = input(\"\\nüëâ Selection: \").strip()\n",
    "    \n",
    "    if selection.lower() == 'all':\n",
    "        feature_cols = available_features\n",
    "    else:\n",
    "        indices = [int(x.strip())-1 for x in selection.split(',')]\n",
    "        feature_cols = [available_features[i] for i in indices if i < len(available_features)]\n",
    "    \n",
    "    print(f\"\\n‚úÖ Selected {len(feature_cols)} features manually\")\n",
    "    \n",
    "else:\n",
    "    # Use all features\n",
    "    feature_cols = available_features\n",
    "    print(f\"\\n‚úÖ Using all {len(feature_cols)} available features\")\n",
    "\n",
    "# Exclude temporal features\n",
    "temporal_excluded = []\n",
    "for pattern in CONFIG['exclude_patterns']:\n",
    "    temporal_excluded.extend([f for f in feature_cols if pattern in f.lower()])\n",
    "\n",
    "feature_cols = [f for f in feature_cols if f not in temporal_excluded]\n",
    "\n",
    "print(f\"\\nüìä FINAL FEATURE SELECTION:\")\n",
    "print(f\"   Selected features: {len(feature_cols)}\")\n",
    "print(f\"   Excluded temporal: {len(temporal_excluded)}\")\n",
    "\n",
    "# Show sample of selected features\n",
    "print(\"\\nSample of selected features:\")\n",
    "for feat in feature_cols[:10]:\n",
    "    print(f\"   ‚Ä¢ {feat}\")\n",
    "if len(feature_cols) > 10:\n",
    "    print(f\"   ... and {len(feature_cols) - 10} more features\")\n",
    "\n",
    "# Store station information if available\n",
    "station_info = None\n",
    "if CONFIG['station_column'] in df.columns:\n",
    "    station_info = df[CONFIG['station_column']].values\n",
    "    print(f\"\\nüìç Station information preserved for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ 5. Data Preparation and Train-Test Split\n",
    "\n",
    "### Data Preparation Steps:\n",
    "1. **Feature matrix creation** - Extract selected features into X\n",
    "2. **Feature reduction** - Apply SelectKBest if too many features (>50)\n",
    "3. **Train-test split** - Stratified split for classification, random split for regression\n",
    "4. **Data preservation** - Keep station information for later analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÇÔ∏è Data split (stratified):\n",
      "   Training: 18,360 samples\n",
      "   Testing: 4,590 samples\n",
      "   Features: 1\n",
      "\n",
      "   Class balance (train): {0: np.float64(0.949), 1: np.float64(0.051)}\n",
      "   Class balance (test): {0: np.float64(0.949), 1: np.float64(0.051)}\n"
     ]
    }
   ],
   "source": [
    "# Prepare features and target\n",
    "X = df[feature_cols]\n",
    "y = df['target']\n",
    "\n",
    "# Feature selection if too many features\n",
    "if X.shape[1] > 50:\n",
    "    print(f\"\\n‚ö†Ô∏è Too many features ({X.shape[1]}). Applying feature selection...\")\n",
    "    k = min(30, X.shape[1] // 2)\n",
    "    \n",
    "    if problem_type == 'classification':\n",
    "        selector = SelectKBest(f_classif, k=k)\n",
    "    else:\n",
    "        selector = SelectKBest(f_regression, k=k)\n",
    "    \n",
    "    X_selected = selector.fit_transform(X, y)\n",
    "    selected_features = X.columns[selector.get_support()].tolist()\n",
    "    X = pd.DataFrame(X_selected, columns=selected_features)\n",
    "    print(f\"‚úÖ Reduced features from {len(feature_cols)} to {k}\")\n",
    "else:\n",
    "    selected_features = feature_cols\n",
    "\n",
    "# Train-test split with appropriate strategy\n",
    "if problem_type == 'classification':\n",
    "    # Stratified split for classification\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    print(f\"\\n‚úÇÔ∏è Data split (stratified):\")\n",
    "else:\n",
    "    # Random split for regression\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    print(f\"\\n‚úÇÔ∏è Data split (random):\")\n",
    "\n",
    "# Also split station info if available\n",
    "if station_info is not None:\n",
    "    train_indices = X_train.index\n",
    "    test_indices = X_test.index\n",
    "    station_train = station_info[train_indices]\n",
    "    station_test = station_info[test_indices]\n",
    "\n",
    "print(f\"   Training: {X_train.shape[0]:,} samples\")\n",
    "print(f\"   Testing: {X_test.shape[0]:,} samples\")\n",
    "print(f\"   Features: {X_train.shape[1]}\")\n",
    "\n",
    "if problem_type == 'classification':\n",
    "    print(f\"\\n   Class balance (train): {dict(y_train.value_counts(normalize=True).round(3))}\")\n",
    "    print(f\"   Class balance (test): {dict(y_test.value_counts(normalize=True).round(3))}\")\n",
    "else:\n",
    "    print(f\"\\n   Target statistics (train):\")\n",
    "    print(f\"     ‚Ä¢ Mean: {y_train.mean():.2f}, Std: {y_train.std():.2f}\")\n",
    "    print(f\"   Target statistics (test):\")\n",
    "    print(f\"     ‚Ä¢ Mean: {y_test.mean():.2f}, Std: {y_test.std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì 6. Manual Gradient Descent Implementation (opt.)\n",
    "\n",
    "This section demonstrates gradient descent optimization from scratch for educational purposes.\n",
    "\n",
    "**Availability**: \n",
    "- Only runs for **regression problems** with **single feature**\n",
    "- Provides visualization of cost function and convergence\n",
    "- For production use, sklearn's optimized implementations are recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí° Current problem type: classification\n",
      "   Number of features: 1\n",
      "\n",
      "üìä Skipping gradient descent demo (only for single-feature regression problems).\n",
      "   Current problem: classification with 1 features\n"
     ]
    }
   ],
   "source": [
    "# Check if gradient descent demo should be run\n",
    "print(f\"\\nüí° Current problem type: {problem_type}\")\n",
    "print(f\"   Number of features: {X_train.shape[1]}\")\n",
    "\n",
    "if problem_type == 'regression' and X_train.shape[1] == 1:\n",
    "    print(\"\\nüéì GRADIENT DESCENT DEMONSTRATION\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Running educational gradient descent implementation...\")\n",
    "    \n",
    "    # Manual Gradient Descent Implementation\n",
    "    class ManualLinearRegression:\n",
    "        def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
    "            self.learning_rate = learning_rate\n",
    "            self.n_iterations = n_iterations\n",
    "            self.theta = None\n",
    "            self.cost_history = []\n",
    "            \n",
    "        def add_intercept(self, X):\n",
    "            \"\"\"Add intercept term (column of ones) to feature matrix\"\"\"\n",
    "            intercept = np.ones((X.shape[0], 1))\n",
    "            return np.c_[intercept, X]\n",
    "        \n",
    "        def cost_function(self, X, y, theta):\n",
    "            \"\"\"Calculate Mean Squared Error cost\"\"\"\n",
    "            m = len(y)\n",
    "            predictions = X.dot(theta)\n",
    "            cost = (1/(2*m)) * np.sum(np.square(predictions - y))\n",
    "            return cost\n",
    "        \n",
    "        def gradient_descent(self, X, y, theta):\n",
    "            \"\"\"Perform one step of gradient descent\"\"\"\n",
    "            m = len(y)\n",
    "            predictions = X.dot(theta)\n",
    "            errors = predictions - y\n",
    "            gradient = (1/m) * X.T.dot(errors)\n",
    "            theta = theta - self.learning_rate * gradient\n",
    "            return theta\n",
    "        \n",
    "        def fit(self, X, y):\n",
    "            \"\"\"Train the model using gradient descent\"\"\"\n",
    "            # Add intercept\n",
    "            X = self.add_intercept(X)\n",
    "            \n",
    "            # Initialize parameters\n",
    "            self.theta = np.zeros(X.shape[1])\n",
    "            \n",
    "            # Gradient descent\n",
    "            for i in range(self.n_iterations):\n",
    "                cost = self.cost_function(X, y, self.theta)\n",
    "                self.cost_history.append(cost)\n",
    "                self.theta = self.gradient_descent(X, y, self.theta)\n",
    "                \n",
    "                if i % 100 == 0:\n",
    "                    print(f\"   Iteration {i}: Cost = {cost:.4f}\")\n",
    "        \n",
    "        def predict(self, X):\n",
    "            \"\"\"Make predictions\"\"\"\n",
    "            X = self.add_intercept(X)\n",
    "            return X.dot(self.theta)\n",
    "    \n",
    "    # Train manual gradient descent model\n",
    "    print(\"\\nüîÑ Training with manual gradient descent...\")\n",
    "    manual_model = ManualLinearRegression(learning_rate=0.01, n_iterations=1000)\n",
    "    manual_model.fit(X_train.values, y_train.values)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_manual = manual_model.predict(X_test.values)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test, y_pred_manual)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred_manual)\n",
    "    r2 = r2_score(y_test, y_pred_manual)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Manual Gradient Descent Results:\")\n",
    "    print(f\"   Final cost: {manual_model.cost_history[-1]:.4f}\")\n",
    "    print(f\"   Test MSE: {mse:.4f}\")\n",
    "    print(f\"   Test RMSE: {rmse:.4f}\")\n",
    "    print(f\"   Test MAE: {mae:.4f}\")\n",
    "    print(f\"   Test R¬≤: {r2:.4f}\")\n",
    "    print(f\"   Coefficients: Œ∏‚ÇÄ={manual_model.theta[0]:.4f}, Œ∏‚ÇÅ={manual_model.theta[1]:.4f}\")\n",
    "    \n",
    "    # Visualize results\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # 1. Cost function during training\n",
    "    axes[0].plot(manual_model.cost_history)\n",
    "    axes[0].set_title('Cost Function During Training')\n",
    "    axes[0].set_xlabel('Iteration')\n",
    "    axes[0].set_ylabel('Cost (MSE)')\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    # 2. Predictions vs Actual\n",
    "    axes[1].scatter(X_test.iloc[:, 0], y_test, alpha=0.5, label='Actual')\n",
    "    axes[1].scatter(X_test.iloc[:, 0], y_pred_manual, alpha=0.5, label='Predicted')\n",
    "    \n",
    "    # Add regression line\n",
    "    x_range = np.linspace(X_test.iloc[:, 0].min(), X_test.iloc[:, 0].max(), 100)\n",
    "    y_range = manual_model.theta[0] + manual_model.theta[1] * x_range\n",
    "    axes[1].plot(x_range, y_range, 'r-', linewidth=2, label='Regression Line')\n",
    "    \n",
    "    axes[1].set_xlabel(X_test.columns[0])\n",
    "    axes[1].set_ylabel('Target')\n",
    "    axes[1].set_title('Predictions vs Actual')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    # 3. Residual plot\n",
    "    residuals = y_test - y_pred_manual\n",
    "    axes[2].scatter(y_pred_manual, residuals, alpha=0.5)\n",
    "    axes[2].axhline(y=0, color='r', linestyle='--')\n",
    "    axes[2].set_xlabel('Predicted Values')\n",
    "    axes[2].set_ylabel('Residuals')\n",
    "    axes[2].set_title('Residual Plot')\n",
    "    axes[2].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Initialize results storage if needed\n",
    "    if 'results' not in locals():\n",
    "        results = {}\n",
    "    if 'best_models' not in locals():\n",
    "        best_models = {}\n",
    "    \n",
    "    # Store results for later comparison\n",
    "    results['Manual Gradient Descent'] = {\n",
    "        'metrics': {\n",
    "            'mse': mse,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'r2_score': r2,\n",
    "            'cv_score': r2,  # Use R¬≤ as proxy for CV score\n",
    "            'train_mse': mean_squared_error(y_train, manual_model.predict(X_train.values)),\n",
    "            'train_r2': r2_score(y_train, manual_model.predict(X_train.values)),\n",
    "            'training_time': 1.0  # Approximate\n",
    "        },\n",
    "        'best_params': {\n",
    "            'learning_rate': 0.01,\n",
    "            'n_iterations': 1000\n",
    "        },\n",
    "        'predictions': y_pred_manual,\n",
    "        'probabilities': None\n",
    "    }\n",
    "    best_models['Manual Gradient Descent'] = manual_model\n",
    "    \n",
    "elif problem_type == 'regression' and X_train.shape[1] > 1:\n",
    "    print(\"\\n‚ö†Ô∏è Gradient descent demo is only available for single-feature regression.\")\n",
    "    print(\"   For multi-feature regression, sklearn's optimized models will handle it efficiently.\")\n",
    "    # Initialize results dictionaries\n",
    "    results = {}\n",
    "    best_models = {}\n",
    "else:\n",
    "    print(\"\\nüìä Skipping gradient descent demo (only for single-feature regression problems).\")\n",
    "    print(f\"   Current problem: {problem_type} with {X_train.shape[1]} features\")\n",
    "    # Initialize results dictionaries\n",
    "    results = {}\n",
    "    best_models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ 7. Model Training with Adaptive Algorithm Selection\n",
    "\n",
    "### Adaptive Model Selection:\n",
    "\n",
    "The notebook automatically selects appropriate algorithms based on your problem type:\n",
    "\n",
    "**For Classification:**\n",
    "- Logistic Regression\n",
    "- Decision Tree Classifier\n",
    "- Random Forest Classifier\n",
    "- Gradient Boosting Classifier\n",
    "- Support Vector Classifier (SVC)\n",
    "- Neural Network Classifier\n",
    "- Naive Bayes\n",
    "\n",
    "**For Regression:**\n",
    "- Linear Regression\n",
    "- Ridge Regression\n",
    "- Lasso Regression\n",
    "- Decision Tree Regressor\n",
    "- Random Forest Regressor\n",
    "- Gradient Boosting Regressor\n",
    "- Support Vector Regressor (SVR)\n",
    "- Neural Network Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ CONFIGURING MODELS FOR CLASSIFICATION\n",
      "============================================================\n",
      "\n",
      "üìä Class Balance Analysis:\n",
      "   Minimum class proportion: 5.11%\n",
      "   Class weighting: Applied (imbalanced)\n",
      "\n",
      "üéØ Training Configuration:\n",
      "   Models to train: 7\n",
      "   Problem type: classification\n",
      "   Cross-validation: 5-fold\n",
      "   Scoring metric: balanced_accuracy\n",
      "   Class weight: balanced\n",
      "\n",
      "üß† Total parameter combinations to test: 204\n"
     ]
    }
   ],
   "source": [
    "# Define models based on problem type\n",
    "print(f\"\\nü§ñ CONFIGURING MODELS FOR {problem_type.upper()}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if problem_type == 'classification':\n",
    "    # Check class balance for classification\n",
    "    class_props = y_train.value_counts(normalize=True)\n",
    "    is_balanced = class_props.min() >= 0.2\n",
    "    class_weight = None if is_balanced else 'balanced'\n",
    "    \n",
    "    print(f\"\\nüìä Class Balance Analysis:\")\n",
    "    print(f\"   Minimum class proportion: {class_props.min():.2%}\")\n",
    "    print(f\"   Class weighting: {'Not needed (balanced)' if is_balanced else 'Applied (imbalanced)'}\")\n",
    "    \n",
    "    # Classification models\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000, class_weight=class_weight, random_state=42),\n",
    "        'Decision Tree': DecisionTreeClassifier(max_depth=10, class_weight=class_weight, random_state=42),\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=10, class_weight=class_weight, n_jobs=-1, random_state=42),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, max_depth=5, random_state=42),\n",
    "        'SVM': SVC(kernel='rbf', probability=True, class_weight=class_weight, random_state=42),\n",
    "        'Neural Network': MLPClassifier(random_state=42),\n",
    "        'Naive Bayes': GaussianNB()\n",
    "    }\n",
    "    \n",
    "    # Classification parameter grids\n",
    "    param_grids = {\n",
    "        'Logistic Regression': {'C': [0.1, 1, 10]},\n",
    "        'Decision Tree': {'max_depth': [5, 10, 15], 'min_samples_split': [2, 10]},\n",
    "        'Random Forest': {'n_estimators': [50, 100], 'max_depth': [10, 20]},\n",
    "        'Gradient Boosting': {'n_estimators': [50, 100], 'learning_rate': [0.1, 0.2]},\n",
    "        'SVM': {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto']},\n",
    "        'Neural Network': CONFIG['neural_network_params'],\n",
    "        'Naive Bayes': {}\n",
    "    }\n",
    "    \n",
    "    cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scoring = 'balanced_accuracy' if not is_balanced else 'accuracy'\n",
    "    \n",
    "else:  # regression\n",
    "    print(f\"\\nüìä Target Variable Analysis:\")\n",
    "    print(f\"   Mean: {y_train.mean():.2f}\")\n",
    "    print(f\"   Std: {y_train.std():.2f}\")\n",
    "    print(f\"   Skewness: {y_train.skew():.2f}\")\n",
    "    \n",
    "    # Regression models\n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Ridge Regression': Ridge(random_state=42),\n",
    "        'Lasso Regression': Lasso(random_state=42),\n",
    "        'Decision Tree': DecisionTreeRegressor(max_depth=10, random_state=42),\n",
    "        'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=10, n_jobs=-1, random_state=42),\n",
    "        'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42),\n",
    "        'SVR': SVR(kernel='rbf'),\n",
    "        'Neural Network': MLPRegressor(random_state=42)\n",
    "    }\n",
    "    \n",
    "    # Regression parameter grids\n",
    "    param_grids = {\n",
    "        'Linear Regression': {},\n",
    "        'Ridge Regression': {'alpha': [0.1, 1.0, 10.0]},\n",
    "        'Lasso Regression': {'alpha': [0.1, 1.0, 10.0]},\n",
    "        'Decision Tree': {'max_depth': [5, 10, 15], 'min_samples_split': [2, 10]},\n",
    "        'Random Forest': {'n_estimators': [50, 100], 'max_depth': [10, 20]},\n",
    "        'Gradient Boosting': {'n_estimators': [50, 100], 'learning_rate': [0.1, 0.2]},\n",
    "        'SVR': {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto']},\n",
    "        'Neural Network': {\n",
    "            'hidden_layer_sizes': [(50,), (100,), (100,50)],\n",
    "            'max_iter': [500, 1000],\n",
    "            'activation': ['relu', 'tanh']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    cv_strategy = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scoring = 'neg_mean_squared_error'  # sklearn uses negative MSE\n",
    "\n",
    "print(f\"\\nüéØ Training Configuration:\")\n",
    "print(f\"   Models to train: {len(models)}\")\n",
    "print(f\"   Problem type: {problem_type}\")\n",
    "print(f\"   Cross-validation: {cv_strategy.n_splits}-fold\")\n",
    "print(f\"   Scoring metric: {scoring}\")\n",
    "if problem_type == 'classification':\n",
    "    print(f\"   Class weight: {class_weight}\")\n",
    "\n",
    "# Calculate total parameter combinations\n",
    "total_combinations = sum([\n",
    "    np.prod([len(v) for v in params.values()]) if params else 1 \n",
    "    for params in param_grids.values()\n",
    "])\n",
    "print(f\"\\nüß† Total parameter combinations to test: {total_combinations:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training with Progress Tracking\n",
    "\n",
    "Train all models with:\n",
    "- **Hyperparameter tuning** using GridSearchCV\n",
    "- **Cross-validation** for robust evaluation\n",
    "- **Progress bars** to track training status\n",
    "- **Appropriate metrics** for each problem type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ TRAINING 7 MODELS\n",
      "================================================================================\n",
      "\n",
      "üìä Parameter combinations per model:\n",
      "   ‚Ä¢ Logistic Regression: 3 combinations\n",
      "   ‚Ä¢ Decision Tree: 6 combinations\n",
      "   ‚Ä¢ Random Forest: 4 combinations\n",
      "   ‚Ä¢ Gradient Boosting: 4 combinations\n",
      "   ‚Ä¢ SVM: 6 combinations\n",
      "   ‚Ä¢ Neural Network: 180 combinations\n",
      "     üß† Total fits: 900\n",
      "   ‚Ä¢ Naive Bayes: 1 combinations\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression:   0%|                                                              | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Model 1/7: Logistic Regression\n",
      "------------------------------------------------------------\n",
      "  üìä Searching 3 parameter combinations...\n",
      "  üîÑ Using 5-fold cross-validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Decision Tree:  14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                   | 1/7 [00:04<00:28,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  üìà Making predictions... Done!\n",
      "  üìä Calculating metrics... Done!\n",
      "\n",
      "  ‚úÖ Results for Logistic Regression:\n",
      "     ‚Ä¢ Best CV Score: 0.8556\n",
      "     ‚Ä¢ Test Accuracy: 0.8244\n",
      "     ‚Ä¢ Test Balanced Accuracy: 0.8431\n",
      "     ‚Ä¢ Test AUC-ROC: 0.9230\n",
      "     ‚Ä¢ Training Time: 4.75s\n",
      "\n",
      "  ‚è±Ô∏è Estimated time remaining: 28.5s\n",
      "\n",
      "üîÑ Model 2/7: Decision Tree\n",
      "------------------------------------------------------------\n",
      "  üìä Searching 6 parameter combinations...\n",
      "  üîÑ Using 5-fold cross-validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Random Forest:  29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                          | 2/7 [00:09<00:22,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  üìà Making predictions... Done!\n",
      "  üìä Calculating metrics... Done!\n",
      "\n",
      "  ‚úÖ Results for Decision Tree:\n",
      "     ‚Ä¢ Best CV Score: 0.8652\n",
      "     ‚Ä¢ Test Accuracy: 0.7749\n",
      "     ‚Ä¢ Test Balanced Accuracy: 0.8613\n",
      "     ‚Ä¢ Test AUC-ROC: 0.9224\n",
      "     ‚Ä¢ Training Time: 4.16s\n",
      "\n",
      "  ‚è±Ô∏è Estimated time remaining: 22.3s\n",
      "\n",
      "üîÑ Model 3/7: Random Forest\n",
      "------------------------------------------------------------\n",
      "  üìä Searching 4 parameter combinations...\n",
      "  üîÑ Using 5-fold cross-validation\n",
      "  üìà Making predictions... Done!\n",
      "  üìä Calculating metrics..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Gradient Boosting:  43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                | 3/7 [00:12<00:16,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Done!\n",
      "\n",
      "  ‚úÖ Results for Random Forest:\n",
      "     ‚Ä¢ Best CV Score: 0.8575\n",
      "     ‚Ä¢ Test Accuracy: 0.7850\n",
      "     ‚Ä¢ Test Balanced Accuracy: 0.8565\n",
      "     ‚Ä¢ Test AUC-ROC: 0.9193\n",
      "     ‚Ä¢ Training Time: 3.64s\n",
      "\n",
      "  ‚è±Ô∏è Estimated time remaining: 16.7s\n",
      "\n",
      "üîÑ Model 4/7: Gradient Boosting\n",
      "------------------------------------------------------------\n",
      "  üìä Searching 4 parameter combinations...\n",
      "  üîÑ Using 5-fold cross-validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training SVM:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 4/7 [00:18<00:14,  4.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  üìà Making predictions... Done!\n",
      "  üìä Calculating metrics... Done!\n",
      "\n",
      "  ‚úÖ Results for Gradient Boosting:\n",
      "     ‚Ä¢ Best CV Score: 0.5370\n",
      "     ‚Ä¢ Test Accuracy: 0.9490\n",
      "     ‚Ä¢ Test Balanced Accuracy: 0.5363\n",
      "     ‚Ä¢ Test AUC-ROC: 0.9201\n",
      "     ‚Ä¢ Training Time: 5.70s\n",
      "\n",
      "  ‚è±Ô∏è Estimated time remaining: 13.7s\n",
      "\n",
      "üîÑ Model 5/7: SVM\n",
      "------------------------------------------------------------\n",
      "  üìä Searching 6 parameter combinations...\n",
      "  üîÑ Using 5-fold cross-validation\n",
      "  üìà Making predictions..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Neural Network:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                | 5/7 [05:47<04:03, 121.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Done!\n",
      "  üìä Calculating metrics... Done!\n",
      "\n",
      "  ‚úÖ Results for SVM:\n",
      "     ‚Ä¢ Best CV Score: 0.8694\n",
      "     ‚Ä¢ Test Accuracy: 0.7749\n",
      "     ‚Ä¢ Test Balanced Accuracy: 0.8613\n",
      "     ‚Ä¢ Test AUC-ROC: 0.8925\n",
      "     ‚Ä¢ Training Time: 305.09s\n",
      "\n",
      "  ‚è±Ô∏è Estimated time remaining: 129.3s\n",
      "\n",
      "üîÑ Model 6/7: Neural Network\n",
      "------------------------------------------------------------\n",
      "  üìä Searching 180 parameter combinations...\n",
      "  üîÑ Using 5-fold cross-validation\n"
     ]
    }
   ],
   "source": [
    "# Model training with progress tracking\n",
    "print(f\"\\nüöÄ TRAINING {len(models)} MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate parameter combinations for progress estimation\n",
    "total_params = {}\n",
    "for name, params in param_grids.items():\n",
    "    if params:\n",
    "        n_combos = 1\n",
    "        for param_values in params.values():\n",
    "            n_combos *= len(param_values)\n",
    "        total_params[name] = n_combos\n",
    "    else:\n",
    "        total_params[name] = 1\n",
    "\n",
    "print(\"\\nüìä Parameter combinations per model:\")\n",
    "for name, n_combos in total_params.items():\n",
    "    print(f\"   ‚Ä¢ {name}: {n_combos} combinations\")\n",
    "    if name == 'Neural Network':\n",
    "        print(f\"     üß† Total fits: {n_combos * cv_strategy.n_splits:,}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "\n",
    "# Training variables\n",
    "training_times = {}\n",
    "\n",
    "# Create main progress bar\n",
    "with tqdm(total=len(models), desc=\"Overall Progress\", position=0, leave=True) as pbar_main:\n",
    "    \n",
    "    for model_idx, (name, model) in enumerate(models.items()):\n",
    "        pbar_main.set_description(f\"Training {name}\")\n",
    "        \n",
    "        print(f\"\\nüîÑ Model {model_idx + 1}/{len(models)}: {name}\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # GridSearchCV with appropriate settings\n",
    "        grid = GridSearchCV(\n",
    "            model, \n",
    "            param_grids[name], \n",
    "            cv=cv_strategy, \n",
    "            scoring=scoring, \n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Show parameter search info\n",
    "        print(f\"  üìä Searching {total_params[name]} parameter combinations...\")\n",
    "        print(f\"  üîÑ Using {cv_strategy.n_splits}-fold cross-validation\")\n",
    "        \n",
    "        # Fit model\n",
    "        try:\n",
    "            grid.fit(X_train, y_train)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\n‚ö†Ô∏è Training interrupted! Saving results so far...\")\n",
    "            break\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        training_times[name] = training_time\n",
    "        \n",
    "        # Store best model\n",
    "        best_models[name] = grid.best_estimator_\n",
    "        \n",
    "        # Make predictions\n",
    "        print(\"  üìà Making predictions...\", end='')\n",
    "        y_pred = grid.best_estimator_.predict(X_test)\n",
    "        y_train_pred = grid.best_estimator_.predict(X_train)\n",
    "        \n",
    "        # Get probabilities for classification\n",
    "        y_pred_proba = None\n",
    "        if problem_type == 'classification' and hasattr(grid.best_estimator_, 'predict_proba'):\n",
    "            y_pred_proba = grid.best_estimator_.predict_proba(X_test)\n",
    "            if y_train.nunique() == 2:  # Binary classification\n",
    "                y_pred_proba = y_pred_proba[:, 1]\n",
    "        \n",
    "        print(\" Done!\")\n",
    "        \n",
    "        # Calculate metrics based on problem type\n",
    "        print(\"  üìä Calculating metrics...\", end='')\n",
    "        \n",
    "        if problem_type == 'classification':\n",
    "            # Classification metrics\n",
    "            metrics = {\n",
    "                'accuracy': accuracy_score(y_test, y_pred),\n",
    "                'balanced_accuracy': balanced_accuracy_score(y_test, y_pred),\n",
    "                'precision': precision_score(y_test, y_pred, average='weighted', zero_division=0),\n",
    "                'recall': recall_score(y_test, y_pred, average='weighted', zero_division=0),\n",
    "                'f1_score': f1_score(y_test, y_pred, average='weighted', zero_division=0),\n",
    "                'cv_score': grid.best_score_,\n",
    "                'train_accuracy': accuracy_score(y_train, y_train_pred),\n",
    "                'train_balanced_accuracy': balanced_accuracy_score(y_train, y_train_pred),\n",
    "                'training_time': training_time\n",
    "            }\n",
    "            \n",
    "            # Add AUC-ROC for binary classification\n",
    "            if y_pred_proba is not None and y_train.nunique() == 2:\n",
    "                metrics['auc_roc'] = roc_auc_score(y_test, y_pred_proba)\n",
    "            \n",
    "        else:\n",
    "            # Regression metrics\n",
    "            metrics = {\n",
    "                'mse': mean_squared_error(y_test, y_pred),\n",
    "                'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "                'mae': mean_absolute_error(y_test, y_pred),\n",
    "                'r2_score': r2_score(y_test, y_pred),\n",
    "                'explained_variance': explained_variance_score(y_test, y_pred),\n",
    "                'cv_score': -grid.best_score_,  # Convert back from negative MSE\n",
    "                'train_mse': mean_squared_error(y_train, y_train_pred),\n",
    "                'train_r2': r2_score(y_train, y_train_pred),\n",
    "                'training_time': training_time\n",
    "            }\n",
    "        \n",
    "        print(\" Done!\")\n",
    "        \n",
    "        # Store results\n",
    "        results[name] = {\n",
    "            'metrics': metrics,\n",
    "            'best_params': grid.best_params_,\n",
    "            'predictions': y_pred,\n",
    "            'probabilities': y_pred_proba\n",
    "        }\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\n  ‚úÖ Results for {name}:\")\n",
    "        print(f\"     ‚Ä¢ Best CV Score: {grid.best_score_:.4f}\")\n",
    "        \n",
    "        if problem_type == 'classification':\n",
    "            print(f\"     ‚Ä¢ Test Accuracy: {metrics['accuracy']:.4f}\")\n",
    "            print(f\"     ‚Ä¢ Test Balanced Accuracy: {metrics['balanced_accuracy']:.4f}\")\n",
    "            if 'auc_roc' in metrics:\n",
    "                print(f\"     ‚Ä¢ Test AUC-ROC: {metrics['auc_roc']:.4f}\")\n",
    "        else:\n",
    "            print(f\"     ‚Ä¢ Test R¬≤: {metrics['r2_score']:.4f}\")\n",
    "            print(f\"     ‚Ä¢ Test RMSE: {metrics['rmse']:.4f}\")\n",
    "            print(f\"     ‚Ä¢ Test MAE: {metrics['mae']:.4f}\")\n",
    "        \n",
    "        print(f\"     ‚Ä¢ Training Time: {training_time:.2f}s\")\n",
    "        \n",
    "        # Special reporting for Neural Network\n",
    "        if name == 'Neural Network' and grid.best_params_:\n",
    "            print(f\"\\n  üß† Best Neural Network Configuration:\")\n",
    "            for param, value in grid.best_params_.items():\n",
    "                print(f\"     ‚Ä¢ {param}: {value}\")\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar_main.update(1)\n",
    "        \n",
    "        # Time estimation\n",
    "        if model_idx < len(models) - 1:\n",
    "            elapsed_time = sum(training_times.values())\n",
    "            avg_time = elapsed_time / (model_idx + 1)\n",
    "            remaining = (len(models) - model_idx - 1) * avg_time\n",
    "            print(f\"\\n  ‚è±Ô∏è Estimated time remaining: {remaining:.1f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ All models trained successfully!\")\n",
    "print(f\"‚è±Ô∏è Total training time: {sum(training_times.values()):.2f}s\")\n",
    "\n",
    "# Summary table\n",
    "print(\"\\nüìä Quick Summary:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "if problem_type == 'classification':\n",
    "    print(f\"{'Model':<25} {'CV Score':>10} {'Test Acc':>10} {'Balanced':>10} {'Time (s)':>10}\")\n",
    "    print(\"-\"*80)\n",
    "    for name in models.keys():\n",
    "        if name in results:\n",
    "            m = results[name]['metrics']\n",
    "            print(f\"{name:<25} {m['cv_score']:>10.4f} {m['accuracy']:>10.4f} \"\n",
    "                  f\"{m['balanced_accuracy']:>10.4f} {m['training_time']:>10.2f}\")\n",
    "else:\n",
    "    print(f\"{'Model':<25} {'CV MSE':>10} {'Test R¬≤':>10} {'RMSE':>10} {'Time (s)':>10}\")\n",
    "    print(\"-\"*80)\n",
    "    for name in models.keys():\n",
    "        if name in results:\n",
    "            m = results[name]['metrics']\n",
    "            print(f\"{name:<25} {m['cv_score']:>10.4f} {m['r2_score']:>10.4f} \"\n",
    "                  f\"{m['rmse']:>10.4f} {m['training_time']:>10.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 8. Model Comparison and Analysis\n",
    "\n",
    "### Analysis Components:\n",
    "1. **Performance comparison** - Compare all models on relevant metrics\n",
    "2. **Overfitting detection** - Identify models with large train-test gaps\n",
    "3. **Complex model analysis** - Special focus on ensemble and neural network models\n",
    "4. **Best model identification** - Select top performer based on problem type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison dataframe\n",
    "comparison_data = []\n",
    "for name, result in results.items():\n",
    "    row = {'Model': name}\n",
    "    row.update(result['metrics'])\n",
    "    \n",
    "    # Calculate overfitting metrics based on problem type\n",
    "    if problem_type == 'classification':\n",
    "        row['overfitting_score'] = row['train_balanced_accuracy'] - row['balanced_accuracy']\n",
    "        row['is_overfitting'] = row['overfitting_score'] > CONFIG['overfitting_threshold']\n",
    "    else:\n",
    "        row['overfitting_score'] = row['train_r2'] - row['r2_score']\n",
    "        row['is_overfitting'] = row['overfitting_score'] > CONFIG['overfitting_threshold']\n",
    "    \n",
    "    comparison_data.append(row)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Sort by appropriate metric\n",
    "if problem_type == 'classification':\n",
    "    comparison_df = comparison_df.sort_values('balanced_accuracy', ascending=False)\n",
    "    primary_metric = 'balanced_accuracy'\n",
    "else:\n",
    "    comparison_df = comparison_df.sort_values('r2_score', ascending=False)\n",
    "    primary_metric = 'r2_score'\n",
    "\n",
    "print(\"\\nüìä MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if problem_type == 'classification':\n",
    "    display_cols = ['Model', 'balanced_accuracy', 'accuracy', 'precision', 'recall', 'f1_score', \n",
    "                    'train_balanced_accuracy', 'overfitting_score', 'training_time']\n",
    "else:\n",
    "    display_cols = ['Model', 'r2_score', 'rmse', 'mae', 'explained_variance',\n",
    "                    'train_r2', 'overfitting_score', 'training_time']\n",
    "\n",
    "print(comparison_df[display_cols].to_string(index=False, float_format='%.4f'))\n",
    "\n",
    "# Overfitting analysis\n",
    "print(\"\\nüîç OVERFITTING ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "overfitting_models = comparison_df[comparison_df['is_overfitting']]\n",
    "if len(overfitting_models) > 0:\n",
    "    print(f\"‚ö†Ô∏è Models showing overfitting (train-test difference > {CONFIG['overfitting_threshold']*100}%):\")\n",
    "    for _, model in overfitting_models.iterrows():\n",
    "        print(f\"   ‚Ä¢ {model['Model']}: {model['overfitting_score']*100:.2f}% difference\")\n",
    "else:\n",
    "    print(\"‚úÖ No models show significant overfitting!\")\n",
    "\n",
    "# Complex model analysis\n",
    "complex_models = ['Neural Network', 'Random Forest', 'Gradient Boosting', 'SVM', 'SVR']\n",
    "print(\"\\nüìà COMPLEX MODEL ANALYSIS\")\n",
    "print(\"-\"*60)\n",
    "for model_name in complex_models:\n",
    "    if model_name in comparison_df['Model'].values:\n",
    "        model_data = comparison_df[comparison_df['Model'] == model_name].iloc[0]\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        if problem_type == 'classification':\n",
    "            print(f\"   ‚Ä¢ Test Balanced Accuracy: {model_data['balanced_accuracy']:.4f}\")\n",
    "        else:\n",
    "            print(f\"   ‚Ä¢ Test R¬≤: {model_data['r2_score']:.4f}\")\n",
    "            print(f\"   ‚Ä¢ Test RMSE: {model_data['rmse']:.4f}\")\n",
    "        print(f\"   ‚Ä¢ Overfitting: {'Yes' if model_data['is_overfitting'] else 'No'} ({model_data['overfitting_score']*100:.2f}%)\")\n",
    "        print(f\"   ‚Ä¢ Training Time: {model_data['training_time']:.2f}s\")\n",
    "\n",
    "# Identify best model\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "best_model = best_models[best_model_name]\n",
    "best_metrics = comparison_df.iloc[0]\n",
    "\n",
    "print(f\"\\nüèÜ BEST MODEL: {best_model_name}\")\n",
    "print(\"-\"*60)\n",
    "if problem_type == 'classification':\n",
    "    print(f\"   ‚Ä¢ Balanced Accuracy: {best_metrics['balanced_accuracy']:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Accuracy: {best_metrics['accuracy']:.4f}\")\n",
    "    print(f\"   ‚Ä¢ F1 Score: {best_metrics['f1_score']:.4f}\")\n",
    "else:\n",
    "    print(f\"   ‚Ä¢ R¬≤ Score: {best_metrics['r2_score']:.4f}\")\n",
    "    print(f\"   ‚Ä¢ RMSE: {best_metrics['rmse']:.4f}\")\n",
    "    print(f\"   ‚Ä¢ MAE: {best_metrics['mae']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåç 9. Station/Group Analysis (if applicable)\n",
    "\n",
    "If the dataset contains station or group information, analyze model performance across different stations/groups to identify:\n",
    "- **High-performance stations** - Where the model works best\n",
    "- **Challenging stations** - Where predictions are less accurate\n",
    "- **Performance distribution** - Statistical summary across all stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform station-wise analysis if station information is available\n",
    "station_results_df = None\n",
    "\n",
    "if station_info is not None and CONFIG['station_column'] in df.columns:\n",
    "    print(\"\\nüåç STATION-WISE ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Get best model predictions\n",
    "    best_pred = results[best_model_name]['predictions']\n",
    "    \n",
    "    # Calculate per-station metrics\n",
    "    unique_stations = np.unique(station_test)\n",
    "    station_metrics = {}\n",
    "    \n",
    "    for station in unique_stations:\n",
    "        station_mask = station_test == station\n",
    "        if np.sum(station_mask) > 10:  # Only analyze stations with sufficient samples\n",
    "            station_y_true = y_test[station_mask]\n",
    "            station_y_pred = best_pred[station_mask]\n",
    "            \n",
    "            if problem_type == 'classification':\n",
    "                station_acc = accuracy_score(station_y_true, station_y_pred)\n",
    "                station_metrics[station] = {\n",
    "                    'accuracy': station_acc,\n",
    "                    'n_samples': np.sum(station_mask),\n",
    "                    'n_correct': np.sum(station_y_true == station_y_pred)\n",
    "                }\n",
    "            else:\n",
    "                station_r2 = r2_score(station_y_true, station_y_pred)\n",
    "                station_rmse = np.sqrt(mean_squared_error(station_y_true, station_y_pred))\n",
    "                station_metrics[station] = {\n",
    "                    'r2_score': station_r2,\n",
    "                    'rmse': station_rmse,\n",
    "                    'n_samples': np.sum(station_mask)\n",
    "                }\n",
    "    \n",
    "    # Create station results dataframe\n",
    "    if problem_type == 'classification':\n",
    "        station_results_df = pd.DataFrame([\n",
    "            {'station': k, 'accuracy': v['accuracy'], 'n_samples': v['n_samples']} \n",
    "            for k, v in station_metrics.items()\n",
    "        ]).sort_values('accuracy', ascending=False)\n",
    "        \n",
    "        # Report high-accuracy stations\n",
    "        high_acc_stations = station_results_df[station_results_df['accuracy'] >= CONFIG['high_accuracy_threshold']]\n",
    "        print(f\"\\nüèÜ Stations with ‚â•{CONFIG['high_accuracy_threshold']*100:.0f}% accuracy: {len(high_acc_stations)}\")\n",
    "        if len(high_acc_stations) > 0:\n",
    "            for _, row in high_acc_stations.head(10).iterrows():\n",
    "                print(f\"   ‚Ä¢ {row['station']}: {row['accuracy']*100:.2f}% ({row['n_samples']} samples)\")\n",
    "        \n",
    "        # Overall statistics\n",
    "        acc_values = station_results_df['accuracy'].values\n",
    "        print(f\"\\nüìä Station Accuracy Statistics:\")\n",
    "        print(f\"   ‚Ä¢ Mean: {np.mean(acc_values)*100:.2f}%\")\n",
    "        print(f\"   ‚Ä¢ Std: {np.std(acc_values)*100:.2f}%\")\n",
    "        print(f\"   ‚Ä¢ Min: {np.min(acc_values)*100:.2f}%\")\n",
    "        print(f\"   ‚Ä¢ Max: {np.max(acc_values)*100:.2f}%\")\n",
    "        \n",
    "    else:  # regression\n",
    "        station_results_df = pd.DataFrame([\n",
    "            {'station': k, 'r2_score': v['r2_score'], 'rmse': v['rmse'], 'n_samples': v['n_samples']} \n",
    "            for k, v in station_metrics.items()\n",
    "        ]).sort_values('r2_score', ascending=False)\n",
    "        \n",
    "        # Report best performing stations\n",
    "        print(f\"\\nüèÜ Top performing stations by R¬≤:\")\n",
    "        for _, row in station_results_df.head(10).iterrows():\n",
    "            print(f\"   ‚Ä¢ {row['station']}: R¬≤={row['r2_score']:.3f}, RMSE={row['rmse']:.2f} ({row['n_samples']} samples)\")\n",
    "        \n",
    "        # Overall statistics\n",
    "        r2_values = station_results_df['r2_score'].values\n",
    "        print(f\"\\nüìä Station R¬≤ Statistics:\")\n",
    "        print(f\"   ‚Ä¢ Mean: {np.mean(r2_values):.3f}\")\n",
    "        print(f\"   ‚Ä¢ Std: {np.std(r2_values):.3f}\")\n",
    "        print(f\"   ‚Ä¢ Min: {np.min(r2_values):.3f}\")\n",
    "        print(f\"   ‚Ä¢ Max: {np.max(r2_values):.3f}\")\n",
    "    \n",
    "    print(f\"\\nüìç Total stations analyzed: {len(station_metrics)}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Station-wise analysis not available (no station information in dataset)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà 10. Comprehensive Visualizations\n",
    "\n",
    "Create interactive visualizations adapted to the problem type:\n",
    "\n",
    "**For Classification:**\n",
    "- Model performance comparison\n",
    "- Confusion matrix\n",
    "- ROC curves (for binary classification)\n",
    "- Feature importance\n",
    "\n",
    "**For Regression:**\n",
    "- Model performance comparison\n",
    "- Actual vs Predicted scatter plot\n",
    "- Residual analysis\n",
    "- Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations based on problem type\n",
    "best_pred = results[best_model_name]['predictions']\n",
    "\n",
    "print(\"\\nüìä GENERATING VISUALIZATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if problem_type == 'classification':\n",
    "    # Classification visualizations\n",
    "    n_classes = y_train.nunique()\n",
    "    \n",
    "    # Create subplots\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Model Performance Comparison', 'Confusion Matrix',\n",
    "                       'Training Time Comparison', 'Overfitting Analysis'),\n",
    "        specs=[[{'type': 'bar'}, {'type': 'heatmap'}],\n",
    "               [{'type': 'bar'}, {'type': 'scatter'}]]\n",
    "    )\n",
    "    \n",
    "    # 1. Model Performance Comparison\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=comparison_df['Model'],\n",
    "            y=comparison_df['balanced_accuracy'],\n",
    "            name='Balanced Accuracy',\n",
    "            marker_color='lightblue',\n",
    "            text=[f\"{val:.3f}\" for val in comparison_df['balanced_accuracy']],\n",
    "            textposition='auto'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, best_pred)\n",
    "    \n",
    "    # Normalize confusion matrix\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # Create labels for confusion matrix\n",
    "    if n_classes <= 10:\n",
    "        labels = [[f\"{cm[i,j]}\\n({cm_normalized[i,j]:.1%})\" for j in range(n_classes)] for i in range(n_classes)]\n",
    "    else:\n",
    "        labels = cm\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=cm_normalized,\n",
    "            text=labels,\n",
    "            texttemplate='%{text}',\n",
    "            colorscale='Blues',\n",
    "            showscale=True\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 3. Training Time Comparison\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=comparison_df['Model'],\n",
    "            y=comparison_df['training_time'],\n",
    "            name='Training Time (s)',\n",
    "            marker_color='lightcoral',\n",
    "            text=[f\"{val:.1f}s\" for val in comparison_df['training_time']],\n",
    "            textposition='auto'\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 4. Overfitting Analysis\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=comparison_df['train_balanced_accuracy'],\n",
    "            y=comparison_df['balanced_accuracy'],\n",
    "            mode='markers+text',\n",
    "            text=comparison_df['Model'],\n",
    "            textposition='top center',\n",
    "            marker=dict(size=10, color='darkblue'),\n",
    "            name='Models'\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    # Add diagonal line for overfitting plot\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[0, 1],\n",
    "            y=[0, 1],\n",
    "            mode='lines',\n",
    "            line=dict(dash='dash', color='red'),\n",
    "            name='No Overfitting Line',\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_xaxes(title_text='Model', row=1, col=1)\n",
    "    fig.update_yaxes(title_text='Balanced Accuracy', row=1, col=1)\n",
    "    fig.update_xaxes(title_text='Predicted', row=1, col=2)\n",
    "    fig.update_yaxes(title_text='Actual', row=1, col=2)\n",
    "    fig.update_xaxes(title_text='Model', row=2, col=1, tickangle=45)\n",
    "    fig.update_yaxes(title_text='Training Time (s)', row=2, col=1)\n",
    "    fig.update_xaxes(title_text='Train Balanced Accuracy', row=2, col=2)\n",
    "    fig.update_yaxes(title_text='Test Balanced Accuracy', row=2, col=2)\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        title_text=f'Classification Model Analysis - {best_model_name}',\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Additional ROC curve for binary classification\n",
    "    if n_classes == 2 and any(results[name]['probabilities'] is not None for name in results):\n",
    "        print(\"\\nüìà Generating ROC curves for binary classification...\")\n",
    "        \n",
    "        from sklearn.metrics import roc_curve, auc\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        for name in results:\n",
    "            if results[name]['probabilities'] is not None:\n",
    "                fpr, tpr, _ = roc_curve(y_test, results[name]['probabilities'])\n",
    "                roc_auc = auc(fpr, tpr)\n",
    "                plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.3f})')\n",
    "        \n",
    "        plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curves - Model Comparison')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "    \n",
    "else:\n",
    "    # Regression visualizations\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Model Performance (R¬≤)', 'Actual vs Predicted',\n",
    "                       'Residual Analysis', 'Model Comparison'),\n",
    "        specs=[[{'type': 'bar'}, {'type': 'scatter'}],\n",
    "               [{'type': 'scatter'}, {'type': 'bar'}]]\n",
    "    )\n",
    "    \n",
    "    # 1. Model Performance Comparison (R¬≤)\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=comparison_df['Model'],\n",
    "            y=comparison_df['r2_score'],\n",
    "            name='R¬≤ Score',\n",
    "            marker_color='lightgreen',\n",
    "            text=[f\"{val:.3f}\" for val in comparison_df['r2_score']],\n",
    "            textposition='auto'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. Actual vs Predicted\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=y_test,\n",
    "            y=best_pred,\n",
    "            mode='markers',\n",
    "            marker=dict(size=5, color='blue', opacity=0.5),\n",
    "            name='Predictions'\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Add perfect prediction line\n",
    "    min_val = min(y_test.min(), best_pred.min())\n",
    "    max_val = max(y_test.max(), best_pred.max())\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[min_val, max_val],\n",
    "            y=[min_val, max_val],\n",
    "            mode='lines',\n",
    "            line=dict(color='red', dash='dash'),\n",
    "            name='Perfect Prediction',\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 3. Residual Plot\n",
    "    residuals = y_test - best_pred\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=best_pred,\n",
    "            y=residuals,\n",
    "            mode='markers',\n",
    "            marker=dict(size=5, color='purple', opacity=0.5),\n",
    "            name='Residuals'\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Add zero line\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[best_pred.min(), best_pred.max()],\n",
    "            y=[0, 0],\n",
    "            mode='lines',\n",
    "            line=dict(color='red', dash='dash'),\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 4. RMSE Comparison\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=comparison_df['Model'],\n",
    "            y=comparison_df['rmse'],\n",
    "            name='RMSE',\n",
    "            marker_color='lightcoral',\n",
    "            text=[f\"{val:.2f}\" for val in comparison_df['rmse']],\n",
    "            textposition='auto'\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_xaxes(title_text='Model', row=1, col=1, tickangle=45)\n",
    "    fig.update_yaxes(title_text='R¬≤ Score', row=1, col=1)\n",
    "    fig.update_xaxes(title_text='Actual', row=1, col=2)\n",
    "    fig.update_yaxes(title_text='Predicted', row=1, col=2)\n",
    "    fig.update_xaxes(title_text='Predicted', row=2, col=1)\n",
    "    fig.update_yaxes(title_text='Residuals', row=2, col=1)\n",
    "    fig.update_xaxes(title_text='Model', row=2, col=2, tickangle=45)\n",
    "    fig.update_yaxes(title_text='RMSE', row=2, col=2)\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        title_text=f'Regression Model Analysis - {best_model_name}',\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Additional residual distribution plot\n",
    "    print(\"\\nüìä Generating residual distribution analysis...\")\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Histogram of residuals\n",
    "    axes[0].hist(residuals, bins=30, edgecolor='black', alpha=0.7)\n",
    "    axes[0].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "    axes[0].set_xlabel('Residuals')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title('Distribution of Residuals')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Q-Q plot\n",
    "    from scipy import stats\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=axes[1])\n",
    "    axes[1].set_title('Q-Q Plot')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance Visualization\n",
    "\n",
    "For models that support feature importance (tree-based models) or coefficients (linear models), visualize which features contribute most to predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance visualization\n",
    "print(\"\\nüîç FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    # Tree-based models\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False).head(20)\n",
    "    \n",
    "    fig = px.bar(importance_df, y='feature', x='importance', orientation='h',\n",
    "                title=f'Top 20 Feature Importances - {best_model_name}',\n",
    "                labels={'importance': 'Importance Score', 'feature': 'Feature'})\n",
    "    fig.update_layout(height=600)\n",
    "    fig.show()\n",
    "    \n",
    "    print(\"\\nTop 10 most important features:\")\n",
    "    for _, row in importance_df.head(10).iterrows():\n",
    "        print(f\"   ‚Ä¢ {row['feature']}: {row['importance']:.4f}\")\n",
    "        \n",
    "elif hasattr(best_model, 'coef_'):\n",
    "    # Linear models\n",
    "    if problem_type == 'classification' and len(best_model.coef_.shape) > 1:\n",
    "        # Multi-class classification\n",
    "        coef = np.abs(best_model.coef_).mean(axis=0)\n",
    "    else:\n",
    "        coef = np.abs(best_model.coef_).flatten()\n",
    "    \n",
    "    coef_df = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'coefficient': coef\n",
    "    }).sort_values('coefficient', ascending=False).head(20)\n",
    "    \n",
    "    fig = px.bar(coef_df, y='feature', x='coefficient', orientation='h',\n",
    "                title=f'Top 20 Feature Coefficients (Absolute) - {best_model_name}',\n",
    "                labels={'coefficient': 'Absolute Coefficient', 'feature': 'Feature'})\n",
    "    fig.update_layout(height=600)\n",
    "    fig.show()\n",
    "    \n",
    "    print(\"\\nTop 10 most influential features:\")\n",
    "    for _, row in coef_df.head(10).iterrows():\n",
    "        print(f\"   ‚Ä¢ {row['feature']}: {row['coefficient']:.4f}\")\n",
    "        \n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Feature importance not available for {best_model_name}\")\n",
    "    print(\"   (Only available for tree-based and linear models)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã 11. Comprehensive Analysis Report and Recommendations\n",
    "\n",
    "Generate a detailed report with:\n",
    "1. **Performance summary** across all models\n",
    "2. **Overfitting analysis** to identify potential issues\n",
    "3. **Model recommendations** based on multiple criteria\n",
    "4. **Actionable insights** for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã COMPREHENSIVE ANALYSIS REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Performance Summary\n",
    "print(\"\\n1Ô∏è‚É£ PERFORMANCE SUMMARY\")\n",
    "print(\"-\"*60)\n",
    "print(f\"Problem Type: {problem_type.capitalize()}\")\n",
    "print(f\"Target Variable: {target_col}\")\n",
    "print(f\"Number of Features: {X_train.shape[1]}\")\n",
    "print(f\"Training Samples: {X_train.shape[0]:,}\")\n",
    "print(f\"Test Samples: {X_test.shape[0]:,}\")\n",
    "\n",
    "if problem_type == 'classification':\n",
    "    print(f\"\\nNumber of Classes: {y_train.nunique()}\")\n",
    "    print(f\"Class Distribution: {dict(y_train.value_counts())}\")\n",
    "    print(f\"\\nBest Model Performance:\")\n",
    "    print(f\"   ‚Ä¢ Model: {best_model_name}\")\n",
    "    print(f\"   ‚Ä¢ Accuracy: {best_metrics['accuracy']:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Balanced Accuracy: {best_metrics['balanced_accuracy']:.4f}\")\n",
    "    print(f\"   ‚Ä¢ F1 Score: {best_metrics['f1_score']:.4f}\")\n",
    "else:\n",
    "    print(f\"\\nTarget Statistics:\")\n",
    "    print(f\"   ‚Ä¢ Mean: {y_train.mean():.2f}\")\n",
    "    print(f\"   ‚Ä¢ Std: {y_train.std():.2f}\")\n",
    "    print(f\"   ‚Ä¢ Range: [{y_train.min():.2f}, {y_train.max():.2f}]\")\n",
    "    print(f\"\\nBest Model Performance:\")\n",
    "    print(f\"   ‚Ä¢ Model: {best_model_name}\")\n",
    "    print(f\"   ‚Ä¢ R¬≤ Score: {best_metrics['r2_score']:.4f}\")\n",
    "    print(f\"   ‚Ä¢ RMSE: {best_metrics['rmse']:.4f}\")\n",
    "    print(f\"   ‚Ä¢ MAE: {best_metrics['mae']:.4f}\")\n",
    "\n",
    "# 2. Station Analysis Summary (if applicable)\n",
    "if station_results_df is not None:\n",
    "    print(\"\\n2Ô∏è‚É£ STATION ANALYSIS SUMMARY\")\n",
    "    print(\"-\"*60)\n",
    "    if problem_type == 'classification':\n",
    "        high_perf_threshold = 0.9\n",
    "        high_perf_stations = station_results_df[station_results_df['accuracy'] >= high_perf_threshold]\n",
    "        low_perf_stations = station_results_df[station_results_df['accuracy'] < 0.7]\n",
    "        \n",
    "        print(f\"High Performance Stations (‚â•{high_perf_threshold*100:.0f}%): {len(high_perf_stations)}\")\n",
    "        print(f\"Low Performance Stations (<70%): {len(low_perf_stations)}\")\n",
    "        \n",
    "        if len(low_perf_stations) > 0:\n",
    "            print(\"\\n‚ö†Ô∏è Stations requiring attention:\")\n",
    "            for _, row in low_perf_stations.head(5).iterrows():\n",
    "                print(f\"   ‚Ä¢ {row['station']}: {row['accuracy']*100:.1f}% accuracy\")\n",
    "    else:\n",
    "        high_perf_stations = station_results_df[station_results_df['r2_score'] >= 0.8]\n",
    "        low_perf_stations = station_results_df[station_results_df['r2_score'] < 0.5]\n",
    "        \n",
    "        print(f\"High Performance Stations (R¬≤ ‚â• 0.8): {len(high_perf_stations)}\")\n",
    "        print(f\"Low Performance Stations (R¬≤ < 0.5): {len(low_perf_stations)}\")\n",
    "\n",
    "# 3. Overfitting Analysis\n",
    "print(\"\\n3Ô∏è‚É£ OVERFITTING ANALYSIS\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "overfitting_summary = comparison_df.groupby('is_overfitting').size()\n",
    "print(f\"Models without overfitting: {overfitting_summary.get(False, 0)}\")\n",
    "print(f\"Models with overfitting: {overfitting_summary.get(True, 0)}\")\n",
    "\n",
    "if len(overfitting_models) > 0:\n",
    "    print(\"\\nOverfitting details:\")\n",
    "    for _, model in overfitting_models.iterrows():\n",
    "        print(f\"   ‚Ä¢ {model['Model']}: {model['overfitting_score']*100:.1f}% gap\")\n",
    "    print(\"\\nüí° Recommendation: Consider regularization or ensemble methods\")\n",
    "\n",
    "# 4. Model Recommendations\n",
    "print(\"\\n4Ô∏è‚É£ MODEL RECOMMENDATIONS\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Score models based on multiple criteria\n",
    "recommendation_scores = []\n",
    "for _, model in comparison_df.iterrows():\n",
    "    score = 0\n",
    "    reasons = []\n",
    "    \n",
    "    # Performance score (40% weight)\n",
    "    if problem_type == 'classification':\n",
    "        perf_score = model['balanced_accuracy'] * 40\n",
    "    else:\n",
    "        perf_score = model['r2_score'] * 40\n",
    "    score += perf_score\n",
    "    \n",
    "    # Speed score (30% weight)\n",
    "    if model['training_time'] < np.percentile(comparison_df['training_time'], 25):\n",
    "        score += 30\n",
    "        reasons.append(\"fast training\")\n",
    "    elif model['training_time'] < np.percentile(comparison_df['training_time'], 50):\n",
    "        score += 20\n",
    "        reasons.append(\"moderate speed\")\n",
    "    elif model['training_time'] < np.percentile(comparison_df['training_time'], 75):\n",
    "        score += 10\n",
    "    \n",
    "    # Generalization score (30% weight)\n",
    "    if not model['is_overfitting']:\n",
    "        score += 30\n",
    "        reasons.append(\"no overfitting\")\n",
    "    elif model['overfitting_score'] < 0.03:\n",
    "        score += 20\n",
    "        reasons.append(\"minimal overfitting\")\n",
    "    elif model['overfitting_score'] < 0.05:\n",
    "        score += 10\n",
    "        reasons.append(\"acceptable overfitting\")\n",
    "    \n",
    "    # Model complexity bonus\n",
    "    simple_models = ['Linear Regression', 'Logistic Regression', 'Naive Bayes']\n",
    "    if model['Model'] in simple_models and score > 70:\n",
    "        reasons.append(\"interpretable\")\n",
    "    \n",
    "    recommendation_scores.append({\n",
    "        'Model': model['Model'],\n",
    "        'Score': score,\n",
    "        'Performance': perf_score/40,\n",
    "        'Training_Time': model['training_time'],\n",
    "        'Overfitting': model['overfitting_score'],\n",
    "        'Reasons': ', '.join(reasons)\n",
    "    })\n",
    "\n",
    "recommendation_df = pd.DataFrame(recommendation_scores).sort_values('Score', ascending=False)\n",
    "\n",
    "print(\"\\nüèÜ TOP 3 RECOMMENDATIONS:\")\n",
    "for i, row in enumerate(recommendation_df.head(3).iterrows(), 1):\n",
    "    _, rec = row\n",
    "    print(f\"\\n{i}. {rec['Model']} (Score: {rec['Score']:.1f}/100)\")\n",
    "    print(f\"   ‚Ä¢ Performance: {rec['Performance']:.2%}\")\n",
    "    print(f\"   ‚Ä¢ Training Time: {rec['Training_Time']:.2f}s\")\n",
    "    print(f\"   ‚Ä¢ Overfitting: {rec['Overfitting']*100:.1f}%\")\n",
    "    if rec['Reasons']:\n",
    "        print(f\"   ‚Ä¢ Strengths: {rec['Reasons']}\")\n",
    "\n",
    "# Final recommendation\n",
    "best_overall = recommendation_df.iloc[0]\n",
    "print(f\"\\nüìå FINAL RECOMMENDATION: {best_overall['Model']}\")\n",
    "print(f\"\\nThis model provides the best balance of:\")\n",
    "print(f\"   ‚Ä¢ Performance: {best_overall['Performance']:.2%}\")\n",
    "print(f\"   ‚Ä¢ Speed: {best_overall['Training_Time']:.2f}s training time\")\n",
    "print(f\"   ‚Ä¢ Generalization: {best_overall['Overfitting']*100:.1f}% train-test gap\")\n",
    "\n",
    "# Deployment considerations\n",
    "print(\"\\n5Ô∏è‚É£ DEPLOYMENT CONSIDERATIONS\")\n",
    "print(\"-\"*60)\n",
    "print(\"‚úì Model is ready for deployment\")\n",
    "print(\"‚úì Consider implementing:\")\n",
    "print(\"   ‚Ä¢ Input validation for feature ranges\")\n",
    "print(\"   ‚Ä¢ Model versioning and monitoring\")\n",
    "print(\"   ‚Ä¢ Regular retraining schedule\")\n",
    "if station_results_df is not None and len(low_perf_stations) > 0:\n",
    "    print(\"   ‚Ä¢ Special handling for low-performance stations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ 12. Save Results and Models\n",
    "\n",
    "Save all analysis outputs:\n",
    "- **Model files** - Serialized models in pickle format\n",
    "- **Performance metrics** - CSV files with detailed results\n",
    "- **Analysis summary** - JSON file with complete analysis\n",
    "- **Predictions** - Actual vs predicted values for validation\n",
    "- **Visualizations** - Optional saving of plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory with timestamp\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "analysis_type = 'classification' if problem_type == 'classification' else 'regression'\n",
    "output_dir = output_path / f'{analysis_type}_analysis_{timestamp}'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\nüíæ SAVING RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "# 1. Save comprehensive summary\n",
    "summary = {\n",
    "    'analysis_info': {\n",
    "        'date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'problem_type': problem_type,\n",
    "        'target_variable': target_col,\n",
    "        'n_features': X_train.shape[1],\n",
    "        'n_train_samples': X_train.shape[0],\n",
    "        'n_test_samples': X_test.shape[0]\n",
    "    },\n",
    "    'configuration': CONFIG,\n",
    "    'feature_selection': {\n",
    "        'method': 'automatic' if 'selection_option' not in locals() else selection_option,\n",
    "        'n_features_selected': len(selected_features),\n",
    "        'features': selected_features\n",
    "    },\n",
    "    'model_comparison': comparison_df.to_dict('records'),\n",
    "    'best_model': {\n",
    "        'name': best_model_name,\n",
    "        'parameters': results[best_model_name]['best_params'],\n",
    "        'metrics': results[best_model_name]['metrics']\n",
    "    },\n",
    "    'recommendations': recommendation_df.head(3).to_dict('records'),\n",
    "    'overfitting_analysis': {\n",
    "        'models_with_overfitting': list(overfitting_models['Model'].values) if len(overfitting_models) > 0 else [],\n",
    "        'overfitting_threshold': CONFIG['overfitting_threshold']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add station analysis if available\n",
    "if station_results_df is not None:\n",
    "    if problem_type == 'classification':\n",
    "        summary['station_analysis'] = {\n",
    "            'n_stations': len(station_results_df),\n",
    "            'mean_accuracy': station_results_df['accuracy'].mean(),\n",
    "            'std_accuracy': station_results_df['accuracy'].std(),\n",
    "            'best_stations': station_results_df.head(5).to_dict('records'),\n",
    "            'worst_stations': station_results_df.tail(5).to_dict('records')\n",
    "        }\n",
    "    else:\n",
    "        summary['station_analysis'] = {\n",
    "            'n_stations': len(station_results_df),\n",
    "            'mean_r2': station_results_df['r2_score'].mean(),\n",
    "            'std_r2': station_results_df['r2_score'].std(),\n",
    "            'best_stations': station_results_df.head(5).to_dict('records'),\n",
    "            'worst_stations': station_results_df.tail(5).to_dict('records')\n",
    "        }\n",
    "\n",
    "# Save JSON summary\n",
    "with open(output_dir / 'analysis_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=4)\n",
    "print(\"‚úÖ Saved analysis summary\")\n",
    "\n",
    "# 2. Save detailed DataFrames\n",
    "comparison_df.to_csv(output_dir / 'model_comparison.csv', index=False)\n",
    "recommendation_df.to_csv(output_dir / 'model_recommendations.csv', index=False)\n",
    "print(\"‚úÖ Saved model comparison and recommendations\")\n",
    "\n",
    "if station_results_df is not None:\n",
    "    station_results_df.to_csv(output_dir / 'station_results.csv', index=False)\n",
    "    print(\"‚úÖ Saved station-wise results\")\n",
    "\n",
    "# 3. Save predictions\n",
    "predictions_df = pd.DataFrame({\n",
    "    'actual': y_test,\n",
    "    'predicted': results[best_model_name]['predictions']\n",
    "})\n",
    "\n",
    "if problem_type == 'regression':\n",
    "    predictions_df['error'] = predictions_df['actual'] - predictions_df['predicted']\n",
    "    predictions_df['abs_error'] = np.abs(predictions_df['error'])\n",
    "    predictions_df['pct_error'] = (predictions_df['error'] / predictions_df['actual'] * 100).round(2)\n",
    "\n",
    "if station_info is not None:\n",
    "    predictions_df['station'] = station_test\n",
    "\n",
    "predictions_df.to_csv(output_dir / 'predictions.csv', index=False)\n",
    "print(\"‚úÖ Saved predictions\")\n",
    "\n",
    "# 4. Save best model\n",
    "model_filename = f'best_model_{best_model_name.lower().replace(\" \", \"_\")}.pkl'\n",
    "joblib.dump(best_models[best_model_name], output_dir / model_filename)\n",
    "print(f\"‚úÖ Saved best model: {model_filename}\")\n",
    "\n",
    "# 5. Save all models (optional)\n",
    "save_all = input(\"\\nüíæ Save all trained models? (y/n): \").strip().lower()\n",
    "if save_all == 'y':\n",
    "    models_dir = output_dir / 'all_models'\n",
    "    models_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    for name, model in best_models.items():\n",
    "        filename = f\"{name.lower().replace(' ', '_')}.pkl\"\n",
    "        joblib.dump(model, models_dir / filename)\n",
    "    \n",
    "    print(f\"‚úÖ Saved all {len(best_models)} models\")\n",
    "\n",
    "# 6. Generate and save model card\n",
    "model_card = f\"\"\"\n",
    "# Model Card: {best_model_name}\n",
    "\n",
    "## Model Details\n",
    "- **Model Type**: {best_model_name}\n",
    "- **Problem Type**: {problem_type.capitalize()}\n",
    "- **Target Variable**: {target_col}\n",
    "- **Number of Features**: {X_train.shape[1]}\n",
    "- **Training Date**: {datetime.now().strftime('%Y-%m-%d')}\n",
    "\n",
    "## Performance Metrics\n",
    "\"\"\"\n",
    "\n",
    "if problem_type == 'classification':\n",
    "    model_card += f\"\"\"\n",
    "- **Accuracy**: {best_metrics['accuracy']:.4f}\n",
    "- **Balanced Accuracy**: {best_metrics['balanced_accuracy']:.4f}\n",
    "- **Precision**: {best_metrics['precision']:.4f}\n",
    "- **Recall**: {best_metrics['recall']:.4f}\n",
    "- **F1 Score**: {best_metrics['f1_score']:.4f}\n",
    "\"\"\"\n",
    "else:\n",
    "    model_card += f\"\"\"\n",
    "- **R¬≤ Score**: {best_metrics['r2_score']:.4f}\n",
    "- **RMSE**: {best_metrics['rmse']:.4f}\n",
    "- **MAE**: {best_metrics['mae']:.4f}\n",
    "- **Explained Variance**: {best_metrics['explained_variance']:.4f}\n",
    "\"\"\"\n",
    "\n",
    "model_card += f\"\"\"\n",
    "\n",
    "## Training Information\n",
    "- **Training Samples**: {X_train.shape[0]:,}\n",
    "- **Test Samples**: {X_test.shape[0]:,}\n",
    "- **Cross-validation**: {cv_strategy.n_splits}-fold\n",
    "- **Training Time**: {best_metrics['training_time']:.2f} seconds\n",
    "\n",
    "## Best Parameters\n",
    "{json.dumps(results[best_model_name]['best_params'], indent=2)}\n",
    "\n",
    "## Usage\n",
    "```python\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# Load model\n",
    "model = joblib.load('{model_filename}')\n",
    "\n",
    "# Prepare features (ensure same order as training)\n",
    "features = {selected_features[:3]} # ... etc\n",
    "X = df[features]\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X)\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "with open(output_dir / 'model_card.md', 'w') as f:\n",
    "    f.write(model_card)\n",
    "print(\"‚úÖ Saved model card\")\n",
    "\n",
    "print(f\"\\nüìÅ All results saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 13. Final Summary and Next Steps\n",
    "\n",
    "Display a concise summary of the entire analysis and provide guidance for next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä ANALYSIS SUMMARY\")\n",
    "print(\"-\"*60)\n",
    "print(f\"Problem Type: {problem_type.capitalize()}\")\n",
    "print(f\"Target Variable: {target_col}\")\n",
    "if 'selected_station' in locals() and selected_station:\n",
    "    print(f\"Selected Station: {selected_station}\")\n",
    "print(f\"Features Used: {len(selected_features)}\")\n",
    "print(f\"Models Trained: {len(results)}\")\n",
    "print(f\"Total Training Time: {sum(training_times.values()):.2f}s\")\n",
    "\n",
    "print(f\"\\nüèÜ BEST MODEL: {best_model_name}\")\n",
    "if problem_type == 'classification':\n",
    "    print(f\"   ‚Ä¢ Accuracy: {best_metrics['accuracy']:.2%}\")\n",
    "    print(f\"   ‚Ä¢ Balanced Accuracy: {best_metrics['balanced_accuracy']:.2%}\")\n",
    "else:\n",
    "    print(f\"   ‚Ä¢ R¬≤ Score: {best_metrics['r2_score']:.4f}\")\n",
    "    print(f\"   ‚Ä¢ RMSE: {best_metrics['rmse']:.4f}\")\n",
    "\n",
    "print(f\"\\nüí° RECOMMENDED MODEL: {best_overall['Model']}\")\n",
    "print(f\"   Recommendation Score: {best_overall['Score']:.1f}/100\")\n",
    "\n",
    "print(f\"\\nüìÅ Results saved to: {output_dir}\")\n",
    "\n",
    "print(\"\\nüöÄ NEXT STEPS:\")\n",
    "print(\"-\"*60)\n",
    "print(\"1. Review the model_card.md for deployment instructions\")\n",
    "print(\"2. Validate model performance on new data\")\n",
    "print(\"3. Set up monitoring for model drift\")\n",
    "print(\"4. Consider ensemble methods if higher accuracy needed\")\n",
    "if station_results_df is not None and len(low_perf_stations) > 0:\n",
    "    print(\"5. Investigate low-performance stations for data quality issues\")\n",
    "\n",
    "print(\"\\n‚ú® Thank you for using the Universal ML Model Training Pipeline!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
